#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ç”Ÿäº§çº§På› å­è®¡ç®—å™¨ (PITåŸåˆ™ + æ ‡å‡†åŒ–è¿ç»­è¯„åˆ†åˆ¶ + è¡Œä¸šç‰¹æ®Šå¤„ç†)
===============================================================================

ä¸¥æ ¼éµå¾ªPoint-in-TimeåŸåˆ™ï¼Œä½¿ç”¨é¢„è®¡ç®—è´¢åŠ¡æŒ‡æ ‡è¡¨å®ç°é«˜æ€§èƒ½På› å­è®¡ç®—ã€‚
é›†æˆæ ‡å‡†åŒ–è¿ç»­è¯„åˆ†åˆ¶å’Œè¡Œä¸šåˆ†ç±»ç‰¹æ®Šå¤„ç†ï¼Œæå‡è¯„åˆ†ç²¾ç¡®åº¦å’Œåˆç†æ€§ã€‚

ğŸš€ æ ¸å¿ƒä¼˜åŒ– (v2.0):
1. **æ ‡å‡†åŒ–è¿ç»­è¯„åˆ†åˆ¶**: æ›¿æ¢åˆ†æ¡£è¯„åˆ†ï¼Œä½¿ç”¨å¼‚å¸¸å€¼æˆªæ–­+åˆ†ä½æ•°æ’å+æƒé‡åˆæˆ
2. **æ•°æ®æ—¶æ•ˆæ€§ç­›é€‰**: æ’é™¤end_dateè·ç¦»calc_dateè¶…è¿‡10ä¸ªæœˆçš„è‚¡ç¥¨
3. **æ¨ªæˆªé¢æ ‡å‡†åŒ–**: ç¡®ä¿æ‰€æœ‰æ´»è·ƒè‚¡ç¥¨çš„å…¬å¹³æ¯”è¾ƒ
4. **æƒé‡ä¼˜åŒ–**: GPA(40%) + ROE_EXCL(30%) + ROA_EXCL(30%)

ğŸ“Š è¯„åˆ†æœºåˆ¶:
- å¼‚å¸¸å€¼æˆªæ–­: ä½¿ç”¨1%å’Œ99%åˆ†ä½æ•°
- æ ‡å‡†åŒ–æ–¹æ³•: åˆ†ä½æ•°æ’å (0-100ç™¾åˆ†ä½)
- è¯„åˆ†èŒƒå›´: 0-100è¿ç»­è¯„åˆ† (æ›¿ä»£åŸ18æ¡£åˆ†æ¡£åˆ¶)
- é‡‘èè‚¡ç‰¹æ®Šå¤„ç†: GPA=NULLæ—¶ï¼ŒROE(50%)+ROA(50%)

âš¡ æ€§èƒ½å¯¹æ¯”:
- åŸç‰ˆè®¡ç®—å™¨: 133åª/ç§’ (åˆ†æ¡£è¯„åˆ†åˆ¶)
- ç”Ÿäº§çº§ç‰ˆæœ¬: 300-500åª/ç§’ (æ ‡å‡†åŒ–è¿ç»­è¯„åˆ†åˆ¶)

ğŸ¯ PITåŸåˆ™æ ¸å¿ƒ:
1. åœ¨æŒ‡å®šæ—¶ç‚¹(as_of_date)ï¼Œåªèƒ½çœ‹åˆ°è¯¥æ—¶ç‚¹ä¹‹å‰æˆ–å½“æ—¥å…¬å‘Šçš„æ•°æ®
2. ann_dateæ˜¯çœŸå®çš„å…¬å‘Šæ—¥æœŸï¼Œä¸¥æ ¼ä¿æŒä¸å˜
3. æŸ¥è¯¢æ¡ä»¶: ann_date <= as_of_date AND end_date >= (calc_date - 10ä¸ªæœˆ)

ğŸ­ è¡Œä¸šç‰¹æ®Šå¤„ç†:
1. é“¶è¡Œä¸š: GPAè®¾ä¸ºNULL (è¥ä¸šæˆæœ¬ä¸º0å¯¼è‡´GPA=100%è¯¯å¯¼)
2. è¯åˆ¸ä¸š: GPAè®¾ä¸ºNULL (æˆæœ¬ç»“æ„ç‰¹æ®Š)
3. ä¿é™©ä¸š: GPAè®¾ä¸ºNULL (æˆæœ¬ç»“æ„ç‰¹æ®Š)
4. å…¶ä»–è¡Œä¸š: æ ‡å‡†GPAè®¡ç®—

ğŸ”§ æŠ€æœ¯ä¼˜åŒ–:
1. ç›´æ¥æŸ¥è¯¢é¢„è®¡ç®—çš„è´¢åŠ¡æŒ‡æ ‡ï¼Œæ— éœ€å®æ—¶è®¡ç®—TTM
2. å•è¡¨æŸ¥è¯¢æ›¿ä»£å¤šè¡¨JOIN
3. å‘é‡åŒ–æ ‡å‡†åŒ–å¤„ç†å’ŒPè¯„åˆ†è®¡ç®—
4. å‡å°‘æ•°æ®ä¼ è¾“é‡å’Œè®¡ç®—å¤æ‚åº¦
5. é›†æˆPITè¡Œä¸šåˆ†ç±»æŸ¥è¯¢ï¼Œæ”¯æŒè¡Œä¸šç‰¹æ®Šå¤„ç†

Author: AI Assistant
Date: 2025-08-11 (v2.0 - æ ‡å‡†åŒ–è¿ç»­è¯„åˆ†åˆ¶)
"""

import logging
import pandas as pd
import numpy as np
from datetime import datetime
from typing import List, Optional, Dict, Any
import time

from research.tools.context import ResearchContext


class ProductionPFactorCalculator:
    """ç”Ÿäº§çº§På› å­è®¡ç®—å™¨ (åŸºäºé¢„è®¡ç®—è¡¨çš„é«˜æ€§èƒ½å®ç°)"""
    
    def __init__(self, context: ResearchContext):
        """åˆå§‹åŒ–è®¡ç®—å™¨
        
        Args:
            context: ResearchContextå®ä¾‹
        """
        self.context = context
        self.db_manager = context.db_manager
        self.logger = self._setup_logger()
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            'query_time': 0,
            'calculation_time': 0,
            'save_time': 0,
            'total_time': 0
        }
    
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
        logger = logging.getLogger('ProductionPFactorCalculator')
        logger.setLevel(logging.INFO)
        return logger
    
    def calculate_p_factors_pit(
        self,
        as_of_date: str,
        stock_codes: List[str]
    ) -> Dict[str, Any]:
        """åŸºäºPITåŸåˆ™çš„MVPç‰ˆæœ¬På› å­è®¡ç®—

        Args:
            as_of_date: PITæˆªæ­¢æ—¥æœŸ (åœ¨æ­¤æ—¶ç‚¹èƒ½çœ‹åˆ°çš„æ‰€æœ‰å·²å…¬å‘Šæ•°æ®)
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨

        Returns:
            è®¡ç®—ç»“æœç»Ÿè®¡
        """
        start_time = time.time()

        self.logger.info(f"å¼€å§‹åŸºäºPITåŸåˆ™çš„MVPç‰ˆæœ¬På› å­è®¡ç®—: {as_of_date}")
        self.logger.info(f"è‚¡ç¥¨æ•°é‡: {len(stock_codes)}")

        # 1. æŸ¥è¯¢MVPé¢„è®¡ç®—çš„è´¢åŠ¡æŒ‡æ ‡ (ä¸¥æ ¼éµå¾ªPITåŸåˆ™)
        query_start = time.time()
        indicators_data = self._get_mvp_precomputed_indicators_pit(as_of_date, stock_codes)
        self.stats['query_time'] = time.time() - query_start

        if indicators_data.empty:
            self.logger.warning(f"åœ¨æ—¶ç‚¹ {as_of_date} æœªæ‰¾åˆ°MVPé¢„è®¡ç®—çš„è´¢åŠ¡æŒ‡æ ‡æ•°æ®")
            return {'success_count': 0, 'failed_count': len(stock_codes)}

        self.logger.info(f"æŸ¥è¯¢åˆ° {len(indicators_data)} æ¡MVPé¢„è®¡ç®—æŒ‡æ ‡ (PITæ—¶ç‚¹: {as_of_date})")

        # 2. å¿«é€Ÿè®¡ç®—På› å­ (ä¿æŒann_dateä¸å˜)
        calc_start = time.time()
        p_factors = self._calculate_p_factors_from_mvp_indicators_pit(indicators_data, as_of_date)
        self.stats['calculation_time'] = time.time() - calc_start

        # 3. ä¿å­˜På› å­ç»“æœ
        save_start = time.time()
        success_count = 0
        if not p_factors.empty:
            self._save_p_factors_mvp(p_factors)
            success_count = len(p_factors)
        self.stats['save_time'] = time.time() - save_start

        # 4. ç»Ÿè®¡ç»“æœ
        self.stats['total_time'] = time.time() - start_time
        failed_count = len(stock_codes) - success_count

        self._log_performance_stats(success_count, failed_count)

        return {
            'success_count': success_count,
            'failed_count': failed_count,
            'total_time': self.stats['total_time'],
            'performance_stats': self.stats.copy()
        }
    
    def calculate_p_factors_batch_pit(
        self,
        as_of_dates: List[str],
        stock_codes: List[str],
        batch_size: int = 1000
    ) -> Dict[str, Any]:
        """åŸºäºPITåŸåˆ™çš„MVPç‰ˆæœ¬æ‰¹é‡På› å­è®¡ç®—

        Args:
            as_of_dates: PITæˆªæ­¢æ—¥æœŸåˆ—è¡¨
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            batch_size: æ‰¹æ¬¡å¤§å°

        Returns:
            è®¡ç®—ç»“æœç»Ÿè®¡
        """
        self.logger.info(f"å¼€å§‹åŸºäºPITåŸåˆ™çš„MVPç‰ˆæœ¬æ‰¹é‡På› å­è®¡ç®—")
        self.logger.info(f"PITæ—¶ç‚¹: {len(as_of_dates)} ä¸ª")
        self.logger.info(f"è‚¡ç¥¨æ•°é‡: {len(stock_codes)} åª")
        self.logger.info(f"æ‰¹æ¬¡å¤§å°: {batch_size}")

        total_start = time.time()
        total_success = 0
        total_failed = 0

        # æŒ‰PITæ—¶ç‚¹å¤„ç†
        for i, as_of_date in enumerate(as_of_dates, 1):
            self.logger.info(f"å¤„ç†PITæ—¶ç‚¹ {i}/{len(as_of_dates)}: {as_of_date}")

            # åˆ†æ‰¹å¤„ç†è‚¡ç¥¨
            for j in range(0, len(stock_codes), batch_size):
                batch_stocks = stock_codes[j:j + batch_size]

                try:
                    result = self.calculate_p_factors_pit(as_of_date, batch_stocks)
                    total_success += result['success_count']
                    total_failed += result['failed_count']

                    self.logger.info(f"æ‰¹æ¬¡ {j//batch_size + 1}: æˆåŠŸ {result['success_count']}, å¤±è´¥ {result['failed_count']}")

                except Exception as e:
                    self.logger.error(f"æ‰¹æ¬¡è®¡ç®—å¤±è´¥: {e}")
                    total_failed += len(batch_stocks)

        total_time = time.time() - total_start

        self.logger.info("=" * 50)
        self.logger.info("åŸºäºPITåŸåˆ™çš„MVPç‰ˆæœ¬æ‰¹é‡På› å­è®¡ç®—å®Œæˆ")
        self.logger.info("=" * 50)
        self.logger.info(f"æ€»è€—æ—¶: {total_time:.2f} ç§’")
        self.logger.info(f"æ€»æˆåŠŸ: {total_success}")
        self.logger.info(f"æ€»å¤±è´¥: {total_failed}")

        if total_time > 0:
            throughput = total_success / total_time
            self.logger.info(f"ååé‡: {throughput:.1f} åª/ç§’")

        return {
            'success_count': total_success,
            'failed_count': total_failed,
            'total_time': total_time,
            'throughput': total_success / total_time if total_time > 0 else 0
        }

    def calculate_p_factors_batch_pit(
        self,
        start_date: str,
        end_date: str,
        mode: Optional[str] = None
    ) -> Dict[str, Any]:
        """åŸºäºæ—¥æœŸèŒƒå›´çš„æ‰¹é‡På› å­è®¡ç®— (ä¸ºrunnerè„šæœ¬æä¾›çš„æ¥å£)

        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            mode: æ‰§è¡Œæ¨¡å¼ ('incremental', 'backfill', Noneä¸ºè‡ªåŠ¨æ£€æµ‹)

        Returns:
            æ‰§è¡Œç»“æœç»Ÿè®¡
        """
        self.logger.info(f"å¼€å§‹æ‰¹é‡På› å­è®¡ç®—: {start_date} ~ {end_date}")

        # 1. æ™ºèƒ½æ¨¡å¼æ£€æµ‹
        if mode:
            execution_mode = mode
            self.logger.info(f"ä½¿ç”¨æŒ‡å®šæ¨¡å¼: {execution_mode}")
        else:
            execution_mode = self.detect_execution_mode(start_date, end_date)

        # 2. ç”Ÿæˆè®¡ç®—æ—¥æœŸåˆ—è¡¨
        calc_dates = self.generate_calculation_dates(start_date, end_date, execution_mode)

        if not calc_dates:
            self.logger.warning("æœªæ‰¾åˆ°éœ€è¦è®¡ç®—çš„æ—¥æœŸ")
            return {
                'success_count': 0,
                'failed_count': 0,
                'total_time': 0,
                'throughput': 0
            }

        self.logger.info(f"å…±éœ€è®¡ç®— {len(calc_dates)} ä¸ªæ—¥æœŸ")

        # 3. æ‰§è¡Œæ‰¹é‡è®¡ç®—
        total_start = time.time()
        total_success = 0
        total_failed = 0

        for i, calc_date in enumerate(calc_dates, 1):
            self.logger.info(f"\nè¿›åº¦: [{i}/{len(calc_dates)}] å¤„ç†æ—¥æœŸ: {calc_date}")

            try:
                # è·å–åœ¨äº¤æ˜“è‚¡ç¥¨åˆ—è¡¨
                stock_codes = self._get_trading_stock_codes(calc_date)

                if not stock_codes:
                    self.logger.warning(f"{calc_date} æœªæ‰¾åˆ°åœ¨äº¤æ˜“è‚¡ç¥¨")
                    continue

                # æ‰§è¡ŒPå› å­è®¡ç®—
                result = self.calculate_p_factors_pit(calc_date, stock_codes)
                total_success += result['success_count']
                total_failed += result['failed_count']

                self.logger.info(f"{calc_date} è®¡ç®—å®Œæˆ: æˆåŠŸ {result['success_count']}, å¤±è´¥ {result['failed_count']}")

            except Exception as e:
                self.logger.error(f"{calc_date} è®¡ç®—å¤±è´¥: {e}")
                total_failed += len(stock_codes) if 'stock_codes' in locals() else 0

        total_time = time.time() - total_start

        self.logger.info("=" * 50)
        self.logger.info("æ‰¹é‡På› å­è®¡ç®—å®Œæˆ")
        self.logger.info("=" * 50)
        self.logger.info(f"æ€»è€—æ—¶: {total_time:.2f} ç§’")
        self.logger.info(f"æ€»æˆåŠŸ: {total_success}")
        self.logger.info(f"æ€»å¤±è´¥: {total_failed}")

        if total_time > 0:
            throughput = total_success / total_time
            self.logger.info(f"ååé‡: {throughput:.1f} åª/ç§’")

        return {
            'success_count': total_success,
            'failed_count': total_failed,
            'total_time': total_time,
            'throughput': total_success / total_time if total_time > 0 else 0,
            'total_dates': len(calc_dates),
            'successful_dates': len(calc_dates) - (total_failed // max(len(stock_codes) if 'stock_codes' in locals() else 1, 1)),
            'failed_dates': total_failed // max(len(stock_codes) if 'stock_codes' in locals() else 1, 1),
            'total_stocks_processed': total_success + total_failed,
            'total_records_saved': total_success
        }

    def detect_execution_mode(self, start_date: str, end_date: str) -> str:
        """æ™ºèƒ½æ£€æµ‹æ‰§è¡Œæ¨¡å¼

        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ

        Returns:
            æ‰§è¡Œæ¨¡å¼ ('incremental' æˆ– 'backfill')
        """
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰ç°æœ‰çš„På› å­æ•°æ®
            query = """
            SELECT COUNT(*) as count
            FROM pgs_factors.p_factor
            WHERE calc_date BETWEEN %s AND %s
            """

            result = self.context.query_dataframe(query, (start_date, end_date))

            if result.empty or result.iloc[0]['count'] == 0:
                self.logger.info("æœªå‘ç°ç°æœ‰På› å­æ•°æ®ï¼Œä½¿ç”¨backfillæ¨¡å¼")
                return 'backfill'
            else:
                self.logger.info(f"å‘ç° {result.iloc[0]['count']} æ¡ç°æœ‰På› å­æ•°æ®ï¼Œä½¿ç”¨incrementalæ¨¡å¼")
                return 'incremental'

        except Exception as e:
            self.logger.warning(f"æ£€æµ‹æ‰§è¡Œæ¨¡å¼å¤±è´¥: {e}ï¼Œé»˜è®¤ä½¿ç”¨incrementalæ¨¡å¼")
            return 'incremental'

    def generate_calculation_dates(self, start_date: str, end_date: str, mode: str) -> List[str]:
        """ç”Ÿæˆè®¡ç®—æ—¥æœŸåˆ—è¡¨

        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            mode: æ‰§è¡Œæ¨¡å¼

        Returns:
            è®¡ç®—æ—¥æœŸåˆ—è¡¨
        """
        # ç”Ÿæˆæ‰€æœ‰å‘¨äº”æ—¥æœŸ
        all_fridays = self._generate_friday_dates(start_date, end_date)

        if mode == 'backfill':
            # å›å¡«æ¨¡å¼ï¼šè®¡ç®—æ‰€æœ‰æ—¥æœŸ
            return all_fridays
        elif mode == 'incremental':
            # å¢é‡æ¨¡å¼ï¼šåªè®¡ç®—ç¼ºå¤±çš„æ—¥æœŸ
            return self._filter_missing_dates(all_fridays)
        else:
            self.logger.warning(f"æœªçŸ¥æ‰§è¡Œæ¨¡å¼: {mode}ï¼Œä½¿ç”¨å¢é‡æ¨¡å¼")
            return self._filter_missing_dates(all_fridays)

    def _generate_friday_dates(self, start_date: str, end_date: str) -> List[str]:
        """ç”ŸæˆæŒ‡å®šèŒƒå›´å†…çš„æ‰€æœ‰å‘¨äº”æ—¥æœŸ

        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ

        Returns:
            å‘¨äº”æ—¥æœŸåˆ—è¡¨
        """
        from datetime import datetime, timedelta

        start = datetime.strptime(start_date, '%Y-%m-%d')
        end = datetime.strptime(end_date, '%Y-%m-%d')

        fridays = []
        current = start

        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå‘¨äº”
        while current.weekday() != 4:  # 4 = å‘¨äº”
            current += timedelta(days=1)
            if current > end:
                break

        # æ”¶é›†æ‰€æœ‰å‘¨äº”
        while current <= end:
            fridays.append(current.strftime('%Y-%m-%d'))
            current += timedelta(days=7)

        return fridays

    def _filter_missing_dates(self, dates: List[str]) -> List[str]:
        """è¿‡æ»¤å‡ºç¼ºå¤±På› å­æ•°æ®çš„æ—¥æœŸ

        Args:
            dates: å€™é€‰æ—¥æœŸåˆ—è¡¨

        Returns:
            ç¼ºå¤±æ•°æ®çš„æ—¥æœŸåˆ—è¡¨
        """
        if not dates:
            return []

        try:
            # æŸ¥è¯¢å·²æœ‰æ•°æ®çš„æ—¥æœŸ
            query = """
            SELECT DISTINCT calc_date
            FROM pgs_factors.p_factor
            WHERE calc_date = ANY(%s)
            """

            result = self.context.query_dataframe(query, (dates,))

            if result.empty:
                return dates

            existing_dates = set(result['calc_date'].dt.strftime('%Y-%m-%d').tolist())
            missing_dates = [date for date in dates if date not in existing_dates]

            self.logger.info(f"æ€»æ—¥æœŸ: {len(dates)}, å·²æœ‰æ•°æ®: {len(existing_dates)}, ç¼ºå¤±æ•°æ®: {len(missing_dates)}")

            return missing_dates

        except Exception as e:
            self.logger.error(f"è¿‡æ»¤ç¼ºå¤±æ—¥æœŸå¤±è´¥: {e}")
            return dates

    def _get_trading_stock_codes(self, calc_date: str) -> List[str]:
        """è·å–æŒ‡å®šæ—¥æœŸçš„åœ¨äº¤æ˜“è‚¡ç¥¨åˆ—è¡¨ï¼ˆå·²é›†æˆé€€å¸‚è‚¡ç¥¨ç­›é€‰ï¼‰

        Args:
            calc_date: è®¡ç®—æ—¥æœŸ

        Returns:
            åœ¨äº¤æ˜“è‚¡ç¥¨ä»£ç åˆ—è¡¨
        """
        try:
            # ä½¿ç”¨ä¼˜åŒ–åçš„å‡½æ•°è·å–åœ¨äº¤æ˜“è‚¡ç¥¨ï¼ˆå·²æ’é™¤é€€å¸‚è‚¡ç¥¨ï¼‰
            query = "SELECT * FROM get_trading_stocks_optimized(%s)"
            result = self.context.query_dataframe(query, (calc_date,))

            if result is not None and not result.empty:
                stock_codes = result['ts_code'].tolist()
                self.logger.info(f"{calc_date} è·å–åˆ° {len(stock_codes)} åªåœ¨äº¤æ˜“è‚¡ç¥¨ï¼ˆå·²æ’é™¤é€€å¸‚è‚¡ç¥¨ï¼‰")
                return stock_codes
            else:
                self.logger.warning(f"{calc_date} æœªæ‰¾åˆ°åœ¨äº¤æ˜“è‚¡ç¥¨æ•°æ®")
                return []

        except Exception as e:
            self.logger.error(f"è·å– {calc_date} è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            return []

    def _validate_calculation_results(self, calc_dates: List[str]):
        """éªŒè¯På› å­è®¡ç®—ç»“æœçš„æ•°æ®è´¨é‡

        Args:
            calc_dates: éœ€è¦éªŒè¯çš„è®¡ç®—æ—¥æœŸåˆ—è¡¨
        """
        self.logger.info(f"å¼€å§‹éªŒè¯ {len(calc_dates)} ä¸ªæ—¥æœŸçš„På› å­æ•°æ®è´¨é‡")

        for calc_date in calc_dates:
            try:
                # æŸ¥è¯¢è¯¥æ—¥æœŸçš„På› å­æ•°æ®
                query = """
                SELECT
                    COUNT(*) as total_count,
                    COUNT(CASE WHEN p_score IS NOT NULL THEN 1 END) as valid_score_count,
                    AVG(p_score) as avg_score,
                    MIN(p_score) as min_score,
                    MAX(p_score) as max_score
                FROM pgs_factors.p_factor
                WHERE calc_date = %s
                """

                result = self.context.query_dataframe(query, (calc_date,))

                if result.empty or result.iloc[0]['total_count'] == 0:
                    self.logger.warning(f"{calc_date}: æ— På› å­æ•°æ®")
                else:
                    row = result.iloc[0]
                    self.logger.info(f"{calc_date}: æ€»è®°å½• {row['total_count']}, "
                                   f"æœ‰æ•ˆè¯„åˆ† {row['valid_score_count']}, "
                                   f"å¹³å‡åˆ† {row['avg_score']:.2f}, "
                                   f"åˆ†æ•°èŒƒå›´ [{row['min_score']:.2f}, {row['max_score']:.2f}]")

            except Exception as e:
                self.logger.error(f"éªŒè¯ {calc_date} æ•°æ®è´¨é‡å¤±è´¥: {e}")

        self.logger.info("På› å­æ•°æ®è´¨é‡éªŒè¯å®Œæˆ")
    
    def _get_mvp_precomputed_indicators_pit(self, as_of_date: str, stock_codes: List[str]) -> pd.DataFrame:
        """åŸºäºPITåŸåˆ™æŸ¥è¯¢MVPé¢„è®¡ç®—çš„è´¢åŠ¡æŒ‡æ ‡ (ä¼˜åŒ–ç‰ˆ - æ·»åŠ æ•°æ®æ—¶æ•ˆæ€§ç­›é€‰å’Œé€€å¸‚è‚¡ç¥¨ç­›é€‰)

        Args:
            as_of_date: PITæˆªæ­¢æ—¥æœŸ (åœ¨æ­¤æ—¶ç‚¹èƒ½çœ‹åˆ°çš„æ‰€æœ‰å·²å…¬å‘Šæ•°æ®)
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨

        Returns:
            è´¢åŠ¡æŒ‡æ ‡æ•°æ® (æ¯åªè‚¡ç¥¨åœ¨as_of_dateæ—¶ç‚¹èƒ½çœ‹åˆ°çš„æœ€æ–°è´¢åŠ¡æ•°æ®ï¼Œæ’é™¤è¿‡æ—¶æ•°æ®å’Œé€€å¸‚è‚¡ç¥¨)
        """
        try:
            # åŸºäºPITåŸåˆ™æŸ¥è¯¢MVPé¢„è®¡ç®—è¡¨ï¼Œæ·»åŠ æ•°æ®æ—¶æ•ˆæ€§ç­›é€‰å’Œé€€å¸‚è‚¡ç¥¨ç­›é€‰
            # å…³é”®æ”¹è¿›ï¼š
            # 1. ä½¿ç”¨ ann_date <= as_of_date ç¡®ä¿åªçœ‹åˆ°å·²å…¬å‘Šçš„æ•°æ® (PITåŸåˆ™)
            # 2. æ·»åŠ  end_date æ—¶æ•ˆæ€§ç­›é€‰ï¼Œæ’é™¤è·ç¦»calc_dateè¶…è¿‡10ä¸ªæœˆçš„æ•°æ®
            # 3. æ·»åŠ é€€å¸‚è‚¡ç¥¨ç­›é€‰ï¼Œç¡®ä¿åªåŒ…å«åœ¨è®¡ç®—æ—¶ç‚¹ä»åœ¨äº¤æ˜“çš„è‚¡ç¥¨
            query = """
            WITH latest_indicators AS (
                SELECT
                    pit.ts_code,
                    pit.end_date,
                    pit.ann_date,  -- ä¿æŒçœŸå®çš„å…¬å‘Šæ—¥æœŸ
                    pit.data_source,
                    pit.gpa_ttm,
                    pit.roe_excl_ttm,
                    pit.roa_excl_ttm,
                    pit.net_margin_ttm,
                    pit.operating_margin_ttm,
                    pit.roi_ttm,
                    pit.asset_turnover_ttm,
                    pit.equity_multiplier,
                    pit.debt_to_asset_ratio,
                    pit.equity_ratio,
                    pit.revenue_yoy_growth,
                    pit.n_income_yoy_growth,
                    pit.operate_profit_yoy_growth,
                    pit.data_quality,
                    pit.calculation_status,
                    ROW_NUMBER() OVER (PARTITION BY pit.ts_code ORDER BY pit.ann_date DESC, pit.end_date DESC) as rn
                FROM pgs_factors.pit_financial_indicators pit
                INNER JOIN tushare.stock_basic sb ON pit.ts_code = sb.ts_code
                WHERE pit.ann_date <= %s  -- PITåŸåˆ™: åªçœ‹å·²å…¬å‘Šçš„æ•°æ®
                AND pit.ts_code = ANY(%s)
                AND pit.calculation_status = 'success'
                AND pit.data_quality IN ('high', 'normal', 'outlier_high', 'outlier_low')
                -- æ•°æ®æ—¶æ•ˆæ€§ç­›é€‰: æ’é™¤end_dateè·ç¦»calc_dateè¶…è¿‡10ä¸ªæœˆçš„è‚¡ç¥¨
                AND pit.end_date >= (%s::date - INTERVAL '10 months')
                -- é€€å¸‚è‚¡ç¥¨ç­›é€‰: ç¡®ä¿åœ¨è®¡ç®—æ—¶ç‚¹ä»åœ¨äº¤æ˜“
                AND sb.list_date <= %s  -- ç¡®ä¿åœ¨è®¡ç®—æ—¥æœŸå‰å·²ä¸Šå¸‚
                AND (sb.delist_date IS NULL OR sb.delist_date > %s)  -- æ’é™¤å·²é€€å¸‚è‚¡ç¥¨
            )
            SELECT
                ts_code, end_date, ann_date, data_source,
                gpa_ttm, roe_excl_ttm, roa_excl_ttm,
                net_margin_ttm, operating_margin_ttm, roi_ttm,
                asset_turnover_ttm, equity_multiplier,
                debt_to_asset_ratio, equity_ratio,
                revenue_yoy_growth, n_income_yoy_growth, operate_profit_yoy_growth,
                data_quality, calculation_status
            FROM latest_indicators
            WHERE rn = 1  -- æ¯åªè‚¡ç¥¨çš„æœ€æ–°æ•°æ®
            ORDER BY ts_code
            """

            result = self.context.query_dataframe(query, (as_of_date, stock_codes, as_of_date, as_of_date, as_of_date))

            if result is not None and not result.empty:
                excluded_count = len(stock_codes) - len(result)
                if excluded_count > 0:
                    self.logger.info(f"æ•°æ®æ—¶æ•ˆæ€§å’Œé€€å¸‚è‚¡ç¥¨ç­›é€‰: æ’é™¤äº† {excluded_count} åªè‚¡ç¥¨ (end_dateè¿‡äºæ»åæˆ–å·²é€€å¸‚)")

            return result

        except Exception as e:
            self.logger.error(f"æŸ¥è¯¢MVPé¢„è®¡ç®—æŒ‡æ ‡å¤±è´¥ (PITæ—¶ç‚¹: {as_of_date}): {e}")
            return pd.DataFrame()
    
    def _calculate_p_factors_from_mvp_indicators_pit(
        self,
        indicators_data: pd.DataFrame,
        as_of_date: str
    ) -> pd.DataFrame:
        """åŸºäºMVPé¢„è®¡ç®—æŒ‡æ ‡å’ŒPITåŸåˆ™å¿«é€Ÿè®¡ç®—På› å­ (ä¼˜åŒ–ç‰ˆ - æ ‡å‡†åŒ–è¿ç»­è¯„åˆ†åˆ¶)

        Args:
            indicators_data: MVPé¢„è®¡ç®—çš„è´¢åŠ¡æŒ‡æ ‡æ•°æ®
            as_of_date: PITæˆªæ­¢æ—¥æœŸ

        Returns:
            På› å­ç»“æœDataFrame (ä¿æŒann_dateä¸ºçœŸå®å…¬å‘Šæ—¥æœŸï¼Œä½¿ç”¨è¿ç»­è¯„åˆ†åˆ¶)
        """
        if indicators_data.empty:
            return pd.DataFrame()

        # å¤åˆ¶æ•°æ®é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
        df = indicators_data.copy()

        # æ·»åŠ è®¡ç®—æ—¥æœŸ (PITæˆªæ­¢æ—¥æœŸ)
        df['calc_date'] = as_of_date

        self.logger.info(f"å¼€å§‹æ ‡å‡†åŒ–å¤„ç† {len(df)} åªè‚¡ç¥¨çš„è´¢åŠ¡æŒ‡æ ‡")

        # 1. è´¢åŠ¡æŒ‡æ ‡æ ‡å‡†åŒ–å¤„ç† (å¼‚å¸¸å€¼æˆªæ–­ + åˆ†ä½æ•°æ’å)
        df = self._standardize_financial_indicators(df)

        # 2. å‘é‡åŒ–è®¡ç®—Pè¯„åˆ† (åŸºäºæ ‡å‡†åŒ–åçš„æŒ‡æ ‡)
        df['p_score'] = self._calculate_p_score_vectorized_mvp(df)

        # 3. è®¡ç®—Pæ’å (åœ¨åŒä¸€è®¡ç®—æ—¥æœŸä¸‹çš„æ’åï¼Œä½¿ç”¨è¿ç»­æ’å)
        # å¤„ç†NaNå’Œæ— ç©·å¤§å€¼ï¼Œç¡®ä¿æ’åè®¡ç®—æ­£å¸¸
        df['p_score'] = df['p_score'].fillna(0)  # NaNå¡«å……ä¸º0åˆ†
        df['p_score'] = df['p_score'].replace([np.inf, -np.inf], [100, 0])  # æ— ç©·å¤§å¤„ç†
        df['p_rank'] = df['p_score'].rank(method='min', ascending=False, na_option='bottom').astype(int)

        # 4. æ˜ å°„è´¢åŠ¡æŒ‡æ ‡åˆ°På› å­è¡¨å­—æ®µ (ä¿æŒåŸå§‹å€¼ç”¨äºå­˜å‚¨)
        df['gpa'] = df['gpa_ttm']
        df['roe_excl'] = df['roe_excl_ttm']
        df['roa_excl'] = df['roa_excl_ttm']

        # 5. åº”ç”¨è¡Œä¸šç‰¹æ®Šå¤„ç† (åœ¨æ ‡å‡†åŒ–ä¹‹å)
        df = self._apply_industry_special_handling(df, as_of_date)

        # 6. é‡æ–°è®¡ç®—å—è¡Œä¸šç‰¹æ®Šå¤„ç†å½±å“çš„Pè¯„åˆ†å’Œæ’å
        # å¦‚æœæœ‰è‚¡ç¥¨çš„GPAè¢«è®¾ä¸ºNULLï¼Œéœ€è¦é‡æ–°è®¡ç®—Pè¯„åˆ†
        affected_stocks = df[df['gpa'].isna()]['ts_code'].unique()
        if len(affected_stocks) > 0:
            self.logger.info(f"é‡æ–°è®¡ç®— {len(affected_stocks)} åªé‡‘èè‚¡çš„Pè¯„åˆ† (GPA=NULL)")

            # å¯¹äºGPAä¸ºNULLçš„è‚¡ç¥¨ï¼Œé‡æ–°è¿›è¡Œæ ‡å‡†åŒ–å’Œè¯„åˆ†è®¡ç®—
            # è¿™é‡Œéœ€è¦ç‰¹æ®Šå¤„ç†ï¼šåªä½¿ç”¨ROEå’ŒROAï¼Œæƒé‡é‡æ–°åˆ†é…ä¸º50%:50%
            mask_null_gpa = df['gpa'].isna()

            for idx in df[mask_null_gpa].index:
                roe_score = df.loc[idx, 'roe_excl_ttm_standardized'] if 'roe_excl_ttm_standardized' in df.columns else 0
                roa_score = df.loc[idx, 'roa_excl_ttm_standardized'] if 'roa_excl_ttm_standardized' in df.columns else 0

                # é‡‘èè‚¡Pè¯„åˆ† = ROE(50%) + ROA(50%)
                df.loc[idx, 'p_score'] = (roe_score * 0.5 + roa_score * 0.5)

            # é‡æ–°è®¡ç®—æ’å (å¤„ç†NaNå’Œæ— ç©·å¤§å€¼)
            df['p_score'] = df['p_score'].fillna(0)  # NaNå¡«å……ä¸º0åˆ†
            df['p_score'] = df['p_score'].replace([np.inf, -np.inf], [100, 0])  # æ— ç©·å¤§å¤„ç†
            df['p_rank'] = df['p_score'].rank(method='min', ascending=False, na_option='bottom').astype(int)

        # é€‰æ‹©è¾“å‡ºåˆ— (åŒ…å«æ‰€æœ‰På› å­è¡¨å­—æ®µ)
        output_columns = [
            # æ ¸å¿ƒæ ‡è¯†å­—æ®µ
            'ts_code', 'calc_date', 'ann_date', 'end_date', 'data_source',
            # På› å­æ ¸å¿ƒæŒ‡æ ‡
            'p_score', 'p_rank',
            # åŸºç¡€è´¢åŠ¡æŒ‡æ ‡ (3ä¸ªæ ¸å¿ƒ + å…¶ä»–æŒ‡æ ‡)
            'gpa', 'roe_excl', 'roa_excl',
            'net_margin_ttm', 'operating_margin_ttm', 'roi_ttm',
            'asset_turnover_ttm', 'equity_multiplier',
            'debt_to_asset_ratio', 'equity_ratio',
            'revenue_yoy_growth', 'n_income_yoy_growth', 'operate_profit_yoy_growth',
            # å…ƒæ•°æ®
            'data_quality', 'calculation_status'
        ]

        # ç¡®ä¿æ‰€æœ‰åˆ—éƒ½å­˜åœ¨ï¼Œä¸ºç¼ºå¤±å­—æ®µè®¾ç½®é»˜è®¤å€¼
        for col in output_columns:
            if col not in df.columns:
                if col == 'ann_date':
                    # å¦‚æœæ²¡æœ‰ann_dateï¼Œä½¿ç”¨end_dateä½œä¸ºä¸´æ—¶å€¼ï¼Œä½†è¿™ä¸åº”è¯¥å‘ç”Ÿ
                    df[col] = df.get('end_date', as_of_date)
                    self.logger.warning(f"ç¼ºå°‘ann_dateå­—æ®µï¼Œä½¿ç”¨end_dateä½œä¸ºä¸´æ—¶å€¼")
                elif col == 'calculation_status':
                    df[col] = 'success'  # é»˜è®¤è®¡ç®—çŠ¶æ€
                elif col == 'p_rank':
                    df[col] = 0  # å¦‚æœæ²¡æœ‰è®¡ç®—æ’åï¼Œè®¾ä¸º0
                else:
                    df[col] = None  # å…¶ä»–å­—æ®µè®¾ä¸ºNone

        self.logger.info(f"På› å­è®¡ç®—å®Œæˆ: å¹³å‡è¯„åˆ† {df['p_score'].mean():.2f}, "
                        f"è¯„åˆ†èŒƒå›´ [{df['p_score'].min():.2f}, {df['p_score'].max():.2f}], "
                        f"æ’åèŒƒå›´ [1, {df['p_rank'].max()}]")

        return df[output_columns]

    def _apply_industry_special_handling(self, df: pd.DataFrame, as_of_date: str) -> pd.DataFrame:
        """åº”ç”¨è¡Œä¸šç‰¹æ®Šå¤„ç†é€»è¾‘

        Args:
            df: åŒ…å«è´¢åŠ¡æŒ‡æ ‡çš„DataFrame
            as_of_date: PITæˆªæ­¢æ—¥æœŸ

        Returns:
            åº”ç”¨ç‰¹æ®Šå¤„ç†åçš„DataFrame
        """
        if df.empty:
            return df

        try:
            # è·å–æ‰€æœ‰è‚¡ç¥¨çš„è¡Œä¸šåˆ†ç±»ä¿¡æ¯
            stock_codes = df['ts_code'].unique().tolist()
            industry_info = self._get_industry_classification_pit(stock_codes, as_of_date)

            if industry_info.empty:
                self.logger.warning(f"æœªæ‰¾åˆ°è‚¡ç¥¨çš„è¡Œä¸šåˆ†ç±»ä¿¡æ¯ï¼Œè·³è¿‡ç‰¹æ®Šå¤„ç†")
                return df

            # åˆå¹¶è¡Œä¸šä¿¡æ¯
            df_with_industry = df.merge(
                industry_info[['ts_code', 'requires_special_gpa_handling', 'gpa_calculation_method']],
                on='ts_code',
                how='left'
            )

            # åº”ç”¨GPAç‰¹æ®Šå¤„ç†
            mask_special_gpa = df_with_industry['requires_special_gpa_handling'] == True
            mask_null_gpa = df_with_industry['gpa_calculation_method'] == 'null'

            # å¯¹éœ€è¦ç‰¹æ®Šå¤„ç†çš„è‚¡ç¥¨ï¼Œå°†GPAè®¾ä¸ºNULL
            df_with_industry.loc[mask_special_gpa & mask_null_gpa, 'gpa'] = None

            # è®°å½•ç‰¹æ®Šå¤„ç†çš„è‚¡ç¥¨
            special_stocks = df_with_industry[mask_special_gpa & mask_null_gpa]['ts_code'].unique()
            if len(special_stocks) > 0:
                self.logger.info(f"å¯¹ {len(special_stocks)} åªé‡‘èè‚¡åº”ç”¨GPAç‰¹æ®Šå¤„ç†: {list(special_stocks)}")

            # ç§»é™¤ä¸´æ—¶åˆ—
            df_result = df_with_industry.drop(['requires_special_gpa_handling', 'gpa_calculation_method'], axis=1)

            return df_result

        except Exception as e:
            self.logger.error(f"åº”ç”¨è¡Œä¸šç‰¹æ®Šå¤„ç†å¤±è´¥: {e}")
            return df

    def _get_industry_classification_pit(self, stock_codes: List[str], as_of_date: str) -> pd.DataFrame:
        """è·å–PITè¡Œä¸šåˆ†ç±»ä¿¡æ¯ (ä¼˜åŒ–ç‰ˆ - ä½¿ç”¨æ–°çš„è¡¨ç»“æ„å’Œå‡½æ•°)

        Args:
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            as_of_date: PITæˆªæ­¢æ—¥æœŸ

        Returns:
            è¡Œä¸šåˆ†ç±»ä¿¡æ¯DataFrame
        """
        try:
            # ä¼˜å…ˆä½¿ç”¨ä¼˜åŒ–åçš„æ‰¹é‡PITæŸ¥è¯¢å‡½æ•° (é»˜è®¤ä½¿ç”¨ç”³ä¸‡æ•°æ®)
            query = """
            SELECT * FROM get_industry_classification_batch_pit_optimized(%s, %s, 'sw')
            """

            df = self.context.query_dataframe(query, (stock_codes, as_of_date))

            # è‹¥æŸ¥è¯¢ç»“æœä¸ºç©ºï¼Œåˆ™å›é€€åˆ°ç›´æ¥åŸºäº Tushare è¡Œä¸šæˆå‘˜è¡¨çš„æŒ‰æ—¶ç‚¹æŸ¥è¯¢
            if df is None or df.empty:
                self.logger.warning(
                    "æœªé€šè¿‡ä¼˜åŒ–å‡½æ•°è·å–åˆ°è¡Œä¸šåˆ†ç±»ï¼Œå°è¯•å›é€€åˆ° Tushare è¡Œä¸šæˆå‘˜è¡¨æŸ¥è¯¢"
                )

                # 1) åœ¨ç±ï¼ˆas_of_date å½“æ—¥æœ‰æ•ˆï¼‰
                active_sql = """
                SELECT 
                    ts_code,
                    l1_name AS industry_level1,
                    l2_name AS industry_level2,
                    l3_name AS industry_level3,
                    l1_code AS industry_code1,
                    l2_code AS industry_code2,
                    l3_code AS industry_code3
                FROM tushare.index_swmember
                WHERE ts_code = ANY(%s)
                  AND l1_name IS NOT NULL
                  AND in_date <= %s
                  AND (out_date IS NULL OR out_date > %s)
                ORDER BY ts_code, in_date DESC
                """

                active_df = self.context.query_dataframe(
                    active_sql,
                    (stock_codes, as_of_date, as_of_date)
                )

                collected: dict[str, dict] = {}
                if active_df is not None and not active_df.empty:
                    # æ¯åªè‚¡ç¥¨å–åœ¨ç±çŠ¶æ€ä¸‹ in_date æœ€è¿‘çš„ä¸€æ¡ï¼ˆå·²æŒ‰ in_date DESCï¼‰
                    active_pick = active_df.groupby('ts_code').first().reset_index()
                    for _, r in active_pick.iterrows():
                        collected[r['ts_code']] = r.to_dict()

                # 2) è‹¥æ— åœ¨ç±ï¼Œåˆ™å– as_of_date ä¹‹å‰æœ€è¿‘çš„ä¸€æ¡ï¼ˆæœ€è¿‘çš„å†å²è®°å½•ï¼‰
                remaining = [c for c in stock_codes if c not in collected]
                if remaining:
                    latest_past_sql = """
                    SELECT DISTINCT ON (ts_code)
                        ts_code,
                        l1_name AS industry_level1,
                        l2_name AS industry_level2,
                        l3_name AS industry_level3,
                        l1_code AS industry_code1,
                        l2_code AS industry_code2,
                        l3_code AS industry_code3,
                        in_date
                    FROM tushare.index_swmember
                    WHERE ts_code = ANY(%s)
                      AND l1_name IS NOT NULL
                      AND in_date <= %s
                    ORDER BY ts_code, in_date DESC
                    """
                    latest_past_df = self.context.query_dataframe(
                        latest_past_sql,
                        (remaining, as_of_date)
                    )
                    if latest_past_df is not None and not latest_past_df.empty:
                        for _, r in latest_past_df.iterrows():
                            collected[r['ts_code']] = r.to_dict()

                # 3) è‹¥ä»æ— ï¼Œåˆ™å–è¯¥è‚¡ç¥¨æœ€æ—©çš„ä¸€æ¡ï¼ˆå…¨å±€æœ€æ—©åˆ†ç±»ï¼‰
                remaining2 = [c for c in stock_codes if c not in collected]
                if remaining2:
                    earliest_sql = """
                    SELECT DISTINCT ON (ts_code)
                        ts_code,
                        l1_name AS industry_level1,
                        l2_name AS industry_level2,
                        l3_name AS industry_level3,
                        l1_code AS industry_code1,
                        l2_code AS industry_code2,
                        l3_code AS industry_code3,
                        in_date
                    FROM tushare.index_swmember
                    WHERE ts_code = ANY(%s)
                      AND l1_name IS NOT NULL
                    ORDER BY ts_code, in_date ASC
                    """
                    earliest_df = self.context.query_dataframe(earliest_sql, (remaining2,))
                    if earliest_df is not None and not earliest_df.empty:
                        for _, r in earliest_df.iterrows():
                            collected[r['ts_code']] = r.to_dict()

                if collected:
                    latest = pd.DataFrame.from_records(list(collected.values()))

                    # åˆ¤å®šæ˜¯å¦ä¸ºé‡‘èè¡Œä¸šï¼Œåº”ç”¨ GPA ç‰¹æ®Šå¤„ç†æ ‡è®°
                    def is_financial_industry(l1: str, l2: str) -> bool:
                        text = f"{l1 or ''} {l2 or ''}"
                        keywords = ['é“¶è¡Œ', 'è¯åˆ¸', 'ä¿é™©', 'ä¿¡æ‰˜', 'æœŸè´§', 'åŸºé‡‘', 'é‡‘è', 'æŠ•èµ„', 'èµ„äº§ç®¡ç†', 'è´¢åŠ¡å…¬å¸']
                        return any(k in text for k in keywords)

                    latest['requires_special_gpa_handling'] = latest.apply(
                        lambda r: is_financial_industry(r.get('industry_level1'), r.get('industry_level2')), axis=1
                    )
                    latest['gpa_calculation_method'] = latest['requires_special_gpa_handling'].apply(
                        lambda x: 'null' if x else 'standard'
                    )
                    latest['data_source'] = 'sw'
                    latest['obs_date'] = as_of_date

                    # å¯¹é½è¿”å›åˆ—
                    cols = [
                        'ts_code', 'obs_date', 'data_source',
                        'industry_level1', 'industry_level2', 'industry_level3',
                        'requires_special_gpa_handling', 'gpa_calculation_method'
                    ]
                    for c in cols:
                        if c not in latest.columns:
                            latest[c] = None

                    df = latest[cols]
                else:
                    df = pd.DataFrame()

            return df if df is not None else pd.DataFrame()

        except Exception as e:
            self.logger.error(f"æŸ¥è¯¢PITè¡Œä¸šåˆ†ç±»å¤±è´¥: {e}")
            return pd.DataFrame()

    def _standardize_financial_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """è´¢åŠ¡æŒ‡æ ‡æ ‡å‡†åŒ–å¤„ç† (å¼‚å¸¸å€¼æˆªæ–­ + åˆ†ä½æ•°æ’å)

        Args:
            df: åŒ…å«è´¢åŠ¡æŒ‡æ ‡çš„DataFrame

        Returns:
            æ ‡å‡†åŒ–åçš„DataFrameï¼ŒåŒ…å«åŸå§‹å€¼å’Œæ ‡å‡†åŒ–åˆ†æ•°
        """
        if df.empty:
            return df

        df_result = df.copy()

        # æ ¸å¿ƒè´¢åŠ¡æŒ‡æ ‡åˆ—è¡¨
        indicators = ['gpa_ttm', 'roe_excl_ttm', 'roa_excl_ttm']

        for indicator in indicators:
            if indicator not in df.columns:
                self.logger.warning(f"ç¼ºå°‘æŒ‡æ ‡ {indicator}ï¼Œè·³è¿‡æ ‡å‡†åŒ–")
                continue

            # è·å–æœ‰æ•ˆæ•°æ® (æ’é™¤NaNå’ŒNone)ï¼Œå¹¶è½¬æ¢ä¸ºfloatç±»å‹
            try:
                # ç¡®ä¿æ•°æ®ç±»å‹ä¸ºfloatï¼Œé¿å…Decimalç±»å‹é—®é¢˜
                df[indicator] = pd.to_numeric(df[indicator], errors='coerce')
                valid_data = df[indicator].dropna()
            except Exception as e:
                self.logger.warning(f"æŒ‡æ ‡ {indicator} æ•°æ®ç±»å‹è½¬æ¢å¤±è´¥: {e}")
                df_result[f'{indicator}_standardized'] = np.nan
                continue

            if len(valid_data) == 0:
                self.logger.warning(f"æŒ‡æ ‡ {indicator} æ— æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡æ ‡å‡†åŒ–")
                df_result[f'{indicator}_standardized'] = np.nan
                continue

            # 1. å¼‚å¸¸å€¼æˆªæ–­ (ä½¿ç”¨1%å’Œ99%åˆ†ä½æ•°)
            try:
                p1 = float(valid_data.quantile(0.01))
                p99 = float(valid_data.quantile(0.99))
            except Exception as e:
                self.logger.warning(f"æŒ‡æ ‡ {indicator} åˆ†ä½æ•°è®¡ç®—å¤±è´¥: {e}")
                df_result[f'{indicator}_standardized'] = np.nan
                continue

            # å¯¹æ‰€æœ‰æ•°æ®è¿›è¡Œæˆªæ–­å¤„ç†
            clipped_values = df[indicator].clip(lower=p1, upper=p99)

            # 2. åˆ†ä½æ•°æ’åæ ‡å‡†åŒ– (0-100ç™¾åˆ†ä½)
            # ä½¿ç”¨rankæ–¹æ³•è®¡ç®—æ’åï¼Œç„¶åè½¬æ¢ä¸ºç™¾åˆ†ä½
            ranks = clipped_values.rank(method='average', na_option='keep')
            max_rank = ranks.max()

            if max_rank > 1:
                # è½¬æ¢ä¸º0-100ç™¾åˆ†ä½
                percentile_scores = (ranks - 1) / (max_rank - 1) * 100
            else:
                # å¦‚æœæ‰€æœ‰å€¼ç›¸åŒï¼Œè®¾ä¸º50åˆ†
                percentile_scores = pd.Series(50.0, index=df.index)

            # ä¿å­˜æ ‡å‡†åŒ–ç»“æœ
            df_result[f'{indicator}_standardized'] = percentile_scores

            # è®°å½•æ ‡å‡†åŒ–ç»Ÿè®¡ä¿¡æ¯
            self.logger.debug(f"{indicator} æ ‡å‡†åŒ–å®Œæˆ: "
                            f"æˆªæ–­èŒƒå›´ [{p1:.2f}, {p99:.2f}], "
                            f"ç™¾åˆ†ä½èŒƒå›´ [{percentile_scores.min():.1f}, {percentile_scores.max():.1f}]")

        return df_result

    def _calculate_p_score_vectorized_mvp(self, df: pd.DataFrame) -> pd.Series:
        """åŸºäºMVPæŒ‡æ ‡å‘é‡åŒ–è®¡ç®—Pè¯„åˆ† (ä¼˜åŒ–ç‰ˆ - æ ‡å‡†åŒ–è¿ç»­è¯„åˆ†åˆ¶)

        Args:
            df: åŒ…å«MVPè´¢åŠ¡æŒ‡æ ‡çš„DataFrame (å·²ç»è¿‡æ ‡å‡†åŒ–å¤„ç†)

        Returns:
            Pè¯„åˆ†Series (0-100è¿ç»­è¯„åˆ†)
        """
        if df.empty:
            return pd.Series(dtype=float)

        # æƒé‡é…ç½®
        weights = {
            'gpa_ttm_standardized': 0.40,      # GPAæƒé‡: 40%
            'roe_excl_ttm_standardized': 0.30, # ROEæƒé‡: 30%
            'roa_excl_ttm_standardized': 0.30  # ROAæƒé‡: 30%
        }

        # åˆå§‹åŒ–Pè¯„åˆ†
        p_scores = pd.Series(0.0, index=df.index)
        total_weight = 0.0

        # æŒ‰æƒé‡åˆæˆPå› å­è¯„åˆ†
        for indicator, weight in weights.items():
            if indicator in df.columns:
                # ä½¿ç”¨æ ‡å‡†åŒ–åçš„ç™¾åˆ†ä½åˆ†æ•° (0-100)
                indicator_scores = df[indicator].fillna(0)  # NaNå¡«å……ä¸º0åˆ†
                p_scores += indicator_scores * weight
                total_weight += weight
            else:
                self.logger.warning(f"ç¼ºå°‘æ ‡å‡†åŒ–æŒ‡æ ‡ {indicator}ï¼Œæƒé‡å°†é‡æ–°åˆ†é…")

        # å¦‚æœæœ‰ç¼ºå¤±æŒ‡æ ‡ï¼Œé‡æ–°å½’ä¸€åŒ–æƒé‡
        if total_weight > 0 and total_weight != 1.0:
            p_scores = p_scores / total_weight

        # ç¡®ä¿è¯„åˆ†åœ¨0-100èŒƒå›´å†…
        p_scores = p_scores.clip(lower=0, upper=100)

        return p_scores
    
    def _save_p_factors_mvp(self, p_factors: pd.DataFrame) -> None:
        """å¿«é€Ÿä¿å­˜På› å­ç»“æœï¼ˆå…ˆåˆ é™¤æ—§æ•°æ®ï¼Œå†æ’å…¥æ–°æ•°æ®ï¼‰

        Args:
            p_factors: På› å­ç»“æœDataFrame
        """
        if p_factors.empty:
            return

        # è·å–è®¡ç®—æ—¥æœŸ
        calc_date = p_factors['calc_date'].iloc[0]

        # å…ˆåˆ é™¤è¯¥è®¡ç®—æ—¥æœŸçš„æ‰€æœ‰æ—§æ•°æ®ï¼ˆç¡®ä¿æ’é™¤çš„è‚¡ç¥¨è¢«å®Œå…¨ç§»é™¤ï¼‰
        delete_sql = """
        DELETE FROM pgs_factors.p_factor
        WHERE calc_date = %s
        """

        self.context.db_manager.execute_sync(delete_sql, (calc_date,))
        self.logger.info(f"å·²åˆ é™¤è®¡ç®—æ—¥æœŸ {calc_date} çš„æ‰€æœ‰æ—§På› å­æ•°æ®")

        # æ„å»ºæ‰¹é‡æ’å…¥SQLï¼ˆä¸å†éœ€è¦ON CONFLICTï¼Œå› ä¸ºå·²åˆ é™¤æ—§æ•°æ®ï¼‰
        insert_sql = """
        INSERT INTO pgs_factors.p_factor
        (ts_code, calc_date, gpa, roe_excl, roa_excl, p_score, data_quality, ann_date, data_source)
        VALUES %s
        """
        
        # å‡†å¤‡æ•°æ® (éµå¾ªPITåŸåˆ™ï¼Œä¿æŒçœŸå®çš„ann_date)
        values = []
        for _, row in p_factors.iterrows():
            values.append((
                row['ts_code'],
                row['calc_date'],
                row['gpa'],
                row['roe_excl'],
                row['roa_excl'],
                row['p_score'],
                row['data_quality'],
                row.get('ann_date', row.get('end_date')),  # ä¼˜å…ˆä½¿ç”¨çœŸå®çš„ann_date
                row['data_source']
            ))
        
        # æ‰¹é‡æ’å…¥ - ä½¿ç”¨å®Œæ•´å­—æ®µçš„é€æ¡æ’å…¥æ–¹å¼ï¼ˆå·²åˆ é™¤æ—§æ•°æ®ï¼Œæ— éœ€ON CONFLICTï¼‰
        for _, row in p_factors.iterrows():
            insert_query = """
            INSERT INTO pgs_factors.p_factor (
                ts_code, calc_date, ann_date, end_date, data_source,
                p_score, p_rank,
                gpa, roe_excl, roa_excl,
                net_margin_ttm, operating_margin_ttm, roi_ttm,
                asset_turnover_ttm, equity_multiplier,
                debt_to_asset_ratio, equity_ratio,
                revenue_yoy_growth, n_income_yoy_growth, operate_profit_yoy_growth,
                data_quality, calculation_status
            ) VALUES (
                %s, %s, %s, %s, %s,
                %s, %s,
                %s, %s, %s,
                %s, %s, %s,
                %s, %s,
                %s, %s,
                %s, %s, %s,
                %s, %s
            )
            """

            # å‡†å¤‡å‚æ•°å€¼
            params = (
                row['ts_code'], row['calc_date'], row['ann_date'], row['end_date'], row['data_source'],
                row['p_score'], row['p_rank'],
                row['gpa'], row['roe_excl'], row['roa_excl'],
                row['net_margin_ttm'], row['operating_margin_ttm'], row['roi_ttm'],
                row['asset_turnover_ttm'], row['equity_multiplier'],
                row['debt_to_asset_ratio'], row['equity_ratio'],
                row['revenue_yoy_growth'], row['n_income_yoy_growth'], row['operate_profit_yoy_growth'],
                row['data_quality'], row['calculation_status']
            )

            # æ‰§è¡Œå•æ¡æ’å…¥
            self.context.db_manager.execute_sync(insert_query, params)
    
    def _log_performance_stats(self, success_count: int, failed_count: int) -> None:
        """è®°å½•æ€§èƒ½ç»Ÿè®¡"""
        stats = self.stats
        total_time = stats['total_time']
        
        self.logger.info("=" * 40)
        self.logger.info("MVPç‰ˆæœ¬På› å­è®¡ç®—å®Œæˆ")
        self.logger.info("=" * 40)
        self.logger.info(f"æˆåŠŸ: {success_count}, å¤±è´¥: {failed_count}")
        self.logger.info(f"æ€»è€—æ—¶: {total_time:.3f} ç§’")
        
        if total_time > 0:
            throughput = success_count / total_time
            self.logger.info(f"ååé‡: {throughput:.1f} åª/ç§’")
        
        self.logger.info(f"æ—¶é—´åˆ†å¸ƒ:")
        if total_time > 0:
            self.logger.info(f"  æŸ¥è¯¢æ—¶é—´: {stats['query_time']:.3f}s ({stats['query_time']/total_time*100:.1f}%)")
            self.logger.info(f"  è®¡ç®—æ—¶é—´: {stats['calculation_time']:.3f}s ({stats['calculation_time']/total_time*100:.1f}%)")
            self.logger.info(f"  ä¿å­˜æ—¶é—´: {stats['save_time']:.3f}s ({stats['save_time']/total_time*100:.1f}%)")
    
    def check_mvp_data_availability(self, calc_date: str, stock_codes: List[str]) -> Dict[str, Any]:
        """æ£€æŸ¥MVPé¢„è®¡ç®—æ•°æ®çš„å¯ç”¨æ€§
        
        Args:
            calc_date: è®¡ç®—æ—¥æœŸ
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            
        Returns:
            æ•°æ®å¯ç”¨æ€§ç»Ÿè®¡
        """
        query = """
        SELECT 
            COUNT(*) as total_count,
            COUNT(CASE WHEN data_quality = 'high' THEN 1 END) as high_quality_count,
            COUNT(CASE WHEN data_quality = 'normal' THEN 1 END) as normal_quality_count,
            COUNT(CASE WHEN data_quality = 'low' THEN 1 END) as low_quality_count,
            COUNT(CASE WHEN calculation_status = 'success' THEN 1 END) as success_count
        FROM pgs_factors.pit_financial_indicators
        WHERE ann_date <= %s  -- ä¿®æ­£ä¸ºPITåŸåˆ™
        AND ts_code = ANY(%s)
        """
        
        result_df = self.context.query_dataframe(query, (calc_date, stock_codes))
        result = result_df.iloc[0].tolist() if result_df is not None and not result_df.empty else None
        
        if result:
            total_count = result[0]
            coverage_rate = (total_count / len(stock_codes) * 100) if len(stock_codes) > 0 else 0
            
            return {
                'total_stocks': len(stock_codes),
                'available_count': total_count,
                'coverage_rate': coverage_rate,
                'high_quality_count': result[1],
                'normal_quality_count': result[2],
                'low_quality_count': result[3],
                'success_count': result[4]
            }
        
        return {
            'total_stocks': len(stock_codes),
            'available_count': 0,
            'coverage_rate': 0,
            'high_quality_count': 0,
            'normal_quality_count': 0,
            'low_quality_count': 0,
            'success_count': 0
        }

    def check_mvp_data_availability_pit(self, as_of_date: str, stock_codes: List[str]) -> Dict[str, Any]:
        """æ£€æŸ¥MVPé¢„è®¡ç®—æ•°æ®çš„å¯ç”¨æ€§ (åŸºäºPITåŸåˆ™)

        Args:
            as_of_date: PITæˆªæ­¢æ—¥æœŸ
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨

        Returns:
            æ•°æ®å¯ç”¨æ€§ç»Ÿè®¡
        """
        query = """
        SELECT
            COUNT(*) as total_count,
            COUNT(CASE WHEN data_quality = 'high' THEN 1 END) as high_quality_count,
            COUNT(CASE WHEN data_quality = 'normal' THEN 1 END) as normal_quality_count,
            COUNT(CASE WHEN data_quality = 'outlier_high' THEN 1 END) as outlier_high_count,
            COUNT(CASE WHEN data_quality = 'outlier_low' THEN 1 END) as outlier_low_count,
            COUNT(CASE WHEN calculation_status = 'success' THEN 1 END) as success_count,
            COUNT(DISTINCT ts_code) as unique_stocks
        FROM pgs_factors.pit_financial_indicators
        WHERE ann_date <= %s  -- PITåŸåˆ™: åªçœ‹å·²å…¬å‘Šçš„æ•°æ®
        AND ts_code = ANY(%s)
        AND calculation_status = 'success'
        """

        try:
            result_df = self.context.query_dataframe(query, (as_of_date, stock_codes))
            result = result_df.iloc[0].tolist() if result_df is not None and not result_df.empty else None

            if result and len(result) >= 7:
                total_records = result[0] if result[0] is not None else 0
                high_quality = result[1] if result[1] is not None else 0
                normal_quality = result[2] if result[2] is not None else 0
                outlier_high = result[3] if result[3] is not None else 0
                outlier_low = result[4] if result[4] is not None else 0
                success_count = result[5] if result[5] is not None else 0
                unique_stocks = result[6] if result[6] is not None else 0

                coverage_rate = (unique_stocks / len(stock_codes) * 100) if len(stock_codes) > 0 else 0

                return {
                    'pit_date': as_of_date,
                    'total_stocks': len(stock_codes),
                    'available_stocks': unique_stocks,
                    'coverage_rate': coverage_rate,
                    'total_records': total_records,
                    'high_quality_count': high_quality,
                    'normal_quality_count': normal_quality,
                    'outlier_high_count': outlier_high,
                    'outlier_low_count': outlier_low,
                    'success_count': success_count
                }
        except Exception as e:
            self.logger.error(f"æŸ¥è¯¢MVPæ•°æ®å¯ç”¨æ€§å¤±è´¥: {e}")

        return {
            'pit_date': as_of_date,
            'total_stocks': len(stock_codes),
            'available_stocks': 0,
            'coverage_rate': 0,
            'total_records': 0,
            'high_quality_count': 0,
            'normal_quality_count': 0,
            'outlier_high_count': 0,
            'outlier_low_count': 0,
            'success_count': 0
        }
