#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ç”Ÿäº§çº§Gå› å­è®¡ç®—å™¨ (PITåŸåˆ™ + æˆé•¿å› å­åˆ†æ)
=======================================================

åŸºäºPå› å­æ•°æ®è®¡ç®—Gå› å­ï¼ˆæˆé•¿èƒ½åŠ›æŒ‡æ ‡ï¼‰ï¼Œä¸¥æ ¼éµå¾ªPoint-in-TimeåŸåˆ™ã€‚
ç®€åŒ–ç‰ˆï¼šç§»é™¤æƒé‡ç³»ç»Ÿï¼Œç»Ÿä¸€ä½¿ç”¨æƒé‡1.0ï¼Œæé«˜å› å­å¯è§£é‡Šæ€§å’Œç¨³å®šæ€§ã€‚

ğŸš€ æ ¸å¿ƒç‰¹æ€§ (v1.1):
1. **å››ä¸ªæˆé•¿å­å› å­**: æ•ˆç‡æƒŠå–œã€æ•ˆç‡åŠ¨é‡ã€è¥æ”¶åŠ¨é‡ã€åˆ©æ¶¦åŠ¨é‡
2. **ç®€åŒ–æƒé‡ç³»ç»Ÿ**: ç»Ÿä¸€æƒé‡1.0ï¼Œæ¶ˆé™¤æ•°æ®æºåå¥½
3. **å‰ç»æ€§æˆé•¿æŒ‡æ ‡**: åŸºäºForecastæ•°æ®çš„å¢é•¿é¢„æœŸ
4. **ç™¾åˆ†ä½æ’åç³»ç»Ÿ**: 0-100åˆ†æ ‡å‡†åŒ–è¯„åˆ†
5. **æ•°æ®æºä¿¡æ¯ä¿ç•™**: è®°å½•æ•°æ®æºç±»å‹ç”¨äºåˆ†æå’Œç›‘æ§

ğŸ“Š Gå› å­è®¡ç®—å…¬å¼:
- G_Efficiency_Surprise = Î”P_score_YoY / Std(Î”P_score_YoY)
- G_Efficiency_Momentum = Î”P_score_YoY
- G_Revenue_Momentum = Revenue_YoY_Growth
- G_Profit_Momentum = N_Income_YoY_Growth
- Final_G_Score = 0.25Ã—Rank_ES + 0.25Ã—Rank_EM + 0.25Ã—Rank_RM + 0.25Ã—Rank_PM

ğŸ¯ PITåŸåˆ™æ ¸å¿ƒ:
1. åœ¨æŒ‡å®šæ—¶ç‚¹(as_of_date)ï¼Œåªèƒ½çœ‹åˆ°è¯¥æ—¶ç‚¹ä¹‹å‰æˆ–å½“æ—¥å…¬å‘Šçš„æ•°æ®
2. åŸºäºPå› å­å†å²æ•°æ®è®¡ç®—åŒæ¯”å¢é•¿å’ŒåŠ¨é‡æŒ‡æ ‡
3. æŸ¥è¯¢æ¡ä»¶: calc_date <= as_of_date

ğŸ”§ æŠ€æœ¯ä¼˜åŒ–:
1. åŸºäºå·²è®¡ç®—çš„På› å­æ•°æ®ï¼Œé¿å…é‡å¤è®¡ç®—
2. å‘é‡åŒ–è®¡ç®—å’Œæ‰¹é‡å¤„ç†
3. æ™ºèƒ½ç¼“å­˜å’Œæ•°æ®é¢„åŠ è½½
4. ç®€åŒ–æƒé‡ç³»ç»Ÿï¼Œæé«˜è®¡ç®—ä¸€è‡´æ€§

Author: AI Assistant
Date: 2025-09-01 (v1.1 - ç®€åŒ–æƒé‡ç³»ç»Ÿ)
"""

import logging
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import List, Optional, Dict, Any
import time

from research.tools.context import ResearchContext


class ProductionGFactorCalculator:
    """ç”Ÿäº§çº§Gå› å­è®¡ç®—å™¨ (åŸºäºPå› å­æ•°æ®çš„é«˜æ€§èƒ½å®ç°)"""
    
    def __init__(self, context: ResearchContext, config: Optional[Dict[str, Any]] = None):
        """åˆå§‹åŒ–è®¡ç®—å™¨

        Args:
            context: ResearchContextå®ä¾‹
            config: é…ç½®å‚æ•°å­—å…¸ï¼ˆå¯é€‰ï¼‰
        """
        self.context = context
        self.db_manager = context.db_manager
        self.logger = self._setup_logger()
        self.config = config or {}

        # ç»Ÿä¸€æƒé‡ä¸º1.0ï¼Œç®€åŒ–è®¡ç®—é€»è¾‘
        self.timeliness_weights = {'express': 1.0, 'forecast': 1.0, 'report': 1.0}

        # æ•ˆç‡æƒŠå–œï¼ˆESï¼‰ç›¸å…³å¯é…ç½®å‚æ•°
        self.es_params = {
            'yoy_interval_weeks': int(self.config.get('yoy_interval_weeks', 52)),
            'yoy_match_tolerance_days': int(self.config.get('yoy_match_tolerance_days', 45)),
            'min_yoy_pairs_for_std': int(self.config.get('min_yoy_pairs_for_std', 8)),
            'min_yoy_pairs_soft': int(self.config.get('min_yoy_pairs_soft', 3)),
            'enable_pooled_std_fallback': bool(self.config.get('enable_pooled_std_fallback', True))
        }
        self.logger.info(
            f"æ•ˆç‡æƒŠå–œå‚æ•°: {self.es_params}"
        )
        
        # å­å› å­æƒé‡é…ç½® (ç­‰æƒé‡åˆæˆ)
        self.subfactor_weights = {
            'efficiency_surprise': 0.25,
            'efficiency_momentum': 0.25,
            'revenue_momentum': 0.25,
            'profit_momentum': 0.25
        }
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            'query_time': 0,
            'calculation_time': 0,
            'save_time': 0,
            'total_time': 0
        }
    
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
        logger = logging.getLogger('ProductionGFactorCalculator')
        logger.setLevel(logging.INFO)
        return logger
    
    def calculate_g_factors_pit(
        self,
        as_of_date: str,
        stock_codes: List[str]
    ) -> Dict[str, Any]:
        """åŸºäºPITåŸåˆ™çš„Gå› å­è®¡ç®—
        
        Args:
            as_of_date: PITæˆªæ­¢æ—¥æœŸ (åœ¨æ­¤æ—¶ç‚¹èƒ½çœ‹åˆ°çš„æ‰€æœ‰å·²å…¬å‘Šæ•°æ®)
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
        
        Returns:
            è®¡ç®—ç»“æœç»Ÿè®¡
        """
        start_time = time.time()
        
        self.logger.info(f"å¼€å§‹åŸºäºPITåŸåˆ™çš„Gå› å­è®¡ç®—: {as_of_date}")
        self.logger.info(f"è‚¡ç¥¨æ•°é‡: {len(stock_codes)}")
        
        # 1. æŸ¥è¯¢På› å­å†å²æ•°æ® (ä¸¥æ ¼éµå¾ªPITåŸåˆ™)
        query_start = time.time()
        p_factor_data = self._get_p_factor_historical_data_pit(as_of_date, stock_codes)
        self.stats['query_time'] = time.time() - query_start
        
        if p_factor_data.empty:
            self.logger.warning(f"åœ¨æ—¶ç‚¹ {as_of_date} æœªæ‰¾åˆ°På› å­å†å²æ•°æ®")
            return {'success_count': 0, 'failed_count': len(stock_codes)}
        
        # 2. è®¡ç®—Gå› å­ (åŸºäºPå› å­æ•°æ®)
        calc_start = time.time()
        g_factor_results = self._calculate_g_factors_from_p_data_pit(p_factor_data, as_of_date)
        self.stats['calculation_time'] = time.time() - calc_start
        
        if g_factor_results.empty:
            self.logger.warning(f"Gå› å­è®¡ç®—ç»“æœä¸ºç©º")
            return {'success_count': 0, 'failed_count': len(stock_codes)}
        
        # 3. ä¿å­˜Gå› å­ç»“æœ
        save_start = time.time()
        success_count = self._save_g_factor_results_pit(g_factor_results, as_of_date)
        self.stats['save_time'] = time.time() - save_start
        
        # 4. ç»Ÿè®¡ç»“æœ
        self.stats['total_time'] = time.time() - start_time
        failed_count = len(stock_codes) - success_count
        
        self._log_performance_stats(success_count, failed_count)
        
        return {
            'success_count': success_count,
            'failed_count': failed_count,
            'total_time': self.stats['total_time'],
            'performance_stats': self.stats.copy()
        }
    
    def _get_p_factor_historical_data_pit(self, as_of_date: str, stock_codes: List[str]) -> pd.DataFrame:
        """åŸºäºPITåŸåˆ™æŸ¥è¯¢På› å­å†å²æ•°æ®
        
        Args:
            as_of_date: PITæˆªæ­¢æ—¥æœŸ
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
        
        Returns:
            På› å­å†å²æ•°æ®DataFrame
        """
        try:
            # è®¡ç®—éœ€è¦çš„å†å²æ—¶é—´èŒƒå›´ (è‡³å°‘éœ€è¦2å¹´æ•°æ®ç”¨äºåŒæ¯”è®¡ç®—)
            as_of_datetime = datetime.strptime(as_of_date, '%Y-%m-%d')
            start_date = (as_of_datetime - timedelta(days=730)).strftime('%Y-%m-%d')  # 2å¹´å‰
            
            self.logger.info(f"æŸ¥è¯¢På› å­å†å²æ•°æ®: {start_date} ~ {as_of_date}")
            
            # æŸ¥è¯¢På› å­å†å²æ•°æ® (PITåŸåˆ™: calc_date <= as_of_date)
            # é‡è¦ï¼šåŒ…å«è¥æ”¶å’Œåˆ©æ¶¦å¢é•¿å­—æ®µç”¨äºGå› å­è®¡ç®—
            query = """
            SELECT
                ts_code,
                calc_date,
                p_score,
                data_source,
                ann_date,
                gpa,
                roe_excl,
                roa_excl,
                revenue_yoy_growth,
                n_income_yoy_growth
            FROM pgs_factors.p_factor
            WHERE ts_code = ANY(%s)
              AND calc_date BETWEEN %s AND %s  -- å·²åŒ…å«ä¸Šç•Œas_of_date
              AND p_score IS NOT NULL
            ORDER BY ts_code, calc_date
            """
            
            result = self.context.query_dataframe(query, (stock_codes, start_date, as_of_date))
            
            if result is not None and not result.empty:
                self.logger.info(f"è·å–åˆ° {len(result)} æ¡På› å­å†å²è®°å½•")
                return result
            else:
                self.logger.warning("æœªæ‰¾åˆ°På› å­å†å²æ•°æ®")
                return pd.DataFrame()
                
        except Exception as e:
            self.logger.error(f"æŸ¥è¯¢På› å­å†å²æ•°æ®å¤±è´¥ (PITæ—¶ç‚¹: {as_of_date}): {e}")
            return pd.DataFrame()
    
    def _calculate_g_factors_from_p_data_pit(
        self,
        p_factor_data: pd.DataFrame,
        as_of_date: str
    ) -> pd.DataFrame:
        """åŸºäºPå› å­æ•°æ®å’ŒPITåŸåˆ™è®¡ç®—Gå› å­
        
        Args:
            p_factor_data: På› å­å†å²æ•°æ®
            as_of_date: PITæˆªæ­¢æ—¥æœŸ
        
        Returns:
            Gå› å­ç»“æœDataFrame
        """
        if p_factor_data.empty:
            return pd.DataFrame()
        
        self.logger.info(f"å¼€å§‹è®¡ç®—Gå› å­ï¼ŒåŸºäº {len(p_factor_data)} æ¡På› å­è®°å½•")
        
        # è½¬æ¢æ—¥æœŸåˆ—
        p_factor_data['calc_date'] = pd.to_datetime(p_factor_data['calc_date'])
        
        # æŒ‰è‚¡ç¥¨åˆ†ç»„è®¡ç®—Gå› å­
        g_factor_results = []
        
        for ts_code, group in p_factor_data.groupby('ts_code'):
            try:
                # è·å–æœ€æ–°çš„På› å­è®°å½• (as_of_dateå½“æ—¥æˆ–ä¹‹å‰æœ€è¿‘çš„è®°å½•)
                latest_record = self._get_latest_p_factor_record(group, as_of_date)
                
                if latest_record is None:
                    continue
                
                # è®¡ç®—Gå› å­å­æŒ‡æ ‡
                g_factors = self._calculate_g_subfactors(group, latest_record, as_of_date)
                
                if g_factors:
                    g_factor_results.append(g_factors)
                    
            except Exception as e:
                self.logger.error(f"è®¡ç®— {ts_code} Gå› å­å¤±è´¥: {e}")
                continue
        
        if not g_factor_results:
            return pd.DataFrame()
        
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(g_factor_results)
        
        # è®¡ç®—æ¨ªæˆªé¢æ’å (0-100ç™¾åˆ†ä½)
        df = self._calculate_cross_sectional_rankings(df)
        
        # è®¡ç®—æœ€ç»ˆGè¯„åˆ†
        df['g_score'] = self._calculate_final_g_score(df)
        
        # æ·»åŠ è®¡ç®—æ—¥æœŸ
        df['calc_date'] = as_of_date
        
        self.logger.info(f"Gå› å­è®¡ç®—å®Œæˆï¼Œå…± {len(df)} åªè‚¡ç¥¨")

        return df

    def _get_latest_p_factor_record(self, group: pd.DataFrame, as_of_date: str) -> Optional[pd.Series]:
        """è·å–æŒ‡å®šæ—¥æœŸçš„æœ€æ–°På› å­è®°å½•

        Args:
            group: å•åªè‚¡ç¥¨çš„På› å­å†å²æ•°æ®
            as_of_date: PITæˆªæ­¢æ—¥æœŸ

        Returns:
            æœ€æ–°çš„På› å­è®°å½•
        """
        as_of_datetime = pd.to_datetime(as_of_date)

        # ç­›é€‰PITæ—¶ç‚¹ä¹‹å‰æˆ–å½“æ—¥çš„è®°å½•
        valid_records = group[group['calc_date'] <= as_of_datetime]

        if valid_records.empty:
            return None

        # è¿”å›æœ€æ–°çš„è®°å½•
        return valid_records.loc[valid_records['calc_date'].idxmax()]

    def _calculate_g_subfactors(
        self,
        group: pd.DataFrame,
        latest_record: pd.Series,
        as_of_date: str
    ) -> Optional[Dict[str, Any]]:
        """è®¡ç®—Gå› å­çš„å››ä¸ªå­å› å­

        Args:
            group: å•åªè‚¡ç¥¨çš„På› å­å†å²æ•°æ®
            latest_record: æœ€æ–°çš„På› å­è®°å½•
            as_of_date: PITæˆªæ­¢æ—¥æœŸ

        Returns:
            Gå› å­å­æŒ‡æ ‡å­—å…¸
        """
        try:
            ts_code = latest_record['ts_code']
            data_source = latest_record['data_source']

            # ç»Ÿä¸€æƒé‡ä¸º1.0ï¼Œç®€åŒ–è®¡ç®—é€»è¾‘
            timeliness_weight = 1.0

            # è®¡ç®—åŒæ¯”æ•°æ® (1å¹´å‰çš„På› å­æ•°æ®)
            yoy_data = self._get_yoy_p_factor_data(group, latest_record['calc_date'])

            # 1. æ•ˆç‡æƒŠå–œå› å­ (G_Efficiency_Surprise)
            efficiency_surprise = self._calculate_efficiency_surprise(
                group, latest_record, yoy_data
            )

            # 2. æ•ˆç‡åŠ¨é‡å› å­ (G_Efficiency_Momentum)
            efficiency_momentum = self._calculate_efficiency_momentum(
                latest_record, yoy_data
            )

            # 3. è¥æ”¶åŠ¨é‡å› å­ (G_Revenue_Momentum) - åŸºäºè´¢åŠ¡æŒ‡æ ‡
            revenue_momentum = self._calculate_revenue_momentum(
                group, latest_record
            )

            # 4. åˆ©æ¶¦åŠ¨é‡å› å­ (G_Profit_Momentum) - åŸºäºè´¢åŠ¡æŒ‡æ ‡
            profit_momentum = self._calculate_profit_momentum(
                group, latest_record
            )

            return {
                'ts_code': ts_code,
                'data_source': data_source,
                'data_timeliness_weight': timeliness_weight,  # ç»Ÿä¸€ä¸º1.0
                'g_efficiency_surprise': efficiency_surprise,
                'g_efficiency_momentum': efficiency_momentum,
                'g_revenue_momentum': revenue_momentum,
                'g_profit_momentum': profit_momentum,
                'ann_date': latest_record['ann_date'],
                'calculation_status': 'success'
            }

        except Exception as e:
            self.logger.error(f"è®¡ç®—Gå› å­å­æŒ‡æ ‡å¤±è´¥: {e}")
            return None

    def _get_yoy_p_factor_data(self, group: pd.DataFrame, current_date: pd.Timestamp) -> Optional[pd.Series]:
        """è·å–åŒæ¯”På› å­æ•°æ® (1å¹´å‰)

        Args:
            group: å•åªè‚¡ç¥¨çš„På› å­å†å²æ•°æ®
            current_date: å½“å‰è®¡ç®—æ—¥æœŸ

        Returns:
            1å¹´å‰çš„På› å­è®°å½•
        """
        # è®¡ç®—1å¹´å‰çš„æ—¥æœŸèŒƒå›´ (å…è®¸Â±30å¤©çš„è¯¯å·®)
        target_date = current_date - pd.DateOffset(years=1)
        start_range = target_date - pd.DateOffset(days=30)
        end_range = target_date + pd.DateOffset(days=30)

        # æŸ¥æ‰¾1å¹´å‰çš„æ•°æ®
        yoy_candidates = group[
            (group['calc_date'] >= start_range) &
            (group['calc_date'] <= end_range)
        ]

        if yoy_candidates.empty:
            return None

        # è¿”å›æœ€æ¥è¿‘ç›®æ ‡æ—¥æœŸçš„è®°å½•
        yoy_candidates = yoy_candidates.copy()
        yoy_candidates['date_diff'] = abs(yoy_candidates['calc_date'] - target_date)
        return yoy_candidates.loc[yoy_candidates['date_diff'].idxmin()]

    def _build_yoy_delta_series_52w(self, group: pd.DataFrame) -> List[float]:
        """æ„å»ºåŸºäºå¯é…ç½®é—´éš”å‘¨æ•°(é»˜è®¤52å‘¨)çš„Î”P_score(YoY)åºåˆ—

        è¯´æ˜:
        - éå†è¯¥è‚¡ç¥¨çš„æ¯ä¸ªè®¡ç®—æ—¶ç‚¹ tï¼Œæ‰¾åˆ°æœ€æ¥è¿‘ t-intervalWeeks çš„è®°å½•ä½œä¸ºåŒæ¯”åŸºå‡†
        - é‡‡ç”¨ Â±toleranceDays å®¹å¿çª—å£ï¼Œä¸ _get_yoy_p_factor_data çš„å£å¾„ä¿æŒä¸€è‡´
        - ä»…å¯¹å­˜åœ¨åŒæ¯”åŸºå‡†çš„æ—¶ç‚¹è®¡å…¥åºåˆ—
        """
        if group is None or group.empty:
            return []

        sorted_group = group.sort_values('calc_date')
        deltas: List[float] = []

        for _, current in sorted_group.iterrows():
            try:
                interval_weeks = max(1, int(self.es_params.get('yoy_interval_weeks', 52)))
                tolerance_days = max(0, int(self.es_params.get('yoy_match_tolerance_days', 45)))

                target_date = current['calc_date'] - pd.DateOffset(weeks=interval_weeks)
                start_range = target_date - pd.DateOffset(days=tolerance_days)
                end_range = target_date + pd.DateOffset(days=tolerance_days)

                yoy_candidates = sorted_group[
                    (sorted_group['calc_date'] >= start_range) &
                    (sorted_group['calc_date'] <= end_range)
                ]

                if yoy_candidates.empty:
                    continue

                candidates = yoy_candidates.copy()
                candidates['date_diff'] = abs(candidates['calc_date'] - target_date)
                yoy_rec = candidates.loc[candidates['date_diff'].idxmin()]

                # ä»…å½“ä¸¤ç«¯å‡æœ‰æœ‰æ•ˆ p_score æ—¶çº³å…¥åºåˆ—
                if pd.notna(current.get('p_score')) and pd.notna(yoy_rec.get('p_score')):
                    delta = float(current['p_score']) - float(yoy_rec['p_score'])
                    deltas.append(delta)
            except Exception:
                # å•ç‚¹å¼‚å¸¸å¿½ç•¥ï¼Œç»§ç»­ç´¯ç§¯å…¶å®ƒæ—¶ç‚¹
                continue

        return deltas

    def _calculate_efficiency_surprise(
        self,
        group: pd.DataFrame,
        latest_record: pd.Series,
        yoy_data: Optional[pd.Series],
        pooled_yoy_std: Optional[float] = None
    ) -> float:
        """è®¡ç®—æ•ˆç‡æƒŠå–œå› å­

        å…¬å¼: Î”P_score_YoY / Std(Î”P_score_YoY)
        """
        if yoy_data is None:
            return np.nan

        # è®¡ç®—Pè¯„åˆ†åŒæ¯”å˜åŒ–
        delta_p_score = float(latest_record['p_score']) - float(yoy_data['p_score'])

        # ä½¿ç”¨é…ç½®çš„YoYå·®åºåˆ—ä½œä¸ºåˆ†æ¯åŸºç¡€
        p_score_changes = self._build_yoy_delta_series_52w(group)
        n_samples = len(p_score_changes)

        hard_n = max(1, int(self.es_params.get('min_yoy_pairs_for_std', 8)))
        soft_n = max(1, int(self.es_params.get('min_yoy_pairs_soft', 3)))
        use_pooled = bool(self.es_params.get('enable_pooled_std_fallback', True))

        if n_samples >= hard_n:
            std_delta = np.std(p_score_changes)
            if std_delta > 0:
                return (delta_p_score / std_delta) * 1.0
            # std=0 åˆ™å›é€€åˆ°æœªå½’ä¸€åŒ–
            return delta_p_score * 1.0

        if n_samples >= soft_n:
            std_delta = np.std(p_score_changes)
            if std_delta > 0:
                # è½¯é˜ˆå€¼ä¸‹æŒ‰æ ·æœ¬å æ¯”è¿›è¡Œè¡°å‡ï¼ŒæŠ‘åˆ¶å™ªå£°
                scale = np.sqrt(n_samples / float(hard_n))
                return (delta_p_score / std_delta) * scale * 1.0
            return delta_p_score * 1.0

        # æ ·æœ¬æ•°ä¸è¶³softé˜ˆå€¼æ—¶ï¼Œå°è¯•æ¨ªæˆªé¢æ± åŒ–æ ‡å‡†å·®
        if use_pooled and pooled_yoy_std is not None and pooled_yoy_std > 0:
            return (delta_p_score / pooled_yoy_std) * 1.0

        # æœ€åå›é€€ï¼šæœªå½’ä¸€åŒ–æƒŠå–œ
        return delta_p_score * 1.0

    def _calculate_efficiency_momentum(
        self,
        latest_record: pd.Series,
        yoy_data: Optional[pd.Series]
    ) -> float:
        """è®¡ç®—æ•ˆç‡åŠ¨é‡å› å­

        å…¬å¼: Î”P_score_YoY
        """
        if yoy_data is None:
            return np.nan

        delta_p_score = float(latest_record['p_score']) - float(yoy_data['p_score'])
        return delta_p_score * 1.0

    def _calculate_revenue_momentum(
        self,
        group: pd.DataFrame,
        latest_record: pd.Series
    ) -> float:
        """è®¡ç®—è¥æ”¶åŠ¨é‡å› å­

        åŸºäºè¥æ”¶åŒæ¯”å¢é•¿ç‡ (revenue_yoy_growth)
        """
        try:
            # ç›´æ¥ä½¿ç”¨æœ€æ–°è®°å½•çš„è¥æ”¶åŒæ¯”å¢é•¿ç‡
            revenue_growth = latest_record.get('revenue_yoy_growth')

            if revenue_growth is None or pd.isna(revenue_growth):
                return np.nan

            # è¥æ”¶å¢é•¿ç‡å·²ç»æ˜¯ç™¾åˆ†æ¯”ï¼Œç›´æ¥ä½¿ç”¨
            return float(revenue_growth) * 1.0

        except Exception as e:
            self.logger.warning(f"è®¡ç®—è¥æ”¶åŠ¨é‡å¤±è´¥: {e}")
            return 0.0

    def _calculate_profit_momentum(
        self,
        group: pd.DataFrame,
        latest_record: pd.Series
    ) -> float:
        """è®¡ç®—åˆ©æ¶¦åŠ¨é‡å› å­

        åŸºäºå‡€åˆ©æ¶¦åŒæ¯”å¢é•¿ç‡ (n_income_yoy_growth)
        """
        try:
            # ç›´æ¥ä½¿ç”¨æœ€æ–°è®°å½•çš„å‡€åˆ©æ¶¦åŒæ¯”å¢é•¿ç‡
            profit_growth = latest_record.get('n_income_yoy_growth')

            if profit_growth is None or pd.isna(profit_growth):
                return np.nan

            # åˆ©æ¶¦å¢é•¿ç‡å·²ç»æ˜¯ç™¾åˆ†æ¯”ï¼Œç›´æ¥ä½¿ç”¨
            return float(profit_growth) * 1.0

        except Exception as e:
            self.logger.warning(f"è®¡ç®—åˆ©æ¶¦åŠ¨é‡å¤±è´¥: {e}")
            return 0.0

    def _calculate_cross_sectional_rankings(self, df: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—æ¨ªæˆªé¢æ’å (0-100ç™¾åˆ†ä½)

        Args:
            df: Gå› å­æ•°æ®DataFrame

        Returns:
            åŒ…å«æ’åçš„DataFrame
        """
        # è®¡ç®—å„å­å› å­çš„ç™¾åˆ†ä½æ’å (0-100)
        # å¯¹äºç¼ºå¤±å€¼ï¼ˆNaNï¼‰ï¼Œä½¿ç”¨na_option='keep'ä¿æŒä¸ºNaNï¼Œä¸å‚ä¸æ’å
        # è¿™æ ·ç©ºå€¼å› å­åœ¨æœ€ç»ˆGè¯„åˆ†è®¡ç®—ä¸­ä¼šè¢«æ­£ç¡®æ’é™¤ï¼Œæƒé‡åŠ¨æ€è°ƒæ•´
        
        # æ•ˆç‡æƒŠå–œæ’åï¼šåªå¯¹æœ‰æ•ˆå€¼è¿›è¡Œæ’å
        df['rank_es'] = df['g_efficiency_surprise'].rank(pct=True, na_option='keep') * 100
        
        # æ•ˆç‡åŠ¨é‡æ’åï¼šåªå¯¹æœ‰æ•ˆå€¼è¿›è¡Œæ’å
        df['rank_em'] = df['g_efficiency_momentum'].rank(pct=True, na_option='keep') * 100
        
        # è¥æ”¶åŠ¨é‡æ’åï¼šåªå¯¹æœ‰æ•ˆå€¼è¿›è¡Œæ’å
        df['rank_rm'] = df['g_revenue_momentum'].rank(pct=True, na_option='keep') * 100
        
        # åˆ©æ¶¦åŠ¨é‡æ’åï¼šåªå¯¹æœ‰æ•ˆå€¼è¿›è¡Œæ’å
        df['rank_pm'] = df['g_profit_momentum'].rank(pct=True, na_option='keep') * 100

        # æ³¨æ„ï¼šä¸å†å°†NaNå¡«å……ä¸º0ï¼Œä¿æŒNaNçŠ¶æ€
        # è¿™æ ·åœ¨æœ€ç»ˆGè¯„åˆ†è®¡ç®—ä¸­ï¼Œç©ºå€¼å› å­ä¼šè¢«æ­£ç¡®æ’é™¤ï¼Œæƒé‡åŠ¨æ€è°ƒæ•´

        return df

    def _calculate_final_g_score(self, df: pd.DataFrame) -> pd.Series:
        """è®¡ç®—æœ€ç»ˆGè¯„åˆ†

        åŠ¨æ€æƒé‡å…¬å¼: å¯¹äºç©ºå€¼å› å­ï¼Œæƒé‡èµ‹ä¸º0
        Final_G_Score = (w1Ã—Rank_ESÃ—logic_ES + w2Ã—Rank_EMÃ—logic_EM + w3Ã—Rank_RMÃ—logic_RM + w4Ã—Rank_PMÃ—logic_PM) / (w1Ã—logic_ES + w2Ã—logic_EM + w3Ã—logic_RM + w4Ã—logic_PM)
        
        å…¶ä¸­ logic_X = 1 if Rank_X is not null else 0
        """
        # æ£€æŸ¥å„å­å› å­æ˜¯å¦æœ‰æœ‰æ•ˆå€¼ï¼ˆéç©ºå€¼ï¼‰
        has_es = df['g_efficiency_surprise'].notna()
        has_em = df['g_efficiency_momentum'].notna()
        has_rm = df['g_revenue_momentum'].notna()
        has_pm = df['g_profit_momentum'].notna()
        
        # è®¡ç®—åŠ¨æ€æƒé‡
        w_es = self.subfactor_weights['efficiency_surprise']
        w_em = self.subfactor_weights['efficiency_momentum']
        w_rm = self.subfactor_weights['revenue_momentum']
        w_pm = self.subfactor_weights['profit_momentum']
        
        # åˆå§‹åŒ–ç»“æœSeries
        g_score = pd.Series(index=df.index, dtype=float)
        
        # é€è¡Œè®¡ç®—Gè¯„åˆ†ï¼Œç¡®ä¿æ­£ç¡®å¤„ç†ç©ºå€¼
        for idx in df.index:
            # è·å–è¯¥è¡Œçš„æœ‰æ•ˆå› å­ä¿¡æ¯
            row_has_es = has_es.loc[idx]
            row_has_em = has_em.loc[idx]
            row_has_rm = has_rm.loc[idx]
            row_has_pm = has_pm.loc[idx]
            
            # è®¡ç®—æœ‰æ•ˆå› å­çš„æƒé‡å’Œ
            total_weight = (
                w_es * row_has_es +
                w_em * row_has_em +
                w_rm * row_has_rm +
                w_pm * row_has_pm
            )
            
            # å¦‚æœæ²¡æœ‰ä»»ä½•æœ‰æ•ˆå› å­ï¼ŒGè¯„åˆ†ä¸º0
            if total_weight == 0:
                g_score.loc[idx] = 0.0
                continue
            
            # è®¡ç®—åŠ æƒæ’åå’Œ
            weighted_sum = 0.0
            if row_has_es and pd.notna(df.loc[idx, 'rank_es']):
                weighted_sum += df.loc[idx, 'rank_es'] * w_es
            if row_has_em and pd.notna(df.loc[idx, 'rank_em']):
                weighted_sum += df.loc[idx, 'rank_em'] * w_em
            if row_has_rm and pd.notna(df.loc[idx, 'rank_rm']):
                weighted_sum += df.loc[idx, 'rank_rm'] * w_rm
            if row_has_pm and pd.notna(df.loc[idx, 'rank_pm']):
                weighted_sum += df.loc[idx, 'rank_pm'] * w_pm
            
            # è®¡ç®—æœ€ç»ˆGè¯„åˆ†
            g_score.loc[idx] = weighted_sum / total_weight
        
        return g_score

    def _save_g_factor_results_pit(self, g_factor_results: pd.DataFrame, calc_date: str) -> int:
        """ä¿å­˜Gå› å­è®¡ç®—ç»“æœ

        Args:
            g_factor_results: Gå› å­ç»“æœDataFrame
            calc_date: è®¡ç®—æ—¥æœŸ

        Returns:
            æˆåŠŸä¿å­˜çš„è®°å½•æ•°
        """
        if g_factor_results.empty:
            return 0

        try:
            self.logger.info(f"å¼€å§‹ä¿å­˜Gå› å­ç»“æœ: {len(g_factor_results)} æ¡è®°å½•")

            # åˆ é™¤è¯¥è®¡ç®—æ—¥æœŸçš„æ—§æ•°æ®
            delete_query = """
            DELETE FROM pgs_factors.g_factor
            WHERE calc_date = %s
            """
            self.context.db_manager.execute_sync(delete_query, (calc_date,))
            self.logger.info(f"å·²åˆ é™¤è®¡ç®—æ—¥æœŸ {calc_date} çš„æ‰€æœ‰æ—§Gå› å­æ•°æ®")

            # å‡†å¤‡æ’å…¥æ•°æ®
            insert_data = []
            for _, row in g_factor_results.iterrows():
                insert_data.append((
                    row['ts_code'],
                    calc_date,
                    row['data_source'],
                    float(row['g_efficiency_surprise']) if pd.notna(row['g_efficiency_surprise']) else None,
                    float(row['g_efficiency_momentum']) if pd.notna(row['g_efficiency_momentum']) else None,
                    float(row['g_revenue_momentum']) if pd.notna(row['g_revenue_momentum']) else None,
                    float(row['g_profit_momentum']) if pd.notna(row['g_profit_momentum']) else None,
                    float(row['rank_es']) if pd.notna(row['rank_es']) else None,
                    float(row['rank_em']) if pd.notna(row['rank_em']) else None,
                    float(row['rank_rm']) if pd.notna(row['rank_rm']) else None,
                    float(row['rank_pm']) if pd.notna(row['rank_pm']) else None,
                    float(row['g_score']) if pd.notna(row['g_score']) else None,
                    float(row['data_timeliness_weight']) if pd.notna(row['data_timeliness_weight']) else None,
                    row['calculation_status'],
                    row['ann_date']
                ))

            # æ‰¹é‡æ’å…¥
            insert_query = """
            INSERT INTO pgs_factors.g_factor
            (ts_code, calc_date, data_source, g_efficiency_surprise, g_efficiency_momentum,
             g_revenue_momentum, g_profit_momentum, rank_es, rank_em, rank_rm, rank_pm,
             g_score, data_timeliness_weight, calculation_status, ann_date)
            VALUES %s
            """

            # ä½¿ç”¨åŒæ­¥æ–¹æ³•é€æ¡æ’å…¥æ•°æ®ï¼ˆä½¿ç”¨æ­£ç¡®çš„è¡¨ç»“æ„ï¼‰
            insert_query = """
            INSERT INTO pgs_factors.g_factor (
                ts_code, calc_date, ann_date, data_source,
                g_efficiency_surprise, g_efficiency_momentum, g_revenue_momentum, g_profit_momentum,
                rank_es, rank_em, rank_rm, rank_pm,
                g_score, data_timeliness_weight, calculation_status
            ) VALUES (
                %s, %s, %s, %s,
                %s, %s, %s, %s,
                %s, %s, %s, %s,
                %s, %s, %s
            )
            """

            # é€æ¡æ’å…¥ï¼ˆå·²åˆ é™¤æ—§æ•°æ®ï¼Œæ— éœ€ON CONFLICTï¼‰
            success_count = 0
            for data_tuple in insert_data:
                try:
                    # æ•°æ®ç»“æ„: (ts_code, calc_date, data_source, g_efficiency_surprise, g_efficiency_momentum,
                    #           g_revenue_momentum, g_profit_momentum, rank_es, rank_em, rank_rm, rank_pm,
                    #           g_score, data_timeliness_weight, calculation_status, ann_date)
                    # é‡æ–°æ’åºä»¥åŒ¹é…INSERTè¯­å¥çš„å­—æ®µé¡ºåº

                    new_data_tuple = (
                        data_tuple[0],   # ts_code
                        data_tuple[1],   # calc_date
                        data_tuple[14],  # ann_date
                        data_tuple[2],   # data_source
                        data_tuple[3],   # g_efficiency_surprise
                        data_tuple[4],   # g_efficiency_momentum
                        data_tuple[5],   # g_revenue_momentum
                        data_tuple[6],   # g_profit_momentum
                        data_tuple[7],   # rank_es
                        data_tuple[8],   # rank_em
                        data_tuple[9],   # rank_rm
                        data_tuple[10],  # rank_pm
                        data_tuple[11],  # g_score
                        data_tuple[12],  # data_timeliness_weight
                        data_tuple[13]   # calculation_status
                    )

                    self.context.db_manager.execute_sync(insert_query, new_data_tuple)
                    success_count += 1
                except Exception as e:
                    self.logger.error(f"æ’å…¥Gå› å­æ•°æ®å¤±è´¥ {data_tuple[0]}: {e}")

            self.logger.info(f"æˆåŠŸä¿å­˜ {success_count}/{len(insert_data)} æ¡Gå› å­è®°å½•")
            return success_count

        except Exception as e:
            self.logger.error(f"ä¿å­˜Gå› å­ç»“æœå¤±è´¥: {e}")
            return 0

    def _log_performance_stats(self, success_count: int, failed_count: int):
        """è®°å½•æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        total_count = success_count + failed_count

        self.logger.info("=" * 50)
        self.logger.info("Gå› å­è®¡ç®—æ€§èƒ½ç»Ÿè®¡")
        self.logger.info("=" * 50)
        self.logger.info(f"æŸ¥è¯¢æ—¶é—´: {self.stats['query_time']:.2f} ç§’")
        self.logger.info(f"è®¡ç®—æ—¶é—´: {self.stats['calculation_time']:.2f} ç§’")
        self.logger.info(f"ä¿å­˜æ—¶é—´: {self.stats['save_time']:.2f} ç§’")
        self.logger.info(f"æ€»è€—æ—¶: {self.stats['total_time']:.2f} ç§’")
        self.logger.info(f"æˆåŠŸ: {success_count} åª")
        self.logger.info(f"å¤±è´¥: {failed_count} åª")
        self.logger.info(f"æˆåŠŸç‡: {(success_count/max(total_count,1)*100):.1f}%")

        if self.stats['total_time'] > 0:
            throughput = success_count / self.stats['total_time']
            self.logger.info(f"ååé‡: {throughput:.1f} åª/ç§’")

    def calculate_g_factors_batch_pit(
        self,
        start_date: str,
        end_date: str,
        mode: Optional[str] = None
    ) -> Dict[str, Any]:
        """åŸºäºæ—¥æœŸèŒƒå›´çš„æ‰¹é‡Gå› å­è®¡ç®— (ä¸ºrunnerè„šæœ¬æä¾›çš„æ¥å£)

        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            mode: æ‰§è¡Œæ¨¡å¼ ('incremental', 'backfill', Noneä¸ºè‡ªåŠ¨æ£€æµ‹)

        Returns:
            æ‰§è¡Œç»“æœç»Ÿè®¡
        """
        self.logger.info(f"å¼€å§‹æ‰¹é‡Gå› å­è®¡ç®—: {start_date} ~ {end_date}")

        # 1. æ™ºèƒ½æ¨¡å¼æ£€æµ‹
        if mode:
            execution_mode = mode
            self.logger.info(f"ä½¿ç”¨æŒ‡å®šæ¨¡å¼: {execution_mode}")
        else:
            execution_mode = self.detect_execution_mode(start_date, end_date)

        # 2. ç”Ÿæˆè®¡ç®—æ—¥æœŸåˆ—è¡¨
        calc_dates = self.generate_calculation_dates(start_date, end_date, execution_mode)

        if not calc_dates:
            self.logger.warning("æœªæ‰¾åˆ°éœ€è¦è®¡ç®—çš„æ—¥æœŸ")
            return {
                'success_count': 0,
                'failed_count': 0,
                'total_time': 0,
                'throughput': 0
            }

        self.logger.info(f"å…±éœ€è®¡ç®— {len(calc_dates)} ä¸ªæ—¥æœŸ")

        # 3. æ‰§è¡Œæ‰¹é‡è®¡ç®—
        total_start = time.time()
        total_success = 0
        total_failed = 0
        per_date_stats: Dict[str, Dict[str, int]] = {}

        for i, calc_date in enumerate(calc_dates, 1):
            self.logger.info(f"\nè¿›åº¦: [{i}/{len(calc_dates)}] å¤„ç†æ—¥æœŸ: {calc_date}")

            try:
                # è·å–åœ¨äº¤æ˜“è‚¡ç¥¨åˆ—è¡¨
                stock_codes = self._get_trading_stock_codes(calc_date)

                if not stock_codes:
                    self.logger.warning(f"{calc_date} æœªæ‰¾åˆ°åœ¨äº¤æ˜“è‚¡ç¥¨")
                    continue

                # æ‰§è¡ŒGå› å­è®¡ç®—
                result = self.calculate_g_factors_pit(calc_date, stock_codes)
                total_success += result['success_count']
                total_failed += result['failed_count']
                per_date_stats[calc_date] = {
                    'success': result['success_count'],
                    'failed': result['failed_count'],
                    'total': result['success_count'] + result['failed_count']
                }

                self.logger.info(f"{calc_date} è®¡ç®—å®Œæˆ: æˆåŠŸ {result['success_count']}, å¤±è´¥ {result['failed_count']}")

            except Exception as e:
                self.logger.error(f"{calc_date} è®¡ç®—å¤±è´¥: {e}")
                total_failed += len(stock_codes) if 'stock_codes' in locals() else 0

        total_time = time.time() - total_start

        self.logger.info("=" * 50)
        self.logger.info("æ‰¹é‡Gå› å­è®¡ç®—å®Œæˆ")
        self.logger.info("=" * 50)
        self.logger.info(f"æ€»è€—æ—¶: {total_time:.2f} ç§’")
        self.logger.info(f"æ€»æˆåŠŸ: {total_success}")
        self.logger.info(f"æ€»å¤±è´¥: {total_failed}")

        if total_time > 0:
            throughput = total_success / total_time
            self.logger.info(f"ååé‡: {throughput:.1f} åª/ç§’")

        # åŸºäºé€æ—¥ç»Ÿè®¡è®¡ç®—æˆåŠŸ/å¤±è´¥æ—¥æœŸæ•°
        successful_dates = 0
        failed_dates = 0
        for d, s in per_date_stats.items():
            if s['total'] == 0:
                # æ— æ ·æœ¬çš„æ—¥æœŸæ—¢ä¸ç®—æˆåŠŸä¹Ÿä¸ç®—å¤±è´¥
                continue
            if s['failed'] == 0 and s['success'] > 0:
                successful_dates += 1
            else:
                failed_dates += 1

        return {
            'success_count': total_success,
            'failed_count': total_failed,
            'total_time': total_time,
            'throughput': total_success / total_time if total_time > 0 else 0,
            'total_dates': len(calc_dates),
            'successful_dates': successful_dates,
            'failed_dates': failed_dates,
            'total_stocks_processed': total_success + total_failed,
            'total_records_saved': total_success
        }

    def detect_execution_mode(self, start_date: str, end_date: str) -> str:
        """æ™ºèƒ½æ£€æµ‹æ‰§è¡Œæ¨¡å¼

        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ

        Returns:
            æ‰§è¡Œæ¨¡å¼ ('incremental' æˆ– 'backfill')
        """
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰ç°æœ‰çš„Gå› å­æ•°æ®
            query = """
            SELECT COUNT(*) as count
            FROM pgs_factors.g_factor
            WHERE calc_date BETWEEN %s AND %s
            """

            result = self.context.query_dataframe(query, (start_date, end_date))

            if result.empty or result.iloc[0]['count'] == 0:
                self.logger.info("æœªå‘ç°ç°æœ‰Gå› å­æ•°æ®ï¼Œä½¿ç”¨backfillæ¨¡å¼")
                return 'backfill'
            else:
                self.logger.info(f"å‘ç° {result.iloc[0]['count']} æ¡ç°æœ‰Gå› å­æ•°æ®ï¼Œä½¿ç”¨incrementalæ¨¡å¼")
                return 'incremental'

        except Exception as e:
            self.logger.warning(f"æ£€æµ‹æ‰§è¡Œæ¨¡å¼å¤±è´¥: {e}ï¼Œé»˜è®¤ä½¿ç”¨incrementalæ¨¡å¼")
            return 'incremental'

    def generate_calculation_dates(self, start_date: str, end_date: str, mode: str) -> List[str]:
        """ç”Ÿæˆè®¡ç®—æ—¥æœŸåˆ—è¡¨

        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            mode: æ‰§è¡Œæ¨¡å¼

        Returns:
            è®¡ç®—æ—¥æœŸåˆ—è¡¨
        """
        # ç”Ÿæˆæ‰€æœ‰å‘¨äº”æ—¥æœŸ
        all_fridays = self._generate_friday_dates(start_date, end_date)

        if mode == 'backfill':
            # å›å¡«æ¨¡å¼ï¼šè®¡ç®—æ‰€æœ‰æ—¥æœŸ
            return all_fridays
        elif mode == 'incremental':
            # å¢é‡æ¨¡å¼ï¼šåªè®¡ç®—ç¼ºå¤±çš„æ—¥æœŸ
            return self._filter_missing_dates(all_fridays)
        else:
            self.logger.warning(f"æœªçŸ¥æ‰§è¡Œæ¨¡å¼: {mode}ï¼Œä½¿ç”¨å¢é‡æ¨¡å¼")
            return self._filter_missing_dates(all_fridays)

    def _generate_friday_dates(self, start_date: str, end_date: str) -> List[str]:
        """ç”ŸæˆæŒ‡å®šèŒƒå›´å†…çš„æ‰€æœ‰å‘¨äº”æ—¥æœŸ

        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ

        Returns:
            å‘¨äº”æ—¥æœŸåˆ—è¡¨
        """
        from datetime import datetime, timedelta

        start = datetime.strptime(start_date, '%Y-%m-%d')
        end = datetime.strptime(end_date, '%Y-%m-%d')

        fridays = []
        current = start

        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå‘¨äº”
        while current.weekday() != 4:  # 4 = å‘¨äº”
            current += timedelta(days=1)
            if current > end:
                break

        # æ”¶é›†æ‰€æœ‰å‘¨äº”
        while current <= end:
            fridays.append(current.strftime('%Y-%m-%d'))
            current += timedelta(days=7)

        return fridays

    def _filter_missing_dates(self, dates: List[str]) -> List[str]:
        """è¿‡æ»¤å‡ºç¼ºå¤±Gå› å­æ•°æ®çš„æ—¥æœŸ

        Args:
            dates: å€™é€‰æ—¥æœŸåˆ—è¡¨

        Returns:
            ç¼ºå¤±æ•°æ®çš„æ—¥æœŸåˆ—è¡¨
        """
        if not dates:
            return []

        try:
            # æŸ¥è¯¢å·²æœ‰æ•°æ®çš„æ—¥æœŸ
            query = """
            SELECT DISTINCT calc_date
            FROM pgs_factors.g_factor
            WHERE calc_date = ANY(%s)
            """

            result = self.context.query_dataframe(query, (dates,))

            if result.empty:
                return dates

            existing_dates = set(result['calc_date'].dt.strftime('%Y-%m-%d').tolist())
            missing_dates = [date for date in dates if date not in existing_dates]

            self.logger.info(f"æ€»æ—¥æœŸ: {len(dates)}, å·²æœ‰æ•°æ®: {len(existing_dates)}, ç¼ºå¤±æ•°æ®: {len(missing_dates)}")

            return missing_dates

        except Exception as e:
            self.logger.error(f"è¿‡æ»¤ç¼ºå¤±æ—¥æœŸå¤±è´¥: {e}")
            return dates

    def _get_trading_stock_codes(self, calc_date: str) -> List[str]:
        """è·å–æŒ‡å®šæ—¥æœŸçš„åœ¨äº¤æ˜“è‚¡ç¥¨åˆ—è¡¨ï¼ˆå·²é›†æˆé€€å¸‚è‚¡ç¥¨ç­›é€‰ï¼‰

        Args:
            calc_date: è®¡ç®—æ—¥æœŸ

        Returns:
            åœ¨äº¤æ˜“è‚¡ç¥¨ä»£ç åˆ—è¡¨
        """
        try:
            # ä¼˜å…ˆä½¿ç”¨ç»Ÿä¸€çš„åœ¨ç±å¯äº¤æ˜“è‚¡ç¥¨é›†åˆï¼ˆä¸På› å­å£å¾„ä¸€è‡´ï¼‰
            query_primary = "SELECT * FROM get_trading_stocks_optimized(%s)"
            df_primary = self.context.query_dataframe(query_primary, (calc_date,))

            if df_primary is not None and not df_primary.empty:
                stock_codes = df_primary['ts_code'].tolist()
                self.logger.info(f"{calc_date} è·å–åˆ° {len(stock_codes)} åªåœ¨äº¤æ˜“è‚¡ç¥¨ï¼ˆä¼˜åŒ–å‡½æ•°ï¼‰")
                return stock_codes

            # å›é€€ï¼šä½¿ç”¨å½“æ—¥å·²æœ‰På› å­æ•°æ®çš„è‚¡ç¥¨é›†åˆ
            self.logger.warning(f"{calc_date} ä¼˜åŒ–å‡½æ•°æ— è¿”å›ï¼Œå›é€€åˆ°På› å­è‚¡ç¥¨é›†åˆ")
            query_fallback = """
            SELECT DISTINCT ts_code
            FROM pgs_factors.p_factor
            WHERE calc_date = %s
            ORDER BY ts_code
            """
            df_fb = self.context.query_dataframe(query_fallback, (calc_date,))
            if df_fb is not None and not df_fb.empty:
                stock_codes = df_fb['ts_code'].tolist()
                self.logger.info(f"{calc_date} å›é€€é›†åˆåŒ…å« {len(stock_codes)} åªè‚¡ç¥¨")
                return stock_codes

            self.logger.warning(f"{calc_date} æœªè·å–åˆ°è‚¡ç¥¨åˆ—è¡¨")
            return []

        except Exception as e:
            self.logger.error(f"è·å– {calc_date} è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            return []

    def _validate_calculation_results(self, calc_dates: List[str]):
        """éªŒè¯Gå› å­è®¡ç®—ç»“æœçš„æ•°æ®è´¨é‡

        Args:
            calc_dates: éœ€è¦éªŒè¯çš„è®¡ç®—æ—¥æœŸåˆ—è¡¨
        """
        self.logger.info(f"å¼€å§‹éªŒè¯ {len(calc_dates)} ä¸ªæ—¥æœŸçš„Gå› å­æ•°æ®è´¨é‡")

        for calc_date in calc_dates:
            try:
                # æŸ¥è¯¢è¯¥æ—¥æœŸçš„Gå› å­æ•°æ®
                query = """
                SELECT
                    COUNT(*) as total_count,
                    COUNT(CASE WHEN g_score IS NOT NULL THEN 1 END) as valid_score_count,
                    AVG(g_score) as avg_score,
                    MIN(g_score) as min_score,
                    MAX(g_score) as max_score,
                    AVG(data_timeliness_weight) as avg_timeliness_weight,
                    COUNT(CASE WHEN data_source = 'express' THEN 1 END) as express_count,
                    COUNT(CASE WHEN data_source = 'forecast' THEN 1 END) as forecast_count
                FROM pgs_factors.g_factor
                WHERE calc_date = %s
                """

                result = self.context.query_dataframe(query, (calc_date,))

                if result.empty or result.iloc[0]['total_count'] == 0:
                    self.logger.warning(f"{calc_date}: æ— Gå› å­æ•°æ®")
                else:
                    row = result.iloc[0]
                    express_forecast_ratio = (row['express_count'] + row['forecast_count']) / row['total_count'] * 100

                    self.logger.info(f"{calc_date}: æ€»è®°å½• {row['total_count']}, "
                                   f"æœ‰æ•ˆè¯„åˆ† {row['valid_score_count']}, "
                                   f"å¹³å‡åˆ† {row['avg_score']:.2f}, "
                                   f"åˆ†æ•°èŒƒå›´ [{row['min_score']:.2f}, {row['max_score']:.2f}], "
                                   f"å¹³å‡æƒé‡ {row['avg_timeliness_weight']:.3f}, "
                                   f"Express+Forecastå æ¯” {express_forecast_ratio:.1f}%")

            except Exception as e:
                self.logger.error(f"éªŒè¯ {calc_date} æ•°æ®è´¨é‡å¤±è´¥: {e}")

        self.logger.info("Gå› å­æ•°æ®è´¨é‡éªŒè¯å®Œæˆ")
