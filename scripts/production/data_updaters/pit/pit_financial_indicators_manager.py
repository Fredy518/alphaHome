#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
PITè´¢åŠ¡æŒ‡æ ‡ç®¡ç†å™¨ï¼ˆåŸ PITFinancialIndicatorsMVPManager é‡å‘½åï¼‰
==============

è´Ÿè´£pit_financial_indicatorsè¡¨çš„å†å²å…¨é‡å›å¡«å’Œå¢é‡æ›´æ–°

åŠŸèƒ½ç‰¹ç‚¹:
1. ä»pit_income_quarterlyç­‰è¡¨è®¡ç®—è´¢åŠ¡æŒ‡æ ‡
2. æ”¯æŒå†å²å…¨é‡å›å¡«å’Œå¢é‡æ›´æ–°
3. è‡ªåŠ¨å¤„ç†æ•°æ®è½¬æ¢å’Œæ¸…æ´—
4. æä¾›æ•°æ®éªŒè¯å’ŒçŠ¶æ€æ£€æŸ¥

Author: AI Assistant
Date: 2025-08-11
"""

import sys
import os
import argparse
from datetime import datetime, date, timedelta
from typing import Dict, List, Optional, Any
import pandas as pd

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

try:
    from .base.pit_table_manager import PITTableManager
    from .base.pit_config import PITConfig
    from .calculators.financial_indicators_calculator import FinancialIndicatorsCalculator
except ImportError:
    from base.pit_table_manager import PITTableManager
    from base.pit_config import PITConfig
    from calculators.financial_indicators_calculator import FinancialIndicatorsCalculator


class PITFinancialIndicatorsManager(PITTableManager):
    """è´¢åŠ¡æŒ‡æ ‡ Managerï¼šè´Ÿè´£å»ºè¡¨ã€ä¾èµ–æ ¡éªŒä¸è°ƒåº¦è®¡ç®—"""

    def __init__(self):
        super().__init__('pit_financial_indicators')
        self.source_tables = self.table_config['source_tables']
        self.key_fields = self.table_config['key_fields']
        self.data_fields = self.table_config['data_fields']
        self.depends_on = self.table_config.get('depends_on', [])
        self.calculator = None

    def ensure_table_exists(self) -> None:
        """ç¡®ä¿è´¢åŠ¡æŒ‡æ ‡è¡¨å­˜åœ¨ï¼ˆDDL èŒè´£å½’ä½åˆ° Managerï¼‰"""
        try:
            # ä¼˜å…ˆ pit_data/database
            sql_path = os.path.join(os.path.dirname(__file__), 'database', 'create_pit_financial_indicators_table.sql')
            sql_path = os.path.normpath(sql_path)
            if not os.path.exists(sql_path):
                alt_path = os.path.join(
                    os.path.dirname(__file__), 'database', 'create_mvp_financial_indicators_table.sql'
                )
                alt_path = os.path.normpath(alt_path)
                if os.path.exists(alt_path):
                    sql_path = alt_path
                else:
                    self.logger.warning(f"æœªæ‰¾åˆ°å»ºè¡¨SQL: {sql_path}")
                    return
            with open(sql_path, 'r', encoding='utf-8') as f:
                create_sql = f.read()
            self.context.db_manager.execute_sync(create_sql)
            self.logger.info("è´¢åŠ¡æŒ‡æ ‡è¡¨åˆ›å»º/éªŒè¯å®Œæˆ")
        except Exception as e:
            self.logger.error(f"åˆ›å»ºè´¢åŠ¡æŒ‡æ ‡è¡¨å¤±è´¥: {e}")

    def _initialize_calculator(self):
        if self.calculator is None:
            self.calculator = FinancialIndicatorsCalculator(self.context)

    def incremental_update(self, days: int = 7, batch_size: int | None = None) -> Dict[str, Any]:
        """
        å¢é‡æ›´æ–°è´¢åŠ¡æŒ‡æ ‡ - æ­£ç¡®å¤„ç†æ¯ä¸ªå…¬å‘Šæ—¥æœŸ

        å¢é‡æ›´æ–°ç­–ç•¥ï¼š
        1. è·å–æœ€è¿‘dayså¤©å†…æœ‰æ–°æŠ«éœ²çš„åˆ©æ¶¦è¡¨è®°å½•
        2. æŒ‰å…¬å‘Šæ—¥æœŸåˆ†ç»„ï¼Œä¸ºæ¯ä¸ªå…¬å‘Šæ—¥æœŸè®¡ç®—è´¢åŠ¡æŒ‡æ ‡
        3. ç¡®ä¿æ¯ä¸ªå†å²æ—¶é—´ç‚¹çš„è´¢åŠ¡æŒ‡æ ‡éƒ½è¢«æ­£ç¡®è®¡ç®—

        ä¿®å¤çš„é—®é¢˜ï¼š
        - ä¹‹å‰åªåœ¨ä¸€ä¸ªå›ºå®šas_of_dateä¸Šè®¡ç®—æ‰€æœ‰è‚¡ç¥¨
        - ç°åœ¨ä¸ºæ¯ä¸ªå…¬å‘Šæ—¥æœŸåˆ†åˆ«è®¡ç®—ï¼Œæ­£ç¡®åæ˜ å†å²çŠ¶æ€
        """
        from datetime import date, timedelta
        if batch_size is None:
            batch_size = self.batch_size
        # å®¹é”™ï¼šå½“è°ƒç”¨æ–¹ä¼ å…¥ days=None æ—¶ï¼Œå›é€€è‡³é…ç½®é»˜è®¤å¤©æ•°ï¼ˆæˆ–7å¤©ï¼‰
        if days is None:
            try:
                days = int(PITConfig.DEFAULT_DATE_RANGES.get('incremental_days', 7))
            except Exception:
                days = 7

        # ç¡®ä¿è¡¨ç»“æ„å®Œæ•´
        self._ensure_table_exists()
        end_date = date.today().isoformat()
        start_date = (date.today() - timedelta(days=days)).isoformat()
        self.ensure_table_exists()  # å†æ¬¡ç¡®è®¤è¡¨å­˜åœ¨
        self._initialize_calculator()

        try:
            # 1. è·å–æœ€è¿‘dayså¤©å†…æ‰€æœ‰åˆ©æ¶¦è¡¨è®°å½•ï¼ŒæŒ‰å…¬å‘Šæ—¥æœŸåˆ†ç»„
            q = """
            SELECT DISTINCT ts_code, end_date, ann_date, data_source
            FROM pgs_factors.pit_income_quarterly
            WHERE ann_date BETWEEN %s AND %s
            ORDER BY ann_date ASC, ts_code, end_date ASC, data_source
            """
            df = self.context.query_dataframe(q, (start_date, end_date))

            if df is None or df.empty:
                self.logger.info("è¿‘æœŸæ— æ–°æŠ«éœ²çš„åˆ©æ¶¦è¡¨æ•°æ®")
                return {'updated_records': 0, 'calculated_dates': 0, 'processed_stocks': 0, 'message': 'æ— éœ€è¦æ›´æ–°çš„æ•°æ®'}

            # è·å–åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
            unique_stocks = df['ts_code'].nunique()
            unique_ann_dates = df['ann_date'].nunique()
            total_records = len(df)

            self.logger.info(f"å¢é‡æ›´æ–°: æ‰¾åˆ° {unique_stocks} åªè‚¡ç¥¨ï¼Œ{unique_ann_dates} ä¸ªå…¬å‘Šæ—¥æœŸï¼Œå…± {total_records} æ¡è®°å½•")

            # 2. ä¸ºæ¯ä¸ªåˆ©æ¶¦è¡¨è®°å½•å•ç‹¬è®¡ç®—è´¢åŠ¡æŒ‡æ ‡
            total_processed = 0
            processed_stocks = set()

            # å¢é‡æ›´æ–°ï¼šæŒ‰å…¬å‘Šæ—¥æœŸåˆ†ç»„ï¼Œä¸¥æ ¼æ­£åºï¼ˆPITï¼‰å¤„ç†
            self.logger.info("å¢é‡æ›´æ–°ï¼šæŒ‰å…¬å‘Šæ—¥æœŸåˆ†ç»„ï¼ŒæŒ‰æ­£åºå¤„ç†ï¼ˆPITï¼‰")

            # æŒ‰å…¬å‘Šæ—¥æœŸåˆ†ç»„ï¼Œç¡®ä¿æ¯ä¸ªå…¬å‘Šæ—¥æœŸçš„æ‰€æœ‰è‚¡ç¥¨éƒ½è¢«å¤„ç†
            unique_ann_dates_list = sorted(df['ann_date'].unique(), reverse=False)

            for ann_date in unique_ann_dates_list:
                try:
                    # è·å–è¯¥å…¬å‘Šæ—¥æœŸçš„æ‰€æœ‰è®°å½•
                    date_records = df[df['ann_date'] == ann_date]

                    # æŒ‰æŠ¥å‘ŠæœŸåˆ†ç»„å¤„ç†ï¼ˆå¤„ç†åŒä¸€å¤©å‘å¸ƒå¤šä»½è´¢æŠ¥çš„æƒ…å†µï¼‰
                    report_periods = date_records.groupby('end_date')
                    self.logger.debug(f"å¢é‡æ›´æ–°å…¬å‘Šæ—¥æœŸ {ann_date}: å‘ç° {len(report_periods)} ä¸ªæŠ¥å‘ŠæœŸ")

                    for end_date, period_records in report_periods:
                        try:
                            # è·å–è¯¥æŠ¥å‘ŠæœŸçš„è‚¡ç¥¨åˆ—è¡¨
                            period_stocks = period_records['ts_code'].unique().tolist()

                            # åˆ†ææ•°æ®æºåˆ†å¸ƒ
                            data_source_counts = period_records['data_source'].value_counts()
                            self.logger.debug(f"  æŠ¥å‘ŠæœŸ {end_date}: {len(period_stocks)} åªè‚¡ç¥¨")
                            self.logger.debug(f"    æ•°æ®æºåˆ†å¸ƒ: {dict(data_source_counts)}")

                            # ä¸ºè¯¥æŠ¥å‘ŠæœŸçš„æ‰€æœ‰è‚¡ç¥¨è®¡ç®—è´¢åŠ¡æŒ‡æ ‡ï¼ˆä»¥å…¬å‘Šæ—¥ä¸ºPITè§‚å¯Ÿæ—¶ç‚¹ï¼Œç¡®ä¿å¤šæ¡åŒend_dateä¸åŒann_dateä¿ç•™ï¼‰
                            as_of = pd.to_datetime(ann_date).date().isoformat() if hasattr(pd.to_datetime(ann_date), 'date') else str(ann_date)
                            res = self.calculator.calculate_indicators_for_date(
                                as_of_date=as_of,
                                stock_codes=period_stocks,
                                batch_size=batch_size,
                                target_data_sources=None  # è®©è®¡ç®—å™¨å†…éƒ¨å¤„ç†æ•°æ®æºä¼˜å…ˆçº§
                            )

                            success_count = int(res.get('success_count', 0))
                            total_processed += success_count

                            if success_count > 0:
                                processed_stocks.update(period_stocks)

                            self.logger.debug(f"    æŠ¥å‘ŠæœŸ {end_date} å¢é‡æ›´æ–°å®Œæˆ: {success_count} æ¡è´¢åŠ¡æŒ‡æ ‡è®°å½•")

                        except Exception as e:
                            self.logger.warning(f"å…¬å‘Šæ—¥æœŸ {ann_date} æŠ¥å‘ŠæœŸ {end_date} å¢é‡æ›´æ–°å¤±è´¥: {e}")
                            continue

                    # å®šæœŸæŠ¥å‘Šè¿›åº¦
                    if len(unique_ann_dates_list) > 10 and unique_ann_dates_list.index(ann_date) % 5 == 0:
                        progress = unique_ann_dates_list.index(ann_date) + 1
                        self.logger.info(f"å¢é‡æ›´æ–°è¿›åº¦: å·²å¤„ç† {progress}/{len(unique_ann_dates_list)} ä¸ªå…¬å‘Šæ—¥æœŸï¼Œ{len(processed_stocks)} åªè‚¡ç¥¨ï¼Œç”Ÿæˆ {total_processed} æ¡è´¢åŠ¡æŒ‡æ ‡è®°å½•")

                except Exception as e:
                    self.logger.warning(f"å…¬å‘Šæ—¥æœŸ {ann_date} å¢é‡æ›´æ–°å¤±è´¥: {e}")
                    continue

            self.logger.info(f"å¢é‡æ›´æ–°å®Œæˆ: å…±å¤„ç† {len(processed_stocks)}/{unique_stocks} åªè‚¡ç¥¨ï¼Œç”Ÿæˆ {total_processed} æ¡è´¢åŠ¡æŒ‡æ ‡è®°å½•")

            return {
                'updated_records': total_processed,
                'processed_stocks': len(processed_stocks),
                'total_stocks': unique_stocks,
                'message': f'æˆåŠŸå¢é‡æ›´æ–° {total_processed} æ¡è´¢åŠ¡æŒ‡æ ‡è®°å½•ï¼Œå…±å¤„ç† {len(processed_stocks)} åªè‚¡ç¥¨'
            }

        except Exception as e:
            self.logger.error(f"å¢é‡æ›´æ–°å¤±è´¥: {e}")
        return {
                'updated_records': 0,
                'calculated_dates': 0,
                'processed_stocks': 0,
                'error': str(e),
                'message': 'å¢é‡æ›´æ–°å¤±è´¥'
        }

    def full_backfill(self, start_date: str | None = None, end_date: str | None = None, batch_size: int | None = None) -> Dict[str, Any]:
        """
        å†å²å…¨é‡å›å¡«è´¢åŠ¡æŒ‡æ ‡ - é‡æ–°è®¡ç®—æ‰€æœ‰å†å²æ—¶é—´ç‚¹çš„è´¢åŠ¡æŒ‡æ ‡

        Args:
            start_date: å¼€å§‹æ—¥æœŸ (ann_date)
            end_date: ç»“æŸæ—¥æœŸ (ann_date)
            batch_size: æ‰¹æ¬¡å¤§å°
            fill_order: å¡«å……é¡ºåº ('asc' for æ­£åº, 'desc' for å€’åº)

        Returns:
            æ‰§è¡Œç»“æœç»Ÿè®¡
        """
        self.logger.info("å¼€å§‹PITè´¢åŠ¡æŒ‡æ ‡å†å²å…¨é‡å›å¡«")

        # è®¾ç½®é»˜è®¤å‚æ•°
        if start_date is None or end_date is None:
            start_date, end_date = PITConfig.get_backfill_date_range(start_date, end_date)

        if batch_size is None:
            batch_size = self.batch_size

        self.logger.info(f"å›å¡«æ—¥æœŸèŒƒå›´: {start_date} ~ {end_date}")
        self.logger.info(f"æ¯æ‰¹è‚¡ç¥¨æ•°: {batch_size}")

        try:
            # 0. ç¡®ä¿ç›®æ ‡è¡¨å­˜åœ¨
            self._ensure_table_exists()
            self._initialize_calculator()

            # 1. è·å–æ‰€æœ‰è‚¡ç¥¨çš„æ‰€æœ‰å†å²åˆ©æ¶¦è¡¨è®°å½•
            q = """
            SELECT ts_code, end_date, ann_date, data_source
            FROM pgs_factors.pit_income_quarterly
            WHERE ann_date BETWEEN %s AND %s
            ORDER BY ann_date ASC, ts_code, end_date ASC, data_source
            """
            df = self.context.query_dataframe(q, (start_date, end_date))

            if df is None or df.empty:
                self.logger.warning("æœªæ‰¾åˆ°éœ€è¦å›å¡«çš„å†å²åˆ©æ¶¦è¡¨æ•°æ®")
                return {'backfilled_records': 0, 'message': 'æ— å›å¡«æ•°æ®'}

            # è·å–åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
            unique_stocks = df['ts_code'].nunique()
            unique_ann_dates = df['ann_date'].nunique()
            total_records = len(df)

            self.logger.info(f"ä»åˆ©æ¶¦è¡¨è·å–åˆ° {unique_stocks} åªè‚¡ç¥¨ï¼Œ{unique_ann_dates} ä¸ªå…¬å‘Šæ—¥æœŸï¼Œå…± {total_records} æ¡è®°å½•")

            # 2. ä¸ºæ¯ä¸ªåˆ©æ¶¦è¡¨è®°å½•å•ç‹¬è®¡ç®—è´¢åŠ¡æŒ‡æ ‡
            total_processed = 0
            processed_stocks = set()
            failed_stocks = set()

            # å…¨é‡å›å¡«ï¼šæŒ‰å…¬å‘Šæ—¥æœŸåˆ†ç»„ï¼Œä¸¥æ ¼æ­£åºå¤„ç†ï¼ˆPITï¼‰
            self.logger.info("å…¨é‡å›å¡«ï¼šæŒ‰å…¬å‘Šæ—¥æœŸæ­£åºå¤„ç†ï¼ˆPITï¼‰")
            unique_ann_dates_list = sorted(df['ann_date'].unique(), reverse=False)

            for ann_date in unique_ann_dates_list:
                try:
                    # è·å–è¯¥å…¬å‘Šæ—¥æœŸçš„æ‰€æœ‰è®°å½•
                    date_records = df[df['ann_date'] == ann_date]

                    # åˆ†æè¯¥å…¬å‘Šæ—¥æœŸçš„æ•°æ®ç»“æ„
                    self.logger.debug(f"åˆ†æå…¬å‘Šæ—¥æœŸ {ann_date} çš„æ•°æ®ç»“æ„...")

                    # æŒ‰æŠ¥å‘ŠæœŸåˆ†ç»„å¤„ç†ï¼ˆå¤„ç†åŒä¸€å¤©å‘å¸ƒå¤šä»½è´¢æŠ¥çš„æƒ…å†µï¼‰
                    report_periods = date_records.groupby('end_date')
                    self.logger.info(f"å…¬å‘Šæ—¥æœŸ {ann_date}: å‘ç° {len(report_periods)} ä¸ªæŠ¥å‘ŠæœŸ")

                    for end_date, period_records in report_periods:
                        try:
                            # è·å–è¯¥æŠ¥å‘ŠæœŸçš„è‚¡ç¥¨åˆ—è¡¨
                            period_stocks = period_records['ts_code'].unique().tolist()

                            # åˆ†ææ•°æ®æºåˆ†å¸ƒ
                            data_source_counts = period_records['data_source'].value_counts()
                            self.logger.debug(f"  æŠ¥å‘ŠæœŸ {end_date}: {len(period_stocks)} åªè‚¡ç¥¨")
                            self.logger.debug(f"    æ•°æ®æºåˆ†å¸ƒ: {dict(data_source_counts)}")

                            # ä¸ºè¯¥æŠ¥å‘ŠæœŸçš„æ‰€æœ‰è‚¡ç¥¨è®¡ç®—è´¢åŠ¡æŒ‡æ ‡
                            # ç»Ÿä¸€ä¿®å¤ï¼šä½¿ç”¨å…¬å‘Šæ—¥ä½œä¸ºPITè§‚å¯Ÿæ—¶ç‚¹ï¼Œç¡®ä¿ forecast/express ä¸è¢«æ—¶é—´çª—å£æ’é™¤
                            as_of = pd.to_datetime(ann_date).date().isoformat() if hasattr(pd.to_datetime(ann_date), 'date') else str(ann_date)
                            res = self.calculator.calculate_indicators_for_date(
                                as_of_date=as_of,
                                stock_codes=period_stocks,
                                batch_size=batch_size,
                                target_data_sources=None  # è®©è®¡ç®—å™¨å†…éƒ¨å¤„ç†æ•°æ®æºä¼˜å…ˆçº§
                            )

                            success_count = int(res.get('success_count', 0))
                            total_processed += success_count

                            if success_count > 0:
                                processed_stocks.update(period_stocks)

                            self.logger.debug(f"    æŠ¥å‘ŠæœŸ {end_date} å¤„ç†å®Œæˆ: {success_count} æ¡è´¢åŠ¡æŒ‡æ ‡è®°å½•")

                        except Exception as e:
                            self.logger.warning(f"å…¬å‘Šæ—¥æœŸ {ann_date} æŠ¥å‘ŠæœŸ {end_date} è®¡ç®—å¤±è´¥: {e}")
                            continue

                    # å®šæœŸæŠ¥å‘Šè¿›åº¦
                    if len(unique_ann_dates_list) > 100 and unique_ann_dates_list.index(ann_date) % 10 == 0:
                        progress = unique_ann_dates_list.index(ann_date) + 1
                        self.logger.info(f"è¿›åº¦: å·²å¤„ç† {progress}/{len(unique_ann_dates_list)} ä¸ªå…¬å‘Šæ—¥æœŸï¼Œ{len(processed_stocks)} åªè‚¡ç¥¨ï¼Œç”Ÿæˆ {total_processed} æ¡è´¢åŠ¡æŒ‡æ ‡è®°å½•")

                except Exception as e:
                    self.logger.warning(f"å…¬å‘Šæ—¥æœŸ {ann_date} è®¡ç®—å¤±è´¥: {e}")
                    continue

            # æ£€æŸ¥æ˜¯å¦æœ‰è‚¡ç¥¨å®Œå…¨æ²¡æœ‰è¢«å¤„ç†
            all_stocks_in_df = set(df['ts_code'].unique())
            unprocessed_stocks = all_stocks_in_df - processed_stocks

            if unprocessed_stocks:
                self.logger.warning(f"å‘ç° {len(unprocessed_stocks)} åªè‚¡ç¥¨æ²¡æœ‰è¢«æˆåŠŸå¤„ç†: {list(unprocessed_stocks)[:10]}...")

            self.logger.info(f"å†å²å…¨é‡å›å¡«å®Œæˆ: å…±å¤„ç† {len(processed_stocks)}/{unique_stocks} åªè‚¡ç¥¨ï¼Œç”Ÿæˆ {total_processed} æ¡è´¢åŠ¡æŒ‡æ ‡è®°å½•")

            return {
                'backfilled_records': total_processed,
                'processed_stocks': len(processed_stocks),
                'total_stocks': unique_stocks,
                'message': f'æˆåŠŸå…¨é‡å›å¡« {total_processed} æ¡è´¢åŠ¡æŒ‡æ ‡è®°å½•ï¼Œå…±å¤„ç† {len(processed_stocks)} åªè‚¡ç¥¨'
            }

        except Exception as e:
            self.logger.error(f"å†å²å…¨é‡å›å¡«å¤±è´¥: {e}")
            return {
                'backfilled_records': 0,
                'error': str(e),
                'message': 'å†å²å…¨é‡å›å¡«å¤±è´¥'
            }

    def single_backfill(self,
                         ts_code: str,
                         start_date: Optional[str] = None,
                         end_date: Optional[str] = None,
                         batch_size: Optional[int] = None,
                         do_validate: bool = True) -> Dict[str, Any]:
        """å•ä¸ªè‚¡ç¥¨å†å²å›å¡«ï¼ˆå¯é€‰éªŒè¯ï¼‰ã€‚

        Args:
            ts_code: è‚¡ç¥¨ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            batch_size: æ‰¹æ¬¡å¤§å°
            do_validate: æ˜¯å¦æ‰§è¡ŒéªŒè¯
            fill_order: å¡«å……é¡ºåº ('asc' for æ­£åº, 'desc' for å€’åº)

        - å¯¹æŒ‡å®š ts_code é‡æ–°è®¡ç®—æ‰€æœ‰å†å²æ—¶é—´ç‚¹çš„è´¢åŠ¡æŒ‡æ ‡
        - æ¯ä¸ªåˆ©æ¶¦è¡¨è®°å½•éƒ½ä¼šå¯¹åº”ç”Ÿæˆè´¢åŠ¡æŒ‡æ ‡è®°å½•
        - ä¸å½±å“æ—¢æœ‰å…¨é‡/å¢é‡é€»è¾‘
        """
        if not ts_code:
            return {'backfilled_records': 0, 'error': 'ç¼ºå°‘ ts_code', 'message': 'å¿…é¡»æä¾› --ts-code æ‰èƒ½æ‰§è¡Œå•è‚¡å›å¡«'}

        self.logger.info(f"å¼€å§‹ä¸ªè‚¡è´¢åŠ¡æŒ‡æ ‡å†å²å›å¡«: ts_code={ts_code}")

        # é»˜è®¤å‚æ•°
        if start_date is None or end_date is None:
            start_date, end_date = PITConfig.get_backfill_date_range(start_date, end_date)
        if batch_size is None:
            batch_size = self.batch_size

        self.logger.info(f"å›å¡«æ—¥æœŸèŒƒå›´: {start_date} ~ {end_date}")

        try:
            # 0. ç¡®ä¿ç›®æ ‡è¡¨å­˜åœ¨
            self._ensure_table_exists()
            self._initialize_calculator()

            # 1. è·å–è¯¥è‚¡ç¥¨çš„æ‰€æœ‰å†å²åˆ©æ¶¦è¡¨è®°å½•
            q = """
            SELECT DISTINCT ts_code, end_date, ann_date, data_source
            FROM pgs_factors.pit_income_quarterly
            WHERE ts_code = %s AND ann_date BETWEEN %s AND %s
            ORDER BY ann_date ASC, end_date ASC, data_source
            """
            df = self.context.query_dataframe(q, (ts_code, start_date, end_date))

            if df is None or df.empty:
                self.logger.warning("è¯¥è‚¡ç¥¨åœ¨æŒ‡å®šæ—¥æœŸèŒƒå›´å†…æ— åˆ©æ¶¦è¡¨æ•°æ®")
                return {'backfilled_records': 0, 'message': 'æ— åˆ©æ¶¦è¡¨æ•°æ®å¯ä¾›è®¡ç®—', 'ts_code': ts_code}

            self.logger.info(f"è¯¥è‚¡ç¥¨åœ¨æŒ‡å®šæ—¥æœŸèŒƒå›´å†…æœ‰ {len(df)} ä¸ªå”¯ä¸€çš„(end_date, ann_date)ç»„åˆ")

            # 2. ä¸ºæ¯ä¸ªå†å²æ—¶é—´ç‚¹è®¡ç®—è´¢åŠ¡æŒ‡æ ‡
            total_processed = 0
            processed_dates = []

            # æŒ‰å…¬å‘Šæ—¥æœŸåˆ†ç»„ï¼Œæ¯æ¬¡è®¡ç®—ä¸€æ‰¹
            unique_ann_dates = df['ann_date'].unique()
            self.logger.info(f"å°†ä¸º {len(unique_ann_dates)} ä¸ªå…¬å‘Šæ—¥æœŸè®¡ç®—è´¢åŠ¡æŒ‡æ ‡")

            # å•è‚¡å›å¡«ï¼šä¸ºè¯¥è‚¡ç¥¨çš„æ‰€æœ‰å…¬å‘Šæ—¥æœŸè®¡ç®—è´¢åŠ¡æŒ‡æ ‡
            self.logger.info(f"å•è‚¡å›å¡«ï¼šä¸ºè‚¡ç¥¨ {ts_code} çš„ {len(unique_ann_dates)} ä¸ªå…¬å‘Šæ—¥æœŸè®¡ç®—è´¢åŠ¡æŒ‡æ ‡")

            # é€ä¸ªå¤„ç†æ¯ä¸ªå…¬å‘Šæ—¥æœŸï¼Œç¡®ä¿æ¯ä¸ªæ—¥æœŸéƒ½èƒ½è·å¾—æ­£ç¡®çš„è´¢åŠ¡æŒ‡æ ‡
            for ann_date in sorted(unique_ann_dates, reverse=False):
                try:
                    # è·å–è¯¥å…¬å‘Šæ—¥æœŸçš„æ‰€æœ‰è®°å½•
                    date_records = df[df['ann_date'] == ann_date]

                    # æŒ‰æŠ¥å‘ŠæœŸåˆ†ç»„å¤„ç†ï¼ˆå¤„ç†åŒä¸€å¤©å‘å¸ƒå¤šä»½è´¢æŠ¥çš„æƒ…å†µï¼‰
                    report_periods = date_records.groupby('end_date')
                    self.logger.debug(f"å¤„ç†è‚¡ç¥¨ {ts_code} å…¬å‘Šæ—¥æœŸ {ann_date}: å‘ç° {len(report_periods)} ä¸ªæŠ¥å‘ŠæœŸ")

                    for end_date, period_records in report_periods:
                        try:
                            # ç¡®ä¿è¯¥è‚¡ç¥¨åœ¨è¯¥æŠ¥å‘ŠæœŸæœ‰è®°å½•
                            stock_period_records = period_records[period_records['ts_code'] == ts_code]
                            if stock_period_records.empty:
                                continue

                            # åˆ†ææ•°æ®æºåˆ†å¸ƒ
                            data_source_counts = stock_period_records['data_source'].value_counts()
                            self.logger.debug(f"  æŠ¥å‘ŠæœŸ {end_date}: {len(stock_period_records)} æ¡è®°å½•")
                            self.logger.debug(f"    æ•°æ®æºåˆ†å¸ƒ: {dict(data_source_counts)}")

                            # ä¸ºè¯¥è‚¡ç¥¨åœ¨è¯¥å…¬å‘Šæ—¥æœŸè®¡ç®—è´¢åŠ¡æŒ‡æ ‡ï¼ˆä»¥å…¬å‘Šæ—¥ä¸ºPITè§‚å¯Ÿæ—¶ç‚¹ï¼‰
                            as_of = pd.to_datetime(ann_date).date().isoformat() if hasattr(pd.to_datetime(ann_date), 'date') else str(ann_date)
                            res = self.calculator.calculate_indicators_for_date(
                                as_of_date=as_of,
                                stock_codes=[ts_code],
                                batch_size=batch_size,
                                target_data_sources=None  # è®©è®¡ç®—å™¨å†…éƒ¨å¤„ç†æ•°æ®æºä¼˜å…ˆçº§
                            )

                            success_count = int(res.get('success_count', 0))
                            total_processed += success_count

                            if success_count > 0:
                                processed_dates.append(f"{ann_date}_{end_date}")
                                self.logger.debug(f"è‚¡ç¥¨ {ts_code} å…¬å‘Šæ—¥æœŸ {ann_date} æŠ¥å‘ŠæœŸ {end_date} è®¡ç®—æˆåŠŸ: {success_count} æ¡è®°å½•")

                        except Exception as e:
                            self.logger.warning(f"è‚¡ç¥¨ {ts_code} å…¬å‘Šæ—¥æœŸ {ann_date} æŠ¥å‘ŠæœŸ {end_date} è®¡ç®—å¤±è´¥: {e}")
                            continue

                except Exception as e:
                    self.logger.warning(f"è‚¡ç¥¨ {ts_code} å…¬å‘Šæ—¥æœŸ {ann_date} è®¡ç®—å¤±è´¥: {e}")
                    continue

            self.logger.info(f"å†å²å›å¡«å®Œæˆ: å…±å¤„ç† {len(processed_dates)} ä¸ªå…¬å‘Šæ—¥æœŸï¼Œç”Ÿæˆ {total_processed} æ¡è´¢åŠ¡æŒ‡æ ‡è®°å½•")

            out = {
                'ts_code': ts_code,
                'backfilled_records': total_processed,
                'processed_dates': len(processed_dates),
                'message': f"å•è‚¡è´¢åŠ¡æŒ‡æ ‡å†å²å›å¡«å®Œæˆï¼Œå…±å¤„ç† {len(processed_dates)} ä¸ªå…¬å‘Šæ—¥æœŸï¼Œç”Ÿæˆ {total_processed} æ¡è®°å½•"
            }

            # 3. å¯é€‰ï¼šé’ˆå¯¹è¯¥è‚¡åšè½»é‡éªŒè¯
            if do_validate:
                try:
                    out['validation'] = self._validate_single_stock(ts_code, start_date, end_date)
                except Exception as ve:
                    self.logger.warning(f"å•è‚¡å›å¡«éªŒè¯å¤±è´¥ï¼ˆå¿½ç•¥ä¸ä¸­æ–­ï¼‰: {ve}")
            return out

        except Exception as e:
            self.logger.error(f"ä¸ªè‚¡è´¢åŠ¡æŒ‡æ ‡å†å²å›å¡«å¤±è´¥: {e}")
            return {
                'ts_code': ts_code,
                'backfilled_records': 0,
                'error': str(e),
                'message': 'ä¸ªè‚¡è´¢åŠ¡æŒ‡æ ‡å†å²å›å¡«å¤±è´¥'
            }

    def _get_table_columns(self, schema: str, table: str) -> set:
        """è·å–è¡¨çš„åˆ—åé›†åˆ"""
        sql = f"SELECT column_name FROM information_schema.columns WHERE table_schema=%s AND table_name=%s"
        try:
            df = self.context.query_dataframe(sql, (schema, table))
            return set(df['column_name'].tolist()) if df is not None else set()
        except Exception:
            return set()

    def _validate_single_stock(self, ts_code: str, start_date: str, end_date: str) -> Dict[str, Any]:
        """å¯¹æŒ‡å®šè‚¡ç¥¨åœ¨ç»™å®šæ—¥æœŸèŒƒå›´å†…è¿›è¡Œè½»é‡éªŒè¯ï¼Œè¾…åŠ©æ ¸å¯¹å¤„ç†é€»è¾‘æ­£ç¡®æ€§ã€‚
        éªŒè¯å†…å®¹ï¼š
        - ç»Ÿè®¡è®°å½•è¡Œæ•°
        - æ ¸å¿ƒå­—æ®µæ˜¯å¦å…¨éƒ¨ä¸ºç©ºçš„è®°å½•æ•°é‡ï¼ˆåº”å°½é‡ä¸º0ï¼‰
        - ts_code/end_date å…³é”®å­—æ®µå®Œæ•´æ€§
        """
        pit_cols = self._get_table_columns(PITConfig.PIT_SCHEMA, self.table_name)
        core_fields = [c for c in self.data_fields if c in pit_cols]
        select_cols = ['ts_code', 'end_date'] + ([
            'as_of_date'] if 'as_of_date' in pit_cols else []) + core_fields
        sql = (
            f"SELECT {', '.join(select_cols)} FROM {PITConfig.PIT_SCHEMA}.{self.table_name} "
            f"WHERE ts_code=%s"
        )
        df = self.context.query_dataframe(sql, (ts_code,))
        if df is None or df.empty:
            return {'ts_code': ts_code, 'range': [start_date, end_date], 'rows': 0}

        work = df.copy()

        # ç»Ÿè®¡
        # æ ¸å¿ƒå­—æ®µå…¨ç©ºç»Ÿè®¡
        if core_fields:
            all_null = work[core_fields].isna().all(axis=1)
            null_count = int(all_null.sum())
        else:
            null_count = 0

        issues = 0
        if null_count > 0:
            issues += 1

        key_null = int(work[['ts_code','end_date']].isna().any(axis=1).sum())
        if key_null > 0:
            issues += 1

        result = {
            'ts_code': ts_code,
            'range': [start_date, end_date],
            'rows': int(len(work)),
            'all_core_null_rows': null_count,
            'key_field_null_rows': key_null,
            'status': 'passed' if issues == 0 else 'warning'
        }
        return result


def main():
    """ä¸»å‡½æ•° - å‘½ä»¤è¡Œæ¥å£"""

    parser = argparse.ArgumentParser(
        description='PITè´¢åŠ¡æŒ‡æ ‡ç®¡ç†å™¨',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:

# å…¨é‡å›å¡«
python pit_financial_indicators_manager.py --mode full-backfill --start-date 2020-01-01 --end-date 2024-12-31

# å¢é‡æ›´æ–°
python pit_financial_indicators_manager.py --mode incremental --days 7

# å•è‚¡å›å¡«
python pit_financial_indicators_manager.py --mode single-backfill --ts-code 600000.SH

# æ˜¾ç¤ºè¡¨çŠ¶æ€
python pit_financial_indicators_manager.py --status

# éªŒè¯æ•°æ®å®Œæ•´æ€§
python pit_financial_indicators_manager.py --validate
        """
    )
    parser.add_argument('--mode', choices=['full-backfill', 'incremental', 'single-backfill'],
                       help='æ‰§è¡Œæ¨¡å¼')
    parser.add_argument('--start-date', help='å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--end-date', help='ç»“æŸæ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--days', type=int, help='å¢é‡æ›´æ–°å¤©æ•°')
    parser.add_argument('--batch-size', type=int, help='æ¯æ‰¹è‚¡ç¥¨æ•°ï¼ˆæŒ‰ ts_code åˆ†æ‰¹ï¼‰')
    # å¼ºåˆ¶æ­£åºï¼Œä¸å†æä¾› fill-order é€‰é¡¹
    parser.add_argument('--status', action='store_true', help='æ˜¾ç¤ºè¡¨çŠ¶æ€')
    parser.add_argument('--validate', action='store_true', help='éªŒè¯æ•°æ®å®Œæ•´æ€§')
    parser.add_argument('--ts-code', help='æŒ‡å®šå•è‚¡ ts_codeï¼ˆå¦‚ 600000.SHï¼‰ï¼Œç”¨äº single-backfill æ¨¡å¼')

    args = parser.parse_args()

    # åˆå§‹åŒ–ç»Ÿä¸€æ—¥å¿—ï¼ˆæ–¹æ¡ˆCï¼‰ï¼šæ§åˆ¶å° +ï¼ˆå¯é€‰ï¼‰æ–‡ä»¶
    try:
        from alphahome.common.logging_utils import setup_logging
        # æ–‡ä»¶åæŒ‰è¡¨ååŒºåˆ†ï¼Œé¿å…æ··æ·†
        log_fn = f"pit_financial_indicators_{datetime.now().strftime('%Y%m%d')}.log"
        setup_logging(log_level="INFO", log_to_file=True, log_dir="logs", log_filename=log_fn)
    except Exception:
        # å¿½ç•¥æ—¥å¿—åˆå§‹åŒ–å¼‚å¸¸ï¼Œç»§ç»­æ‰§è¡Œ
        pass

    print("ğŸ“Š PITè´¢åŠ¡æŒ‡æ ‡ç®¡ç†å™¨")
    print("=" * 60)

    try:
        with PITFinancialIndicatorsManager() as manager:

            # æ˜¾ç¤ºè¡¨çŠ¶æ€
            if args.status:
                print("ğŸ“ˆ è¡¨çŠ¶æ€:")
                status = manager.get_table_status()
                for key, value in status.items():
                    print(f"  {key}: {value}")
                return 0

            # ä»…å½“æœªæŒ‡å®š mode æ—¶ï¼Œå•ç‹¬æ‰§è¡Œå…¨è¡¨éªŒè¯
            if args.validate and not args.mode:
                print("ğŸ” æ•°æ®å®Œæ•´æ€§éªŒè¯:")
                validation = manager.validate_data_integrity()
                print(f"  æ€»ä½“çŠ¶æ€: {validation['overall_status']}")
                print(f"  å‘ç°é—®é¢˜: {validation['issues_found']} ä¸ª")
                for check in validation['checks']:
                    status_icon = "âœ…" if check['status'] == 'passed' else "âŒ"
                    print(f"  {status_icon} {check['check_name']}: {check['message']}")
                return 0

            # æ‰§è¡Œä¸»è¦åŠŸèƒ½
            if args.mode and args.mode == 'full-backfill':
                result = manager.full_backfill(
                    start_date=args.start_date,
                    end_date=args.end_date,
                    batch_size=args.batch_size
                )
            elif args.mode and args.mode == 'incremental':
                result = manager.incremental_update(
                    days=args.days,
                    batch_size=args.batch_size
                )
            elif args.mode and args.mode == 'single-backfill':
                result = manager.single_backfill(
                    ts_code=args.ts_code,
                    start_date=args.start_date,
                    end_date=args.end_date,
                    batch_size=args.batch_size,
                    do_validate=args.validate
                )

            if args.mode:
                print(f"\nâœ… æ‰§è¡Œç»“æœ:")
                for key, value in result.items():
                    print(f"  {key}: {value}")

                # è‹¥æŒ‡å®šäº† validate ä¸”éå•è‚¡æ¨¡å¼ï¼Œåˆ™åœ¨æ‰§è¡Œåè¿›è¡Œå…¨è¡¨æ•°æ®éªŒè¯
                if args.validate and args.mode != 'single-backfill':
                    print("\nğŸ” æ‰§è¡Œåæ•°æ®å®Œæ•´æ€§éªŒè¯:")
                    validation = manager.validate_data_integrity()
                    print(f"  æ€»ä½“çŠ¶æ€: {validation['overall_status']}")
                    print(f"  å‘ç°é—®é¢˜: {validation['issues_found']} ä¸ª")
                    for check in validation['checks']:
                        status_icon = "âœ…" if check['status'] == 'passed' else "âŒ"
                        print(f"  {status_icon} {check['check_name']}: {check['message']}")

                return 0 if 'error' not in result else 1

            return 0

    except Exception as e:
        print(f"\nâŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)

