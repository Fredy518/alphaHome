#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
PITè¡Œä¸šåˆ†ç±»ç®¡ç†å™¨
================

è´Ÿè´£pit_industry_classificationè¡¨çš„å†å²å…¨é‡å›å¡«å’Œå¢é‡æ›´æ–°

åŠŸèƒ½ç‰¹ç‚¹:
1. åŸºäºæœˆåº¦å¿«ç…§æœºåˆ¶ç®¡ç†è¡Œä¸šåˆ†ç±»æ•°æ®
2. æ”¯æŒç”³ä¸‡å’Œä¸­ä¿¡åŒé‡åˆ†ç±»ä½“ç³»
3. è‡ªåŠ¨æ£€æµ‹è¡Œä¸šå˜æ›´å¹¶ç”Ÿæˆæ–°å¿«ç…§
4. æä¾›å†å²å…¨é‡å›å¡«å’Œå¢é‡æ›´æ–°

Author: AI Assistant
Date: 2025-08-11
"""

import sys
import os
import argparse
from datetime import datetime, date, timedelta
from dateutil.relativedelta import relativedelta
from typing import Dict, List, Optional, Any
import pandas as pd

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

try:
    from .base.pit_table_manager import PITTableManager
    from .base.pit_config import PITConfig
except ImportError:
    from base.pit_table_manager import PITTableManager
    from base.pit_config import PITConfig

class PITIndustryClassificationManager(PITTableManager):
    """PITè¡Œä¸šåˆ†ç±»ç®¡ç†å™¨"""
    
    def __init__(self):
        super().__init__('pit_industry_classification')
        
        # è¡Œä¸šåˆ†ç±»ç‰¹å®šé…ç½®
        self.tushare_tables = self.table_config['tushare_tables']
        self.key_fields = self.table_config['key_fields']
        self.data_fields = self.table_config['data_fields']
        self.snapshot_mode = self.table_config.get('snapshot_mode', True)
    
    def full_backfill(self, 
                     start_date: str = None, 
                     end_date: str = None,
                     batch_size: int = None) -> Dict[str, Any]:
        """
        å†å²å…¨é‡å›å¡« - ç”Ÿæˆå†å²æœˆåº¦å¿«ç…§
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            batch_size: æ‰¹æ¬¡å¤§å°ï¼ˆæœˆä»½æ•°ï¼‰
            
        Returns:
            æ‰§è¡Œç»“æœç»Ÿè®¡
        """
        self.logger.info("å¼€å§‹PITè¡Œä¸šåˆ†ç±»å†å²å…¨é‡å›å¡«")
        
        # è®¾ç½®é»˜è®¤å‚æ•°
        if start_date is None or end_date is None:
            start_date, end_date = PITConfig.get_backfill_date_range(start_date, end_date)
        
        if batch_size is None:
            batch_size = 12  # é»˜è®¤12ä¸ªæœˆä¸ºä¸€æ‰¹
        
        self.logger.info(f"å›å¡«æ—¥æœŸèŒƒå›´: {start_date} ~ {end_date}")
        self.logger.info(f"æ‰¹æ¬¡å¤§å°: {batch_size} ä¸ªæœˆ")
        
        try:
            # 0. ç¡®ä¿ç›®æ ‡è¡¨å­˜åœ¨
            self._ensure_table_exists()
            # 1. è·å–éœ€è¦å›å¡«çš„æœˆä»½
            missing_months = self._find_missing_months(start_date, end_date)

            if not missing_months:
                self.logger.info("æ²¡æœ‰éœ€è¦å›å¡«çš„å†å²æ•°æ®")
                return {'backfilled_records': 0, 'message': 'æ•°æ®å·²å®Œæ•´'}
            
            self.logger.info(f"éœ€è¦å›å¡« {len(missing_months)} ä¸ªæœˆçš„æ•°æ®")
            
            # 2. åˆ†æ‰¹å¤„ç†æœˆä»½
            total_records = 0
            backfilled_months = 0
            
            for i in range(0, len(missing_months), batch_size):
                batch_months = missing_months[i:i + batch_size]
                
                self.logger.info(f"å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}: {len(batch_months)} ä¸ªæœˆ")
                
                try:
                    batch_records = self._process_month_batch(batch_months)
                    total_records += batch_records
                    backfilled_months += len(batch_months)
                    
                    self.logger.info(f"æ‰¹æ¬¡å®Œæˆ: {batch_records} æ¡è®°å½•")
                    
                except Exception as e:
                    self.logger.error(f"æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}")
                    continue
            
            return {
                'backfilled_records': total_records,
                'backfilled_months': backfilled_months,
                'message': f'æˆåŠŸå›å¡« {backfilled_months} ä¸ªæœˆï¼Œ{total_records} æ¡è®°å½•'
            }
            
        except Exception as e:
            self.logger.error(f"å†å²å›å¡«å¤±è´¥: {e}")
            return {
                'backfilled_records': 0,
                'error': str(e),
                'message': 'å†å²å›å¡«å¤±è´¥'
            }
    
    def incremental_update(self, 
                          months: int = None,
                          batch_size: int = None) -> Dict[str, Any]:
        """
        å¢é‡æ›´æ–° - æ£€æµ‹è¡Œä¸šå˜æ›´å¹¶æ›´æ–°å¿«ç…§
        
        Args:
            months: æ£€æŸ¥æœ€è¿‘å‡ ä¸ªæœˆçš„å˜æ›´
            batch_size: æ‰¹æ¬¡å¤§å°
            
        Returns:
            æ‰§è¡Œç»“æœç»Ÿè®¡
        """
        self.logger.info("å¼€å§‹PITè¡Œä¸šåˆ†ç±»å¢é‡æ›´æ–°")
        
        # è®¾ç½®é»˜è®¤å‚æ•°
        if months is None:
            months = 3  # é»˜è®¤æ£€æŸ¥æœ€è¿‘3ä¸ªæœˆ
        
        # è®¡ç®—æ£€æŸ¥æ—¥æœŸèŒƒå›´
        end_date = datetime.now().date()
        start_date = end_date - relativedelta(months=months)
        
        self.logger.info(f"æ£€æŸ¥å˜æ›´æ—¥æœŸèŒƒå›´: {start_date} ~ {end_date}")
        
        try:
            # 0. ç¡®ä¿ç›®æ ‡è¡¨å­˜åœ¨
            self._ensure_table_exists()
            # 1. æ£€æµ‹è¡Œä¸šå˜æ›´
            changes = self._detect_industry_changes(start_date.strftime('%Y-%m-%d'))

            if not changes['has_changes']:
                self.logger.info("æœªæ£€æµ‹åˆ°è¡Œä¸šå˜æ›´")
                return {'updated_records': 0, 'message': 'æ— è¡Œä¸šå˜æ›´'}
            
            self.logger.info(f"æ£€æµ‹åˆ°è¡Œä¸šå˜æ›´: SW {changes['sw_changes']}, CI {changes['ci_changes']}")
            
            # 2. è·å–å—å½±å“çš„æœˆä»½
            affected_months = self._get_affected_months(start_date.strftime('%Y-%m-%d'))
            
            # 3. é‡æ–°ç”Ÿæˆå—å½±å“æœˆä»½çš„å¿«ç…§
            total_records = 0
            
            for month_date in affected_months:
                self.logger.info(f"é‡æ–°ç”Ÿæˆå¿«ç…§: {month_date}")
                
                try:
                    # åˆ é™¤ç°æœ‰å¿«ç…§
                    self._delete_existing_snapshot(month_date)
                    
                    # ç”Ÿæˆæ–°å¿«ç…§
                    month_records = self._generate_monthly_snapshot(month_date)
                    total_records += month_records
                    
                    self.logger.info(f"å¿«ç…§ {month_date} é‡æ–°ç”Ÿæˆå®Œæˆ: {month_records} æ¡è®°å½•")
                    
                except Exception as e:
                    self.logger.error(f"é‡æ–°ç”Ÿæˆå¿«ç…§ {month_date} å¤±è´¥: {e}")
                    continue
            
            return {
                'updated_records': total_records,
                'affected_months': len(affected_months),
                'message': f'åŸºäºè¡Œä¸šå˜æ›´æ›´æ–°äº† {len(affected_months)} ä¸ªæœˆåº¦å¿«ç…§'
            }
            
        except Exception as e:
            self.logger.error(f"å¢é‡æ›´æ–°å¤±è´¥: {e}")
            return {
                'updated_records': 0,
                'error': str(e),
                'message': 'å¢é‡æ›´æ–°å¤±è´¥'
            }
    
    def _find_missing_months(self, start_date: str, end_date: str) -> List[date]:
        """æŸ¥æ‰¾ç¼ºå¤±çš„æœˆä»½"""
        
        start_dt = datetime.strptime(start_date, '%Y-%m-%d').date().replace(day=1)
        end_dt = datetime.strptime(end_date, '%Y-%m-%d').date().replace(day=1)
        
        # è·å–ç°æœ‰çš„æœˆä»½
        existing_months_query = """
        SELECT DISTINCT DATE_TRUNC('month', obs_date)::date as month_date
        FROM pgs_factors.pit_industry_classification
        WHERE obs_date >= %s AND obs_date <= %s
        ORDER BY month_date
        """
        
        existing_result = self.context.query_dataframe(
            existing_months_query, 
            (start_dt, self._get_month_end_date(end_dt))
        )
        
        existing_months = set()
        if existing_result is not None and not existing_result.empty:
            existing_months = set(existing_result['month_date'].tolist())
        
        # ç”Ÿæˆåº”è¯¥å­˜åœ¨çš„æ‰€æœ‰æœˆä»½
        should_exist_months = []
        current_month = start_dt
        
        while current_month <= end_dt:
            should_exist_months.append(current_month)
            current_month = current_month + relativedelta(months=1)
        
        # æ‰¾å‡ºç¼ºå¤±çš„æœˆä»½
        missing_months = [month for month in should_exist_months if month not in existing_months]
        
        return missing_months
    
    def _process_month_batch(self, months: List[date]) -> int:
        """å¤„ç†æœˆä»½æ‰¹æ¬¡"""
        
        total_records = 0
        
        for month_date in months:
            month_end = self._get_month_end_date(month_date)
            
            self.logger.info(f"ç”Ÿæˆå¿«ç…§: {month_end}")
            
            # ç”Ÿæˆç”³ä¸‡å¿«ç…§
            sw_records = self._generate_industry_snapshot('sw', month_end)
            total_records += len(sw_records)
            
            # ç”Ÿæˆä¸­ä¿¡å¿«ç…§
            ci_records = self._generate_industry_snapshot('ci', month_end)
            total_records += len(ci_records)
            
            # æ‰¹é‡æ’å…¥
            all_records = sw_records + ci_records
            if all_records:
                self._insert_industry_snapshot_batch(all_records)
                self.logger.info(f"å¿«ç…§ {month_end}: SW {len(sw_records)}, CI {len(ci_records)}")
        
        return total_records
    
    def _generate_monthly_snapshot(self, month_date: date) -> int:
        """ç”ŸæˆæŒ‡å®šæœˆä»½çš„è¡Œä¸šåˆ†ç±»å¿«ç…§"""
        
        # è®¡ç®—æœˆæœ«æ—¥æœŸ
        month_end = self._get_month_end_date(month_date)
        
        total_records = 0
        
        # ç”Ÿæˆç”³ä¸‡å¿«ç…§
        sw_records = self._generate_industry_snapshot('sw', month_end)
        total_records += len(sw_records)
        
        # ç”Ÿæˆä¸­ä¿¡å¿«ç…§
        ci_records = self._generate_industry_snapshot('ci', month_end)
        total_records += len(ci_records)
        
        # æ‰¹é‡æ’å…¥
        all_records = sw_records + ci_records
        if all_records:
            self._insert_industry_snapshot_batch(all_records)
        
        return total_records
    
    def _generate_industry_snapshot(self, data_source: str, snapshot_date: date) -> List[Dict]:
        """ç”ŸæˆæŒ‡å®šæ•°æ®æºçš„è¡Œä¸šåˆ†ç±»å¿«ç…§"""
        
        # ç¡®å®štushareè¡¨å
        tushare_table = 'index_swmember' if data_source == 'sw' else 'index_cimember'
        
        # æŸ¥è¯¢åœ¨å¿«ç…§æ—¥æœŸæœ‰æ•ˆçš„è¡Œä¸šåˆ†ç±»
        query = f"""
        SELECT 
            ts_code,
            l1_code, l1_name,
            l2_code, l2_name, 
            l3_code, l3_name,
            in_date, out_date
        FROM tushare.{tushare_table}
        WHERE (
            (in_date <= %s AND (out_date IS NULL OR out_date > %s))
            OR 
            (in_date <= %s AND out_date IS NULL)
        )
        AND l1_name IS NOT NULL
        ORDER BY ts_code, in_date DESC
        """
        
        industry_data = self.context.query_dataframe(
            query, 
            (snapshot_date, snapshot_date, snapshot_date)
        )
        
        if industry_data is None or industry_data.empty:
            self.logger.warning(f"æœªæ‰¾åˆ° {data_source} åœ¨ {snapshot_date} çš„è¡Œä¸šæ•°æ®")
            return []
        
        # æ¯åªè‚¡ç¥¨å–æœ€æ–°çš„è¡Œä¸šåˆ†ç±»
        latest_data = industry_data.groupby('ts_code').first().reset_index()
        
        # è½¬æ¢ä¸ºPITæ ¼å¼
        pit_records = []
        
        for _, row in latest_data.iterrows():
            # ç¡®å®šç‰¹æ®Šå¤„ç†æ ‡è¯†
            requires_special_gpa = self._is_financial_industry(row['l1_name'], row['l2_name'])
            gpa_method = 'null' if requires_special_gpa else 'standard'
            special_reason = self._get_special_handling_reason(row['l1_name'], row['l2_name']) if requires_special_gpa else None
            
            pit_record = {
                'ts_code': row['ts_code'],
                'obs_date': snapshot_date,
                'data_source': data_source,
                'industry_level1': row['l1_name'],
                'industry_level2': row['l2_name'],
                'industry_level3': row['l3_name'],
                'industry_code1': row['l1_code'],
                'industry_code2': row['l2_code'],
                'industry_code3': row['l3_code'],
                'requires_special_gpa_handling': requires_special_gpa,
                'gpa_calculation_method': gpa_method,
                'special_handling_reason': special_reason,
                'data_quality': 'normal',
                'snapshot_version': f"backfill_{snapshot_date.strftime('%Y-%m')}"
            }
            pit_records.append(pit_record)
        
        return pit_records

    def ensure_table_exists(self) -> None:
        """ç¡®ä¿è¡Œä¸šåˆ†ç±»è¡¨å­˜åœ¨ï¼ˆæ”¯æŒæœ¬åœ°DDLï¼‰"""
        import os
        try:
            sql_path = os.path.join(os.path.dirname(__file__), 'database', 'create_pit_industry_classification_table.sql')
            sql_path = os.path.normpath(sql_path)
            if not os.path.exists(sql_path):
                self.logger.warning(f"æœªæ‰¾åˆ°è¡Œä¸šåˆ†ç±»å»ºè¡¨SQL: {sql_path}")
                return
            with open(sql_path, 'r', encoding='utf-8') as f:
                ddl = f.read()
            self.context.db_manager.execute_sync(ddl)
            self.logger.info("è¡Œä¸šåˆ†ç±»è¡¨åˆ›å»º/éªŒè¯å®Œæˆ")
        except Exception as e:
            self.logger.error(f"åˆ›å»ºè¡Œä¸šåˆ†ç±»è¡¨å¤±è´¥: {e}")

    def _insert_industry_snapshot_batch(self, records: List[Dict]):
        """æ‰¹é‡æ’å…¥è¡Œä¸šåˆ†ç±»å¿«ç…§"""

        if not records:
            return

        # æ„å»ºUPSERT SQL
        insert_sql = """
        INSERT INTO pgs_factors.pit_industry_classification (
            ts_code, obs_date, data_source,
            industry_level1, industry_level2, industry_level3,
            industry_code1, industry_code2, industry_code3,
            requires_special_gpa_handling, gpa_calculation_method, special_handling_reason,
            data_quality, snapshot_version
        ) VALUES (
            %(ts_code)s, %(obs_date)s, %(data_source)s,
            %(industry_level1)s, %(industry_level2)s, %(industry_level3)s,
            %(industry_code1)s, %(industry_code2)s, %(industry_code3)s,
            %(requires_special_gpa_handling)s, %(gpa_calculation_method)s, %(special_handling_reason)s,
            %(data_quality)s, %(snapshot_version)s
        )
        ON CONFLICT (ts_code, obs_date, data_source) DO UPDATE SET
            industry_level1 = EXCLUDED.industry_level1,
            industry_level2 = EXCLUDED.industry_level2,
            industry_level3 = EXCLUDED.industry_level3,
            industry_code1 = EXCLUDED.industry_code1,
            industry_code2 = EXCLUDED.industry_code2,
            industry_code3 = EXCLUDED.industry_code3,
            requires_special_gpa_handling = EXCLUDED.requires_special_gpa_handling,
            gpa_calculation_method = EXCLUDED.gpa_calculation_method,
            special_handling_reason = EXCLUDED.special_handling_reason,
            data_quality = EXCLUDED.data_quality,
            snapshot_version = EXCLUDED.snapshot_version,
            updated_at = CURRENT_TIMESTAMP
        """

        # åˆ†æ‰¹æ’å…¥
        batch_size = 1000
        for i in range(0, len(records), batch_size):
            batch = records[i:i + batch_size]

            for record in batch:
                self.context.db_manager.execute_sync(insert_sql, record)

    def _detect_industry_changes(self, since_date: str) -> Dict:
        """æ£€æµ‹è¡Œä¸šå˜æ›´"""

        since_dt = datetime.strptime(since_date, '%Y-%m-%d').date()

        # æ£€æŸ¥ç”³ä¸‡æ•°æ®å˜æ›´
        sw_changes = self.context.query_dataframe("""
            SELECT COUNT(*) as change_count
            FROM tushare.index_swmember
            WHERE in_date > %s OR out_date > %s
        """, (since_dt, since_dt))

        # æ£€æŸ¥ä¸­ä¿¡æ•°æ®å˜æ›´
        ci_changes = self.context.query_dataframe("""
            SELECT COUNT(*) as change_count
            FROM tushare.index_cimember
            WHERE in_date > %s OR out_date > %s
        """, (since_dt, since_dt))

        sw_count = sw_changes.iloc[0]['change_count'] if sw_changes is not None and not sw_changes.empty else 0
        ci_count = ci_changes.iloc[0]['change_count'] if ci_changes is not None and not ci_changes.empty else 0

        return {
            'has_changes': sw_count > 0 or ci_count > 0,
            'sw_changes': sw_count,
            'ci_changes': ci_count
        }

    def _get_affected_months(self, since_date: str) -> List[date]:
        """è·å–å—è¡Œä¸šå˜æ›´å½±å“çš„æœˆä»½"""

        since_dt = datetime.strptime(since_date, '%Y-%m-%d').date()
        current_date = datetime.now().date()

        # ä»å˜æ›´å¼€å§‹æ—¥æœŸåˆ°å½“å‰æ—¥æœŸçš„æ‰€æœ‰æœˆä»½
        affected_months = []
        current_month = since_dt.replace(day=1)
        end_month = current_date.replace(day=1)

        while current_month <= end_month:
            affected_months.append(current_month)
            current_month = current_month + relativedelta(months=1)

        return affected_months

    def _delete_existing_snapshot(self, month_date: date):
        """åˆ é™¤ç°æœ‰å¿«ç…§"""

        month_end = self._get_month_end_date(month_date)

        delete_sql = """
        DELETE FROM pgs_factors.pit_industry_classification
        WHERE obs_date = %s
        """

        self.context.db_manager.execute_sync(delete_sql, (month_end,))
        self.logger.info(f"åˆ é™¤ç°æœ‰å¿«ç…§: {month_end}")

    def _get_month_end_date(self, month_start: date) -> date:
        """è·å–æœˆæœ«æ—¥æœŸ"""
        if month_start.month == 12:
            next_month = month_start.replace(year=month_start.year + 1, month=1)
        else:
            next_month = month_start.replace(month=month_start.month + 1)

        return next_month - timedelta(days=1)

    def _is_financial_industry(self, l1_name: str, l2_name: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºé‡‘èè¡Œä¸š"""
        financial_keywords = [
            'é“¶è¡Œ', 'è¯åˆ¸', 'ä¿é™©', 'ä¿¡æ‰˜', 'æœŸè´§', 'åŸºé‡‘',
            'é‡‘è', 'æŠ•èµ„', 'èµ„äº§ç®¡ç†', 'è´¢åŠ¡å…¬å¸'
        ]

        industry_text = f"{l1_name} {l2_name}".lower()

        for keyword in financial_keywords:
            if keyword in industry_text:
                return True

        return False

    def _get_special_handling_reason(self, l1_name: str, l2_name: str) -> str:
        """è·å–ç‰¹æ®Šå¤„ç†åŸå› """
        if 'é“¶è¡Œ' in f"{l1_name} {l2_name}":
            return "é“¶è¡Œä¸šè¥ä¸šæˆæœ¬ä¸º0å¯¼è‡´GPA=100%ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†"
        elif 'è¯åˆ¸' in f"{l1_name} {l2_name}":
            return "è¯åˆ¸ä¸šæˆæœ¬ç»“æ„ç‰¹æ®Šï¼ŒGPAæŒ‡æ ‡ä¸é€‚ç”¨"
        elif 'ä¿é™©' in f"{l1_name} {l2_name}":
            return "ä¿é™©ä¸šæˆæœ¬ç»“æ„ç‰¹æ®Šï¼ŒGPAæŒ‡æ ‡ä¸é€‚ç”¨"
        else:
            return "é‡‘èä¸šæˆæœ¬ç»“æ„ç‰¹æ®Šï¼ŒGPAæŒ‡æ ‡å¯èƒ½ä¸é€‚ç”¨"

def main():
    """ä¸»å‡½æ•° - å‘½ä»¤è¡Œæ¥å£"""

    parser = argparse.ArgumentParser(description='PITè¡Œä¸šåˆ†ç±»ç®¡ç†å™¨')
    parser.add_argument('--mode', choices=['full-backfill', 'incremental'],
                       required=True, help='æ‰§è¡Œæ¨¡å¼')
    parser.add_argument('--start-date', help='å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--end-date', help='ç»“æŸæ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--months', type=int, help='å¢é‡æ›´æ–°æ£€æŸ¥æœˆæ•°')
    parser.add_argument('--batch-size', type=int, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--status', action='store_true', help='æ˜¾ç¤ºè¡¨çŠ¶æ€')
    parser.add_argument('--validate', action='store_true', help='éªŒè¯æ•°æ®å®Œæ•´æ€§')

    args = parser.parse_args()

    print("ğŸ­ PITè¡Œä¸šåˆ†ç±»ç®¡ç†å™¨")
    print("=" * 60)

    try:
        with PITIndustryClassificationManager() as manager:

            # æ˜¾ç¤ºè¡¨çŠ¶æ€
            if args.status:
                print("ğŸ“ˆ è¡¨çŠ¶æ€:")
                status = manager.get_table_status()
                for key, value in status.items():
                    print(f"  {key}: {value}")
                return 0

            # éªŒè¯æ•°æ®å®Œæ•´æ€§
            if args.validate:
                print("ğŸ” æ•°æ®å®Œæ•´æ€§éªŒè¯:")
                validation = manager.validate_data_integrity()
                print(f"  æ€»ä½“çŠ¶æ€: {validation['overall_status']}")
                print(f"  å‘ç°é—®é¢˜: {validation['issues_found']} ä¸ª")
                for check in validation['checks']:
                    status_icon = "âœ…" if check['status'] == 'passed' else "âŒ"
                    print(f"  {status_icon} {check['check_name']}: {check['message']}")
                return 0

            # æ‰§è¡Œä¸»è¦åŠŸèƒ½
            if args.mode == 'full-backfill':
                result = manager.full_backfill(
                    start_date=args.start_date,
                    end_date=args.end_date,
                    batch_size=args.batch_size
                )
            elif args.mode == 'incremental':
                result = manager.incremental_update(
                    months=args.months,
                    batch_size=args.batch_size
                )

            print(f"\nâœ… æ‰§è¡Œç»“æœ:")
            for key, value in result.items():
                print(f"  {key}: {value}")

            return 0 if 'error' not in result else 1

    except Exception as e:
        print(f"\nâŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
