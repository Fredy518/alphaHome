#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Tushare æ•°æ®æºæ™ºèƒ½å¢é‡æ›´æ–°ç”Ÿäº§è„šæœ¬
è‡ªåŠ¨æ‰§è¡Œæ‰€æœ‰ tushare fetchers ä»»åŠ¡çš„æ™ºèƒ½å¢é‡æ›´æ–°

ä½¿ç”¨æ–¹æ³•ï¼š
python scripts/production/data_updaters/tushare/tushare_smart_update_production.py --workers 5 --max_retries 3

åŠŸèƒ½ç‰¹æ€§ï¼š
- è‡ªåŠ¨å‘ç°æ‰€æœ‰ tushare ç›¸å…³çš„ fetch ä»»åŠ¡
- æ”¯æŒå¹¶è¡Œæ‰§è¡Œï¼Œæå‡æ›´æ–°æ•ˆç‡
- æ™ºèƒ½è·³è¿‡ä¸æ”¯æŒæ™ºèƒ½å¢é‡çš„ä»»åŠ¡
- è¯¦ç»†çš„æ‰§è¡Œæ—¥å¿—å’ŒçŠ¶æ€ç›‘æ§
- æ”¯æŒé‡è¯•æœºåˆ¶å’Œé”™è¯¯æ¢å¤
- ç”Ÿäº§çº§åˆ«çš„æ•°æ®ä¸€è‡´æ€§ä¿è¯
"""

import argparse
import asyncio
import logging
import sys
import time
from datetime import datetime
from typing import List, Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor

import pandas as pd

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, '.')

from alphahome.common.db_manager import create_async_manager
from alphahome.common.logging_utils import get_logger
from alphahome.common.task_system import UnifiedTaskFactory
from alphahome.common.constants import UpdateTypes
from alphahome.common.config_manager import get_database_url

logger = get_logger(__name__)


class TushareProductionUpdater:
    """Tushare æ•°æ®æºç”Ÿäº§çº§æ›´æ–°å™¨"""

    def __init__(self, max_workers: int = 3, max_retries: int = 3, retry_delay: int = 5, dry_run: bool = False):
        self.max_workers = max_workers
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.dry_run = dry_run
        self.db_manager = None
        self.executor = ThreadPoolExecutor(max_workers=max_workers)

        # Tushare API å¹¶å‘é™åˆ¶è¯´æ˜
        self.tushare_concurrency_note = """
        ğŸ“‹ Tushare API å¹¶å‘è¯´æ˜:
        - Tushare API æœ¬èº«æœ‰å¹¶å‘é™åˆ¶ï¼ˆé»˜è®¤20ä¸ªå¹¶å‘ï¼‰
        - ä¸åŒAPIæœ‰ä¸åŒé™åˆ¶ï¼ˆå¦‚daily:80, stock_basic:20ï¼‰
        - å»ºè®®è„šæœ¬å¹¶å‘æ•°ä¸è¶…è¿‡ Tushare API é™åˆ¶çš„ 1/2
        - å½“å‰è®¾ç½®: è„šæœ¬å¹¶å‘={}, Tushareé»˜è®¤å¹¶å‘=20
        """.format(max_workers)

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_tasks': 0,
            'successful_tasks': 0,
            'failed_tasks': 0,
            'skipped_tasks': 0,
            'start_time': None,
            'end_time': None
        }

    async def initialize(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥å’Œä»»åŠ¡å·¥å‚"""
        try:
            logger.info("æ­£åœ¨åˆå§‹åŒ–æ•°æ®åº“è¿æ¥...")

            # è·å–æ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²
            db_url = get_database_url()
            if not db_url:
                raise ValueError("æ— æ³•è·å–æ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²ï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶")

            self.db_manager = create_async_manager(db_url)
            await UnifiedTaskFactory.initialize()

            logger.info("âœ… æ•°æ®åº“è¿æ¥å’Œä»»åŠ¡å·¥å‚åˆå§‹åŒ–æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    async def get_tushare_tasks(self) -> List[str]:
        """è·å–æ‰€æœ‰ tushare ç›¸å…³çš„ fetch ä»»åŠ¡"""
        try:
            # è·å–æ‰€æœ‰å·²æ³¨å†Œçš„ä»»åŠ¡
            all_tasks = UnifiedTaskFactory.get_all_task_names()

            tushare_tasks = []
            for task_name in all_tasks:
                try:
                    task_info = UnifiedTaskFactory.get_task_info(task_name)

                    # ç­›é€‰æ¡ä»¶ï¼šdata_source ä¸º tushare ä¸” task_type ä¸º fetch
                    if (task_info.get('type') == 'fetch' and
                        hasattr(UnifiedTaskFactory._task_registry[task_name], 'data_source') and
                        UnifiedTaskFactory._task_registry[task_name].data_source == 'tushare'):

                        tushare_tasks.append(task_name)
                        logger.debug(f"å‘ç° Tushare ä»»åŠ¡: {task_name}")

                except Exception as e:
                    logger.warning(f"è·å–ä»»åŠ¡ä¿¡æ¯å¤±è´¥ {task_name}: {e}")
                    continue

            logger.info(f"âœ… å‘ç° {len(tushare_tasks)} ä¸ª Tushare fetch ä»»åŠ¡")
            return sorted(tushare_tasks)

        except Exception as e:
            logger.error(f"âŒ è·å– Tushare ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {e}")
            return []

    async def execute_task_with_retry(self, task_name: str, attempt: int = 1) -> Dict[str, Any]:
        """æ‰§è¡Œå•ä¸ªä»»åŠ¡ï¼Œæ”¯æŒé‡è¯•æœºåˆ¶"""
        try:
            logger.info(f"[{task_name}] å¼€å§‹æ‰§è¡Œ (å°è¯• {attempt}/{self.max_retries + 1})")

            # å¹²è¿è¡Œæ¨¡å¼ï¼šä¸å®é™…æ‰§è¡Œä»»åŠ¡
            if self.dry_run:
                logger.info(f"[{task_name}] å¹²è¿è¡Œæ¨¡å¼ï¼Œè·³è¿‡å®é™…æ‰§è¡Œ")
                return {
                    'task_name': task_name,
                    'status': 'skipped_dry_run',
                    'message': 'å¹²è¿è¡Œæ¨¡å¼',
                    'execution_time': 0.0,
                    'attempts': attempt
                }

            # åˆ›å»ºä»»åŠ¡å®ä¾‹
            task_instance = await UnifiedTaskFactory.create_task_instance(
                task_name,
                update_type=UpdateTypes.SMART  # ä½¿ç”¨æ™ºèƒ½å¢é‡æ¨¡å¼
            )

            # æ£€æŸ¥æ˜¯å¦æ”¯æŒæ™ºèƒ½å¢é‡æ›´æ–°
            if not task_instance.supports_incremental_update():
                skip_reason = getattr(task_instance, 'get_incremental_skip_reason', lambda: 'ä¸æ”¯æŒæ™ºèƒ½å¢é‡')()
                logger.warning(f"[{task_name}] è·³è¿‡: {skip_reason}")
                return {
                    'task_name': task_name,
                    'status': 'skipped',
                    'message': skip_reason,
                    'attempts': attempt
                }

            # æ‰§è¡Œä»»åŠ¡
            start_time = time.time()
            result = await task_instance.execute()
            execution_time = time.time() - start_time

            if isinstance(result, dict):
                task_status = result.get('status', 'unknown')
                if task_status == 'success':
                    logger.info(f"[{task_name}] æ‰§è¡ŒæˆåŠŸï¼Œè€—æ—¶: {execution_time:.2f}ç§’")
                    return {
                        'task_name': task_name,
                        'status': 'success',
                        'result': result,
                        'execution_time': execution_time,
                        'attempts': attempt
                    }
                elif task_status == 'partial_success':
                    logger.info(f"[{task_name}] éƒ¨åˆ†æˆåŠŸï¼ˆæœ‰éªŒè¯è­¦å‘Šï¼‰ï¼Œè€—æ—¶: {execution_time:.2f}ç§’")
                    return {
                        'task_name': task_name,
                        'status': 'partial_success',
                        'result': result,
                        'execution_time': execution_time,
                        'attempts': attempt
                    }
                else:
                    logger.warning(f"[{task_name}] æ‰§è¡Œå®Œæˆä½†çŠ¶æ€å¼‚å¸¸: {result}")
                    return {
                        'task_name': task_name,
                        'status': task_status,  # ä½¿ç”¨åŸå§‹çŠ¶æ€
                        'result': result,
                        'execution_time': execution_time,
                        'attempts': attempt
                    }
            else:
                logger.warning(f"[{task_name}] æ‰§è¡Œç»“æœæ ¼å¼å¼‚å¸¸: {result}")
                return {
                    'task_name': task_name,
                    'status': 'error',
                    'result': result,
                    'execution_time': execution_time,
                    'attempts': attempt
                }

        except Exception as e:
            logger.error(f"[{task_name}] æ‰§è¡Œå¤±è´¥ (å°è¯• {attempt}): {e}")

            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡è¯•
            if attempt <= self.max_retries:
                logger.info(f"[{task_name}] {self.retry_delay}ç§’åé‡è¯•...")
                await asyncio.sleep(self.retry_delay)
                return await self.execute_task_with_retry(task_name, attempt + 1)
            else:
                return {
                    'task_name': task_name,
                    'status': 'failed',
                    'error': str(e),
                    'attempts': attempt
                }

    async def execute_tasks_parallel(self, task_names: List[str]) -> List[Dict[str, Any]]:
        """å¹¶è¡Œæ‰§è¡Œå¤šä¸ªä»»åŠ¡"""
        logger.info(f"ğŸš€ å¼€å§‹å¹¶è¡Œæ‰§è¡Œ {len(task_names)} ä¸ªä»»åŠ¡ (æœ€å¤§å¹¶å‘: {self.max_workers})")

        # åˆ›å»ºä»»åŠ¡åˆ—è¡¨
        tasks = []
        semaphore = asyncio.Semaphore(self.max_workers)

        async def execute_with_semaphore(task_name: str):
            async with semaphore:
                return await self.execute_task_with_retry(task_name)

        # å¯åŠ¨æ‰€æœ‰ä»»åŠ¡
        for task_name in task_names:
            task = asyncio.create_task(execute_with_semaphore(task_name))
            tasks.append(task)

        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # å¤„ç†ç»“æœ
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                task_name = task_names[i]
                logger.error(f"[{task_name}] ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {result}")
                processed_results.append({
                    'task_name': task_name,
                    'status': 'error',
                    'error': str(result),
                    'attempts': 1
                })
            else:
                processed_results.append(result)

        return processed_results

    def print_execution_summary(self, results: List[Dict[str, Any]]):
        """æ‰“å°æ‰§è¡Œæ‘˜è¦"""
        total_time = self.stats['end_time'] - self.stats['start_time']
        total_time_minutes = total_time.total_seconds() / 60

        print("\n" + "="*80)
        print("ğŸ“Š Tushare æ™ºèƒ½å¢é‡æ›´æ–°æ‰§è¡Œæ‘˜è¦")
        print("="*80)
        print(f"æ‰§è¡Œæ—¶é—´: {self.stats['start_time'].strftime('%Y-%m-%d %H:%M:%S')} - {self.stats['end_time'].strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"æ€»è€—æ—¶: {total_time_minutes:.2f} åˆ†é’Ÿ")
        print(f"æ€»ä»»åŠ¡æ•°: {self.stats['total_tasks']}")
        print(f"âœ… æˆåŠŸä»»åŠ¡: {self.stats['successful_tasks']}")
        print(f"âŒ å¤±è´¥ä»»åŠ¡: {self.stats['failed_tasks']}")
        print(f"â­ï¸ è·³è¿‡ä»»åŠ¡: {self.stats['skipped_tasks']}")
        print(f"âš ï¸  å¼‚å¸¸ä»»åŠ¡: {sum(1 for r in results if r.get('status') == 'error' and isinstance(r, dict))}")
        print(f"ğŸ”¶ éƒ¨åˆ†æˆåŠŸ: {sum(1 for r in results if r.get('status') == 'partial_success' and isinstance(r, dict))}")
        print(f"æˆåŠŸç‡: {(self.stats['successful_tasks'] / max(self.stats['total_tasks'], 1) * 100):.2f}%")
        if self.stats['successful_tasks'] > 0:
            avg_time_per_task = sum(r.get('execution_time', 0) for r in results if r.get('execution_time')) / self.stats['successful_tasks']
            print(f"å¹³å‡ä»»åŠ¡è€—æ—¶: {avg_time_per_task:.2f}ç§’")
        print()

        # æ˜¾ç¤ºå¤±è´¥çš„ä»»åŠ¡è¯¦æƒ…
        failed_tasks = [r for r in results if isinstance(r, dict) and r.get('status') in ['failed', 'error']]
        if failed_tasks:
            print("âŒ å¤±è´¥ä»»åŠ¡è¯¦æƒ…:")
            for task in failed_tasks:
                error_msg = task.get('error', 'æœªçŸ¥é”™è¯¯')
                status = task.get('status', 'unknown')
                print(f"   - {task['task_name']} ({status}): {error_msg}")
            print()

        # æ˜¾ç¤ºéƒ¨åˆ†æˆåŠŸçš„ä»»åŠ¡è¯¦æƒ…
        partial_success_tasks = [r for r in results if isinstance(r, dict) and r.get('status') == 'partial_success']
        if partial_success_tasks:
            print("ğŸ”¶ éƒ¨åˆ†æˆåŠŸä»»åŠ¡è¯¦æƒ…:")
            for task in partial_success_tasks:
                result = task.get('result', {})
                validation_details = result.get('validation_details', {})
                failed_count = len(validation_details.get('failed_validations', {}))
                print(f"   - {task['task_name']}: {failed_count}ä¸ªéªŒè¯è§„åˆ™å¤±è´¥")
            print()

        # æ˜¾ç¤ºè·³è¿‡çš„ä»»åŠ¡è¯¦æƒ…
        skipped_tasks = [r for r in results if isinstance(r, dict) and r.get('status') == 'skipped']
        if skipped_tasks:
            print("â­ï¸ è·³è¿‡ä»»åŠ¡è¯¦æƒ…:")
            for task in skipped_tasks:
                print(f"   - {task['task_name']}: {task.get('message', 'ä¸æ”¯æŒæ™ºèƒ½å¢é‡')}")
            print()

        print("ğŸ¯ å»ºè®®:")
        if self.stats['failed_tasks'] > 0:
            print("   - æ£€æŸ¥å¤±è´¥ä»»åŠ¡çš„ç½‘ç»œè¿æ¥æˆ– API æƒé™")
            print("   - æŸ¥çœ‹è¯¦ç»†æ—¥å¿—äº†è§£å…·ä½“é”™è¯¯åŸå› ")
        if self.stats['successful_tasks'] / max(self.stats['total_tasks'], 1) < 0.8:
            print("   - æˆåŠŸç‡è¾ƒä½ï¼Œå»ºè®®é™ä½å¹¶å‘æ•°æˆ–å¢åŠ é‡è¯•æ¬¡æ•°")
        else:
            print("   - æ›´æ–°æ‰§è¡ŒæˆåŠŸï¼Œæ•°æ®å·²ä¿æŒæœ€æ–°çŠ¶æ€")

    async def run_production_update(self) -> bool:
        """è¿è¡Œç”Ÿäº§çº§æ›´æ–°"""
        self.stats['start_time'] = datetime.now()

        try:
            # åˆå§‹åŒ–
            if not await self.initialize():
                return False

            # è·å– Tushare ä»»åŠ¡åˆ—è¡¨
            tushare_tasks = await self.get_tushare_tasks()
            if not tushare_tasks:
                logger.error("âŒ æœªå‘ç°ä»»ä½• Tushare ä»»åŠ¡")
                return False

            self.stats['total_tasks'] = len(tushare_tasks)

            # æ‰§è¡Œä»»åŠ¡
            logger.info("ğŸš€ å¼€å§‹ç”Ÿäº§çº§ Tushare æ•°æ®æ›´æ–°...")
            results = await self.execute_tasks_parallel(tushare_tasks)

            # ç»Ÿè®¡ç»“æœ
            for result in results:
                status = result.get('status', 'unknown')
                if status in ['success', 'partial_success']:
                    self.stats['successful_tasks'] += 1
                elif status in ['failed', 'error']:
                    self.stats['failed_tasks'] += 1
                elif status in ['skipped', 'skipped_dry_run']:
                    self.stats['skipped_tasks'] += 1
                elif status == 'completed_with_warnings':
                    # å…¼å®¹æ—§çš„çŠ¶æ€ï¼Œå½’ç±»ä¸ºéƒ¨åˆ†æˆåŠŸ
                    self.stats['successful_tasks'] += 1
                else:
                    # å¤„ç†å…¶ä»–æœªçŸ¥çŠ¶æ€
                    logger.warning(f"æœªçŸ¥ä»»åŠ¡çŠ¶æ€: {status} for task {result.get('task_name')}")
                    self.stats['failed_tasks'] += 1  # å½’ç±»ä¸ºå¤±è´¥

            # æ‰“å°æ‘˜è¦
            self.stats['end_time'] = datetime.now()
            self.print_execution_summary(results)

            # è¿”å›æˆåŠŸçŠ¶æ€
            success_rate = self.stats['successful_tasks'] / max(self.stats['total_tasks'], 1)
            return success_rate >= 0.8  # 80% æˆåŠŸç‡è§†ä¸ºæ•´ä½“æˆåŠŸ

        except Exception as e:
            logger.error(f"âŒ ç”Ÿäº§çº§æ›´æ–°æ‰§è¡Œå¤±è´¥: {e}")
            return False
        finally:
            # æ¸…ç†èµ„æº
            if self.executor:
                self.executor.shutdown(wait=True)
            if self.db_manager:
                await self.db_manager.close()


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Tushare æ•°æ®æºæ™ºèƒ½å¢é‡æ›´æ–°ç”Ÿäº§è„šæœ¬')
    parser.add_argument('--workers', type=int, default=3,
                       help='æœ€å¤§å¹¶å‘å·¥ä½œè¿›ç¨‹æ•° (é»˜è®¤: 3)')
    parser.add_argument('--max_retries', type=int, default=3,
                       help='å•ä¸ªä»»åŠ¡æœ€å¤§é‡è¯•æ¬¡æ•° (é»˜è®¤: 3)')
    parser.add_argument('--retry_delay', type=int, default=5,
                       help='é‡è¯•é—´éš”ç§’æ•° (é»˜è®¤: 5)')
    parser.add_argument('--log_level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
                       default='INFO', help='æ—¥å¿—çº§åˆ« (é»˜è®¤: INFO)')
    parser.add_argument('--dry-run', action='store_true',
                       help='å¯ç”¨å¹²è¿è¡Œæ¨¡å¼ï¼Œåªæ˜¾ç¤ºå°†è¦æ‰§è¡Œçš„ä»»åŠ¡ï¼Œä¸å®é™…æ‰§è¡Œ')

    args = parser.parse_args()

    # è®¾ç½®æ—¥å¿—çº§åˆ«
    logging.getLogger().setLevel(getattr(logging, args.log_level))

    print("ğŸš€ Tushare æ•°æ®æºæ™ºèƒ½å¢é‡æ›´æ–°ç”Ÿäº§è„šæœ¬")
    print("=" * 60)
    print(f"å¹¶å‘è¿›ç¨‹æ•°: {args.workers}")
    print(f"æœ€å¤§é‡è¯•æ¬¡æ•°: {args.max_retries}")
    print(f"é‡è¯•é—´éš”: {args.retry_delay}ç§’")
    print(f"æ—¥å¿—çº§åˆ«: {args.log_level}")
    print(f"å¹²è¿è¡Œæ¨¡å¼: {'æ˜¯' if args.dry_run else 'å¦'}")
    print(f"å¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()

    # æ˜¾ç¤º Tushare API å¹¶å‘é™åˆ¶è¯´æ˜
    updater = TushareProductionUpdater(
        max_workers=args.workers,
        max_retries=args.max_retries,
        retry_delay=args.retry_delay,
        dry_run=args.dry_run
    )
    print(updater.tushare_concurrency_note)
    print()

    # åˆ›å»ºæ›´æ–°å™¨å¹¶æ‰§è¡Œ
    # é‡ç”¨ä¹‹å‰åˆ›å»ºçš„updaterå®ä¾‹ï¼Œç¡®ä¿å‚æ•°ä¸€è‡´
    # updater å·²ç»åœ¨å‰é¢åˆ›å»ºè¿‡äº†ï¼Œè¿™é‡Œç›´æ¥ä½¿ç”¨

    success = await updater.run_production_update()

    # è¿”å›é€€å‡ºç 
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    asyncio.run(main())
