#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
æ•°æ®åº“æ‰¹é‡ä¿å­˜æ€§èƒ½ä¼˜åŒ–æµ‹è¯•è„šæœ¬

è¯¥è„šæœ¬ç”¨äºæµ‹è¯•å’ŒéªŒè¯ç¬¬ä¸€é˜¶æ®µæ•°æ®åº“æ€§èƒ½ä¼˜åŒ–çš„æ•ˆæœï¼ŒåŒ…æ‹¬ï¼š
1. è¿æ¥æ± é…ç½®ä¼˜åŒ–æµ‹è¯•
2. æ€§èƒ½ç›‘æ§åŠŸèƒ½æµ‹è¯•
3. æ‰¹é‡å†™å…¥æ€§èƒ½åŸºå‡†æµ‹è¯•
4. é…ç½®æ–‡ä»¶æ”¯æŒæµ‹è¯•

ä½¿ç”¨æ–¹æ³•ï¼š
    python test_db_performance_optimization.py

æ³¨æ„ï¼š
    - éœ€è¦æœ‰æ•ˆçš„æ•°æ®åº“è¿æ¥é…ç½®
    - ä¼šåˆ›å»ºæµ‹è¯•è¡¨ï¼Œæµ‹è¯•å®Œæˆåè‡ªåŠ¨æ¸…ç†
    - å»ºè®®åœ¨æµ‹è¯•ç¯å¢ƒä¸­è¿è¡Œ
"""

import asyncio
import time
import pandas as pd
import numpy as np
from datetime import datetime, date
from typing import Dict, Any

# å¯¼å…¥ä¼˜åŒ–åçš„æ•°æ®åº“ç®¡ç†å™¨
from alphahome.common.db_manager import create_async_manager
from alphahome.common.config_manager import load_config


class DatabasePerformanceTest:
    """æ•°æ®åº“æ€§èƒ½æµ‹è¯•ç±»"""
    
    def __init__(self):
        self.db_manager = None
        self.test_table_name = f"test_performance_{int(time.time())}"
        self.test_results = {}
    
    async def setup(self):
        """åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ"""
        print("ğŸ”§ åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ...")
        
        # åŠ è½½é…ç½®
        config = load_config()
        db_url = config.get('database', {}).get('url')
        
        if not db_url:
            raise ValueError("æœªæ‰¾åˆ°æ•°æ®åº“è¿æ¥é…ç½®ï¼Œè¯·æ£€æŸ¥ config.json æ–‡ä»¶")
        
        # åˆ›å»ºæ•°æ®åº“ç®¡ç†å™¨
        self.db_manager = create_async_manager(db_url)
        await self.db_manager.connect()
        
        # åˆ›å»ºæµ‹è¯•è¡¨
        await self._create_test_table()
        
        print(f"âœ… æµ‹è¯•ç¯å¢ƒåˆå§‹åŒ–å®Œæˆï¼Œæµ‹è¯•è¡¨: {self.test_table_name}")
    
    async def _create_test_table(self):
        """åˆ›å»ºæµ‹è¯•è¡¨"""
        create_table_sql = f"""
        CREATE TABLE IF NOT EXISTS {self.test_table_name} (
            id SERIAL PRIMARY KEY,
            ts_code VARCHAR(15) NOT NULL,
            trade_date DATE NOT NULL,
            open_price FLOAT,
            close_price FLOAT,
            volume BIGINT,
            amount FLOAT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            UNIQUE(ts_code, trade_date)
        );
        """
        await self.db_manager.execute(create_table_sql)
    
    def generate_test_data(self, num_rows: int) -> pd.DataFrame:
        """ç”Ÿæˆæµ‹è¯•æ•°æ®"""
        np.random.seed(42)  # ç¡®ä¿å¯é‡å¤æ€§
        
        # ç”Ÿæˆè‚¡ç¥¨ä»£ç 
        stock_codes = [f"{str(i).zfill(6)}.SZ" for i in range(1, 101)]
        
        # ç”Ÿæˆæ—¥æœŸèŒƒå›´
        start_date = date(2024, 1, 1)
        dates = pd.date_range(start_date, periods=num_rows//len(stock_codes) + 1, freq='D')
        
        data = []
        for i in range(num_rows):
            data.append({
                'ts_code': np.random.choice(stock_codes),
                'trade_date': np.random.choice(dates).date(),
                'open_price': np.random.uniform(10, 100),
                'close_price': np.random.uniform(10, 100),
                'volume': np.random.randint(1000, 1000000),
                'amount': np.random.uniform(10000, 10000000)
            })
        
        return pd.DataFrame(data)
    
    async def test_connection_pool_config(self):
        """æµ‹è¯•è¿æ¥æ± é…ç½®"""
        print("\nğŸ“Š æµ‹è¯•è¿æ¥æ± é…ç½®...")
        
        # æ£€æŸ¥è¿æ¥æ± æ˜¯å¦æ­£ç¡®é…ç½®
        pool = self.db_manager.pool
        if pool:
            print(f"âœ… è¿æ¥æ± å·²åˆ›å»º")
            print(f"   - æœ€å°è¿æ¥æ•°: {pool._minsize}")
            print(f"   - æœ€å¤§è¿æ¥æ•°: {pool._maxsize}")
            
            # æµ‹è¯•å¹¶å‘è¿æ¥
            start_time = time.time()
            tasks = []
            for i in range(10):
                task = asyncio.create_task(
                    self.db_manager.fetch_val("SELECT 1")
                )
                tasks.append(task)
            
            results = await asyncio.gather(*tasks)
            end_time = time.time()
            
            print(f"âœ… å¹¶å‘æŸ¥è¯¢æµ‹è¯•å®Œæˆ")
            print(f"   - å¹¶å‘ä»»åŠ¡æ•°: {len(tasks)}")
            print(f"   - æ€»è€—æ—¶: {end_time - start_time:.3f}s")
            print(f"   - å¹³å‡è€—æ—¶: {(end_time - start_time) / len(tasks):.3f}s")
            
            self.test_results['connection_pool'] = {
                'min_size': pool._minsize,
                'max_size': pool._maxsize,
                'concurrent_queries': len(tasks),
                'total_time': end_time - start_time,
                'avg_time_per_query': (end_time - start_time) / len(tasks)
            }
        else:
            print("âŒ è¿æ¥æ± æœªåˆ›å»º")
    
    async def test_performance_monitoring(self):
        """æµ‹è¯•æ€§èƒ½ç›‘æ§åŠŸèƒ½"""
        print("\nğŸ“ˆ æµ‹è¯•æ€§èƒ½ç›‘æ§åŠŸèƒ½...")
        
        # é‡ç½®æ€§èƒ½ç»Ÿè®¡
        self.db_manager.reset_performance_statistics()
        
        # æ‰§è¡Œå‡ æ¬¡ä¸åŒå¤§å°çš„æ‰¹é‡æ“ä½œ
        batch_sizes = [1000, 5000, 10000]
        
        for batch_size in batch_sizes:
            print(f"   æµ‹è¯•æ‰¹æ¬¡å¤§å°: {batch_size} è¡Œ")
            
            # ç”Ÿæˆæµ‹è¯•æ•°æ®
            test_data = self.generate_test_data(batch_size)
            
            # æ‰§è¡Œæ‰¹é‡æ’å…¥
            start_time = time.time()
            result = await self.db_manager.copy_from_dataframe(
                df=test_data,
                target=self.test_table_name,
                conflict_columns=['ts_code', 'trade_date'],
                update_columns=['open_price', 'close_price', 'volume', 'amount']
            )
            end_time = time.time()
            
            print(f"     - å¤„ç†è¡Œæ•°: {result}")
            print(f"     - è€—æ—¶: {end_time - start_time:.3f}s")
            print(f"     - ååé‡: {result / (end_time - start_time):.0f} è¡Œ/ç§’")
        
        # è·å–æ€§èƒ½ç»Ÿè®¡
        stats = self.db_manager.get_performance_statistics()
        print(f"\nğŸ“Š æ€§èƒ½ç»Ÿè®¡æ‘˜è¦:")
        print(f"   - æ€»æ“ä½œæ•°: {stats['total_operations']}")
        print(f"   - æ€»å¤„ç†è¡Œæ•°: {stats['total_rows_processed']}")
        print(f"   - å¹³å‡ååé‡: {stats['average_throughput']:.0f} è¡Œ/ç§’")
        print(f"   - æœ€è¿‘å¹³å‡ååé‡: {stats['recent_average_throughput']:.0f} è¡Œ/ç§’")
        print(f"   - å»ºè®®æ‰¹æ¬¡å¤§å°: {stats['optimal_batch_size']} è¡Œ")
        
        self.test_results['performance_monitoring'] = stats
    
    async def test_batch_size_optimization(self):
        """æµ‹è¯•ä¸åŒæ‰¹æ¬¡å¤§å°çš„æ€§èƒ½"""
        print("\nğŸ¯ æµ‹è¯•æ‰¹æ¬¡å¤§å°ä¼˜åŒ–...")
        
        batch_sizes = [500, 1000, 2500, 5000, 7500, 10000, 15000]
        results = {}
        
        for batch_size in batch_sizes:
            print(f"   æµ‹è¯•æ‰¹æ¬¡å¤§å°: {batch_size} è¡Œ")
            
            # ç”Ÿæˆæµ‹è¯•æ•°æ®
            test_data = self.generate_test_data(batch_size)
            
            # æ‰§è¡Œæ‰¹é‡æ“ä½œå¹¶æµ‹é‡æ€§èƒ½
            start_time = time.time()
            try:
                result = await self.db_manager.copy_from_dataframe(
                    df=test_data,
                    target=self.test_table_name,
                    conflict_columns=['ts_code', 'trade_date']
                )
                end_time = time.time()
                
                processing_time = end_time - start_time
                throughput = result / processing_time if processing_time > 0 else 0
                
                results[batch_size] = {
                    'rows_processed': result,
                    'processing_time': processing_time,
                    'throughput': throughput
                }
                
                print(f"     - æˆåŠŸ: {throughput:.0f} è¡Œ/ç§’")
                
            except Exception as e:
                print(f"     - å¤±è´¥: {str(e)}")
                results[batch_size] = {
                    'error': str(e)
                }
        
        # æ‰¾åˆ°æœ€ä¼˜æ‰¹æ¬¡å¤§å°
        valid_results = {k: v for k, v in results.items() if 'throughput' in v}
        if valid_results:
            optimal_batch_size = max(valid_results.keys(), 
                                   key=lambda k: valid_results[k]['throughput'])
            optimal_throughput = valid_results[optimal_batch_size]['throughput']
            
            print(f"\nğŸ† æœ€ä¼˜æ‰¹æ¬¡å¤§å°: {optimal_batch_size} è¡Œ")
            print(f"   æœ€é«˜ååé‡: {optimal_throughput:.0f} è¡Œ/ç§’")
            
            self.test_results['batch_optimization'] = {
                'optimal_batch_size': optimal_batch_size,
                'optimal_throughput': optimal_throughput,
                'all_results': results
            }
    
    async def cleanup(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        print(f"\nğŸ§¹ æ¸…ç†æµ‹è¯•ç¯å¢ƒ...")
        
        try:
            # åˆ é™¤æµ‹è¯•è¡¨
            await self.db_manager.execute(f"DROP TABLE IF EXISTS {self.test_table_name}")
            print(f"âœ… æµ‹è¯•è¡¨ {self.test_table_name} å·²åˆ é™¤")
            
            # å…³é—­æ•°æ®åº“è¿æ¥
            await self.db_manager.close()
            print("âœ… æ•°æ®åº“è¿æ¥å·²å…³é—­")
            
        except Exception as e:
            print(f"âš ï¸ æ¸…ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
    
    def print_summary(self):
        """æ‰“å°æµ‹è¯•æ€»ç»“"""
        print("\n" + "="*60)
        print("ğŸ“‹ æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–æµ‹è¯•æ€»ç»“")
        print("="*60)
        
        if 'connection_pool' in self.test_results:
            pool_stats = self.test_results['connection_pool']
            print(f"\nğŸ”— è¿æ¥æ± é…ç½®:")
            print(f"   æœ€å¤§è¿æ¥æ•°: {pool_stats['max_size']} (ä¼˜åŒ–å‰: 10)")
            print(f"   å¹¶å‘æŸ¥è¯¢æ€§èƒ½: {pool_stats['avg_time_per_query']:.3f}s/æŸ¥è¯¢")
        
        if 'performance_monitoring' in self.test_results:
            perf_stats = self.test_results['performance_monitoring']
            print(f"\nğŸ“Š æ€§èƒ½ç›‘æ§:")
            print(f"   æ€»æ“ä½œæ•°: {perf_stats['total_operations']}")
            print(f"   å¹³å‡ååé‡: {perf_stats['average_throughput']:.0f} è¡Œ/ç§’")
            print(f"   å»ºè®®æ‰¹æ¬¡å¤§å°: {perf_stats['optimal_batch_size']} è¡Œ")
        
        if 'batch_optimization' in self.test_results:
            batch_stats = self.test_results['batch_optimization']
            print(f"\nğŸ¯ æ‰¹æ¬¡ä¼˜åŒ–:")
            print(f"   æœ€ä¼˜æ‰¹æ¬¡å¤§å°: {batch_stats['optimal_batch_size']} è¡Œ")
            print(f"   æœ€é«˜ååé‡: {batch_stats['optimal_throughput']:.0f} è¡Œ/ç§’")
        
        print(f"\nâœ… æµ‹è¯•å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æ•°æ®åº“æ‰¹é‡ä¿å­˜æ€§èƒ½ä¼˜åŒ–æµ‹è¯•")
    print("="*60)
    
    test = DatabasePerformanceTest()
    
    try:
        # åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ
        await test.setup()
        
        # æ‰§è¡Œå„é¡¹æµ‹è¯•
        await test.test_connection_pool_config()
        await test.test_performance_monitoring()
        await test.test_batch_size_optimization()
        
        # æ‰“å°æµ‹è¯•æ€»ç»“
        test.print_summary()
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        # æ¸…ç†æµ‹è¯•ç¯å¢ƒ
        await test.cleanup()


if __name__ == "__main__":
    asyncio.run(main())
