#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
BT Extensions å…¨å¸‚åœºå›æµ‹æ•ˆç‡æµ‹è¯•

æµ‹è¯•æ–°è®¾è®¡çš„ bt_extensions æ¨¡å—åœ¨å¤§è§„æ¨¡å›æµ‹ä¸­çš„æ€§èƒ½è¡¨ç°ï¼š
1. æ‰¹é‡æ•°æ®åŠ è½½æ•ˆç‡
2. å¹¶è¡Œå›æµ‹æ€§èƒ½
3. ç¼“å­˜ç³»ç»Ÿæ•ˆæœ
4. å†…å­˜ä½¿ç”¨ç›‘æ§
5. ä¸ä¹‹å‰ç‰ˆæœ¬çš„æ€§èƒ½å¯¹æ¯”
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import time
import psutil
import backtrader as bt
from datetime import date, datetime
from typing import List, Dict, Any

from alphahome.bt_extensions import (
    BatchDataLoader,
    ParallelBacktestRunner,
    CacheManager,
    PerformanceMonitor,
    PostgreSQLDataFeed
)
from alphahome.common.db_manager import create_sync_manager
from alphahome.common.config_manager import ConfigManager


class SimpleMAStrategy(bt.Strategy):
    """ç®€å•ç§»åŠ¨å¹³å‡ç­–ç•¥ï¼Œç”¨äºæ€§èƒ½æµ‹è¯•"""
    
    params = (
        ('ma_period', 20),
    )
    
    def __init__(self):
        self.ma = bt.indicators.SMA(self.data.close, period=self.params.ma_period)
    
    def next(self):
        if not self.position:
            if self.data.close[0] > self.ma[0]:
                self.buy()
        else:
            if self.data.close[0] < self.ma[0]:
                self.sell()


class PerformanceTester:
    """æ€§èƒ½æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.config_manager = ConfigManager()
        db_url = self.config_manager.get_database_url()
        self.db_manager = create_sync_manager(db_url)
        self.results = {}
    
    def get_stock_list(self, limit: int = None) -> List[str]:
        """è·å–è‚¡ç¥¨åˆ—è¡¨"""
        sql = """
        SELECT DISTINCT ts_code 
        FROM tushare_stock_daily 
        WHERE trade_date >= '2023-01-01' 
        AND trade_date <= '2023-12-31'
        ORDER BY ts_code
        """
        
        if limit:
            sql += f" LIMIT {limit}"
        
        records = self.db_manager.fetch(sql)
        return [record['ts_code'] for record in records]
    
    def test_batch_loading_performance(self, stock_counts: List[int]):
        """æµ‹è¯•æ‰¹é‡æ•°æ®åŠ è½½æ€§èƒ½"""
        print("\n" + "="*80)
        print("1. æ‰¹é‡æ•°æ®åŠ è½½æ€§èƒ½æµ‹è¯•")
        print("="*80)
        
        # ä¸åŒç¼“å­˜é…ç½®çš„æµ‹è¯•
        cache_configs = [
            {"name": "æ— ç¼“å­˜", "cache": None},
            {"name": "å†…å­˜ç¼“å­˜(256MB)", "cache": CacheManager(max_memory_mb=256, enable_disk_cache=False)},
            {"name": "æ··åˆç¼“å­˜(256MB+ç£ç›˜)", "cache": CacheManager(max_memory_mb=256, enable_disk_cache=True)}
        ]
        
        results = {}
        
        for stock_count in stock_counts:
            print(f"\næµ‹è¯• {stock_count} åªè‚¡ç¥¨çš„åŠ è½½æ€§èƒ½:")
            stock_list = self.get_stock_list(stock_count)
            
            results[stock_count] = {}
            
            for config in cache_configs:
                print(f"  {config['name']}:")
                
                batch_loader = BatchDataLoader(self.db_manager, config['cache'])
                
                # ç¬¬ä¸€æ¬¡åŠ è½½ï¼ˆå†·å¯åŠ¨ï¼‰
                start_time = time.time()
                start_memory = psutil.Process().memory_info().rss / 1024 / 1024
                
                stock_data = batch_loader.load_stocks_data(
                    stock_codes=stock_list,
                    start_date=date(2023, 1, 1),
                    end_date=date(2023, 12, 31),
                    use_cache=config['cache'] is not None
                )
                
                cold_time = time.time() - start_time
                cold_memory = psutil.Process().memory_info().rss / 1024 / 1024
                
                # ç¬¬äºŒæ¬¡åŠ è½½ï¼ˆçƒ­å¯åŠ¨ï¼Œæµ‹è¯•ç¼“å­˜æ•ˆæœï¼‰
                if config['cache'] is not None:
                    start_time = time.time()
                    stock_data_cached = batch_loader.load_stocks_data(
                        stock_codes=stock_list,
                        start_date=date(2023, 1, 1),
                        end_date=date(2023, 12, 31),
                        use_cache=True
                    )
                    hot_time = time.time() - start_time
                    hot_memory = psutil.Process().memory_info().rss / 1024 / 1024
                    
                    # ç¼“å­˜ç»Ÿè®¡
                    cache_stats = batch_loader.get_cache_stats()
                    hit_rate = cache_stats.get('overall_hit_rate', 0)
                    cache_memory = cache_stats.get('memory_size_mb', 0)
                else:
                    hot_time = None
                    hot_memory = cold_memory
                    hit_rate = 0
                    cache_memory = 0
                
                results[stock_count][config['name']] = {
                    'loaded_stocks': len(stock_data),
                    'cold_time': cold_time,
                    'hot_time': hot_time,
                    'memory_growth': cold_memory - start_memory,
                    'final_memory': hot_memory,
                    'hit_rate': hit_rate,
                    'cache_memory': cache_memory,
                    'records_per_second': sum(len(df) for df in stock_data.values()) / cold_time
                }
                
                print(f"    å†·å¯åŠ¨: {cold_time:.2f}ç§’, å†…å­˜å¢é•¿: {cold_memory - start_memory:.1f}MB")
                if hot_time is not None:
                    print(f"    çƒ­å¯åŠ¨: {hot_time:.2f}ç§’, ç¼“å­˜å‘½ä¸­ç‡: {hit_rate:.1f}%")
                    if hot_time > 0:
                        print(f"    æ€§èƒ½æå‡: {cold_time/hot_time:.1f}x")
                    else:
                        print(f"    æ€§èƒ½æå‡: >1000x (ç¬æ—¶ç¼“å­˜å‘½ä¸­)")
                print(f"    æ•°æ®å¤„ç†é€Ÿåº¦: {results[stock_count][config['name']]['records_per_second']:.0f} è®°å½•/ç§’")
                
                # æ¸…ç†ç¼“å­˜
                if config['cache']:
                    config['cache'].clear()
        
        self.results['batch_loading'] = results
        return results
    
    def test_parallel_execution_performance(self, stock_count: int, worker_configs: List[Dict]):
        """æµ‹è¯•å¹¶è¡Œæ‰§è¡Œæ€§èƒ½"""
        print(f"\n" + "="*80)
        print(f"2. å¹¶è¡Œæ‰§è¡Œæ€§èƒ½æµ‹è¯• ({stock_count} åªè‚¡ç¥¨)")
        print("="*80)
        
        stock_list = self.get_stock_list(stock_count)
        results = {}
        
        for config in worker_configs:
            workers = config['workers']
            batch_size = config['batch_size']
            name = f"{workers}è¿›ç¨‹-æ‰¹æ¬¡{batch_size}"
            
            print(f"\næµ‹è¯•é…ç½®: {name}")
            
            # åˆ›å»ºå¹¶è¡Œæ‰§è¡Œå™¨
            db_url = self.config_manager.get_database_url()
            parallel_runner = ParallelBacktestRunner(
                max_workers=workers,
                batch_size=batch_size,
                db_config=db_url
            )
            
            # å¯åŠ¨æ€§èƒ½ç›‘æ§
            monitor = PerformanceMonitor(monitor_interval=1.0)
            monitor.start_monitoring()
            
            start_time = time.time()
            start_memory = psutil.Process().memory_info().rss / 1024 / 1024
            
            # æ‰§è¡Œå¹¶è¡Œå›æµ‹
            backtest_results = parallel_runner.run_parallel_backtests(
                stock_codes=stock_list,
                strategy_class=SimpleMAStrategy,
                strategy_params={'ma_period': 20},
                start_date=date(2023, 1, 1),
                end_date=date(2023, 12, 31),
                initial_cash=100000.0,
                commission=0.001
            )
            
            total_time = time.time() - start_time
            final_memory = psutil.Process().memory_info().rss / 1024 / 1024
            
            # åœæ­¢ç›‘æ§
            perf_stats = monitor.stop_monitoring()
            
            # åˆ†æç»“æœ
            summary = backtest_results['summary']
            successful_stocks = summary['successful_stocks']
            
            results[name] = {
                'total_time': total_time,
                'successful_stocks': successful_stocks,
                'success_rate': summary['success_rate'],
                'stocks_per_second': successful_stocks / total_time,
                'memory_growth': final_memory - start_memory,
                'avg_cpu': perf_stats.get('cpu', {}).get('avg_percent', 0),
                'peak_memory': perf_stats.get('memory', {}).get('peak_rss_mb', 0),
                'avg_return': summary.get('avg_return', 0),
                'performance_stats': perf_stats
            }
            
            print(f"  æ‰§è¡Œæ—¶é—´: {total_time:.1f}ç§’")
            print(f"  æˆåŠŸè‚¡ç¥¨: {successful_stocks}/{stock_count} ({summary['success_rate']:.1f}%)")
            print(f"  å¤„ç†é€Ÿåº¦: {successful_stocks / total_time:.1f} è‚¡ç¥¨/ç§’")
            print(f"  å†…å­˜å¢é•¿: {final_memory - start_memory:.1f}MB")
            print(f"  å¹³å‡CPU: {perf_stats.get('cpu', {}).get('avg_percent', 0):.1f}%")
            print(f"  å³°å€¼å†…å­˜: {perf_stats.get('memory', {}).get('peak_rss_mb', 0):.1f}MB")
        
        self.results['parallel_execution'] = results
        return results
    
    def test_scaling_analysis(self, stock_counts: List[int]):
        """æµ‹è¯•æ‰©å±•æ€§åˆ†æ"""
        print(f"\n" + "="*80)
        print("3. æ‰©å±•æ€§åˆ†ææµ‹è¯•")
        print("="*80)
        
        results = {}
        
        # ä½¿ç”¨æœ€ä¼˜é…ç½®è¿›è¡Œæ‰©å±•æ€§æµ‹è¯•
        best_config = {
            'workers': min(4, psutil.cpu_count()),
            'batch_size': 50
        }
        
        for stock_count in stock_counts:
            print(f"\næµ‹è¯• {stock_count} åªè‚¡ç¥¨:")
            
            stock_list = self.get_stock_list(stock_count)
            actual_count = len(stock_list)
            
            if actual_count < stock_count:
                print(f"  è­¦å‘Š: å®é™…åªæœ‰ {actual_count} åªè‚¡ç¥¨æ•°æ®")
            
            # æ‰§è¡Œå›æµ‹
            db_url = self.config_manager.get_database_url()
            parallel_runner = ParallelBacktestRunner(
                max_workers=best_config['workers'],
                batch_size=best_config['batch_size'],
                db_config=db_url
            )
            
            start_time = time.time()
            start_memory = psutil.Process().memory_info().rss / 1024 / 1024
            
            backtest_results = parallel_runner.run_parallel_backtests(
                stock_codes=stock_list,
                strategy_class=SimpleMAStrategy,
                strategy_params={'ma_period': 20},
                start_date=date(2023, 1, 1),
                end_date=date(2023, 12, 31),
                initial_cash=100000.0,
                commission=0.001
            )
            
            total_time = time.time() - start_time
            final_memory = psutil.Process().memory_info().rss / 1024 / 1024
            
            summary = backtest_results['summary']
            
            results[stock_count] = {
                'actual_stocks': actual_count,
                'successful_stocks': summary['successful_stocks'],
                'total_time': total_time,
                'stocks_per_second': summary['successful_stocks'] / total_time,
                'memory_growth': final_memory - start_memory,
                'memory_per_stock': (final_memory - start_memory) / actual_count if actual_count > 0 else 0,
                'success_rate': summary['success_rate']
            }
            
            print(f"  å®é™…å¤„ç†: {actual_count} åªè‚¡ç¥¨")
            print(f"  æˆåŠŸå®Œæˆ: {summary['successful_stocks']} åª")
            print(f"  æ‰§è¡Œæ—¶é—´: {total_time:.1f}ç§’")
            print(f"  å¤„ç†é€Ÿåº¦: {summary['successful_stocks'] / total_time:.1f} è‚¡ç¥¨/ç§’")
            print(f"  å†…å­˜æ•ˆç‡: {(final_memory - start_memory) / actual_count:.2f}MB/è‚¡ç¥¨")
        
        self.results['scaling'] = results
        return results
    
    def generate_performance_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        print("\n" + "="*80)
        print("ğŸ“Š BT Extensions æ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
        print("="*80)
        
        # æ‰¹é‡åŠ è½½æ€§èƒ½æŠ¥å‘Š
        if 'batch_loading' in self.results:
            print("\nğŸš€ æ‰¹é‡æ•°æ®åŠ è½½æ€§èƒ½:")
            batch_results = self.results['batch_loading']
            
            for stock_count, configs in batch_results.items():
                print(f"\n  {stock_count} åªè‚¡ç¥¨:")
                for config_name, stats in configs.items():
                    cold_time = stats['cold_time']
                    hot_time = stats.get('hot_time')
                    hit_rate = stats['hit_rate']
                    
                    print(f"    {config_name}: {cold_time:.2f}ç§’", end="")
                    if hot_time is not None:
                        print(f" â†’ {hot_time:.2f}ç§’ (ç¼“å­˜å‘½ä¸­ç‡{hit_rate:.1f}%)")
                    else:
                        print()
        
        # å¹¶è¡Œæ‰§è¡Œæ€§èƒ½æŠ¥å‘Š
        if 'parallel_execution' in self.results:
            print("\nâš¡ å¹¶è¡Œæ‰§è¡Œæ€§èƒ½:")
            parallel_results = self.results['parallel_execution']
            
            best_config = min(parallel_results.items(), key=lambda x: x[1]['total_time'])
            print(f"  æœ€ä¼˜é…ç½®: {best_config[0]}")
            print(f"  æœ€ä½³æ€§èƒ½: {best_config[1]['stocks_per_second']:.1f} è‚¡ç¥¨/ç§’")
            
            for config_name, stats in parallel_results.items():
                print(f"    {config_name}: {stats['stocks_per_second']:.1f} è‚¡ç¥¨/ç§’, "
                      f"æˆåŠŸç‡ {stats['success_rate']:.1f}%")
        
        # æ‰©å±•æ€§åˆ†ææŠ¥å‘Š
        if 'scaling' in self.results:
            print("\nğŸ“ˆ æ‰©å±•æ€§åˆ†æ:")
            scaling_results = self.results['scaling']
            
            stock_counts = sorted(scaling_results.keys())
            for stock_count in stock_counts:
                stats = scaling_results[stock_count]
                print(f"  {stock_count:4d} è‚¡ç¥¨: {stats['stocks_per_second']:5.1f} è‚¡ç¥¨/ç§’, "
                      f"{stats['memory_per_stock']:4.2f}MB/è‚¡ç¥¨")
            
            # çº¿æ€§åº¦åˆ†æ
            if len(stock_counts) >= 2:
                first_perf = scaling_results[stock_counts[0]]['stocks_per_second']
                last_perf = scaling_results[stock_counts[-1]]['stocks_per_second']
                efficiency = (last_perf / first_perf) * 100
                print(f"\n  æ‰©å±•æ•ˆç‡: {efficiency:.1f}% (ç†æƒ³å€¼: 100%)")
        
        # å…¨å¸‚åœºå›æµ‹é¢„ä¼°
        if 'scaling' in self.results:
            print("\nğŸ¯ å…¨å¸‚åœºå›æµ‹é¢„ä¼°:")
            
            # åŸºäºæœ€å¤§æµ‹è¯•è§„æ¨¡é¢„ä¼°
            max_tested = max(self.results['scaling'].keys())
            max_stats = self.results['scaling'][max_tested]
            avg_speed = max_stats['stocks_per_second']
            
            full_market_estimates = [
                (1000, "å°ç›˜è‚¡é€‰è‚¡"),
                (2000, "ä¸­ç­‰è§„æ¨¡å›æµ‹"),
                (4000, "å…¨å¸‚åœºå›æµ‹"),
                (5000, "å…¨å¸‚åœº+æŒ‡æ•°")
            ]
            
            for stock_count, description in full_market_estimates:
                estimated_time = stock_count / avg_speed
                estimated_memory = max_stats['memory_per_stock'] * stock_count
                
                hours = int(estimated_time // 3600)
                minutes = int((estimated_time % 3600) // 60)
                seconds = int(estimated_time % 60)
                
                time_str = f"{hours:02d}:{minutes:02d}:{seconds:02d}"
                
                print(f"  {stock_count:4d} è‚¡ç¥¨ ({description}): "
                      f"é¢„ä¼° {time_str}, å†…å­˜éœ€æ±‚ {estimated_memory:.0f}MB")
        
        print("\n" + "="*80)


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("BT Extensions å…¨å¸‚åœºå›æµ‹æ•ˆç‡æµ‹è¯•")
    print("="*80)
    
    tester = PerformanceTester()
    
    try:
        # 1. æ‰¹é‡åŠ è½½æ€§èƒ½æµ‹è¯•ï¼ˆä¸åŒè‚¡ç¥¨æ•°é‡ï¼‰
        print("å¼€å§‹æ‰¹é‡æ•°æ®åŠ è½½æ€§èƒ½æµ‹è¯•...")
        batch_results = tester.test_batch_loading_performance([10, 50, 100, 200])
        
        # 2. å¹¶è¡Œæ‰§è¡Œæ€§èƒ½æµ‹è¯•
        print("\nå¼€å§‹å¹¶è¡Œæ‰§è¡Œæ€§èƒ½æµ‹è¯•...")
        parallel_configs = [
            {'workers': 1, 'batch_size': 25},   # å•è¿›ç¨‹åŸºå‡†
            {'workers': 2, 'batch_size': 25},   # åŒè¿›ç¨‹
            {'workers': 4, 'batch_size': 25},   # å››è¿›ç¨‹
            {'workers': 2, 'batch_size': 50},   # å¤§æ‰¹æ¬¡
            {'workers': 4, 'batch_size': 100}   # å¤§æ‰¹æ¬¡+å¤šè¿›ç¨‹
        ]
        parallel_results = tester.test_parallel_execution_performance(100, parallel_configs)
        
        # 3. æ‰©å±•æ€§åˆ†ææµ‹è¯•
        print("\nå¼€å§‹æ‰©å±•æ€§åˆ†ææµ‹è¯•...")
        scaling_results = tester.test_scaling_analysis([50, 100, 200, 500])
        
        # 4. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        tester.generate_performance_report()
        
        print(f"\nâœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        print(f"ğŸ“‹ è¯¦ç»†ç»“æœå·²ä¿å­˜åœ¨æµ‹è¯•å™¨å¯¹è±¡ä¸­")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 